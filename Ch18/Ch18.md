
## Operational recipes

This chapter is going to be a short one, mostly consisting of walk throughs of particular tasks that you might need to 
do as part of operating a RavenDB cluster. This isn't meant to replace the online documentation but complement it. 
I tried to cover most of the common scenarios that you might run into in your systems. Beyond using this as a reference
I also suggest going through the various recipes to get some insight about the manner in which such issues are resolved.

Many of the methods listed in this chapter aren't new, we have covered them in previous chapters. Here they are just 
laid out in practical terms as a set of operations to achieve a specific goal.

### Cluster recipes

The cluster is the workhorse that users of RavenDB rarely think about. The cluster is responsible for managing all the 
nodes in the cluster, assign work, monitor the health of the nodes, etc. Most of the time, you setup the cluster once
and then let it do its thing.

This section is going to talk about when you need to go in and interve. The usual reasons for that is when you want to 
modify your cluster topology. The cluster topology isn't meant to be static and RavenDB explicitly support online 
modifications of the topology, so there isn't a high level of complexity involved. There are some details thah you should
take into account, though.

#### Adding a new node to the cluster

You might have started with a single node or have few members in the cluster already, but at some point, you'll want to
add a new node to your cluster. Most of the time, you setup the cluster itself as a whole using the setup wizard, so you
might not even be aware how to do this task.

> **Prerequisites for the new node**
>
> I'm assuming that you are running in a secured mode (the only reason you aren't is if you are playing around on the 
> local machine). That means that in order to add the new node, you'll need to have mutual trust between the new node
> and the existing cluster. The easiest way to do is to use a single wildcard certificate for all the nodes in 
> the cluster.
> In this way, because the new node it using the same certificate as the rest of the cluster, it will intrisinctly
> trust the remote node from cluster and accept its addition to the cluster.
>
> If you don't have the same certificate for all the nodes, you'll need to register the cluster certificate in the new
> node using (`rvn admin-channel` and then the `trustServerCert` command). 

You'll have to take care of the certificate (it is easiest to use a single wildcard certificate for all the nodes in the 
cluster, although that is not required), DNS updates, firewall configuration, etc. I'm assuming that all of this is all
properly setup ahead of time. There is nothing special about that part of the preparation.

When you start a new node for the first time, it defaults to using `Setup.Mode=Initial` and will start the setup wizard.
This is _not_ what you want in this case. Take a look at the `settings.json` file from one of your existing cluster nodes
and use that as the template for the configuration of the new node. You'll have to change the `ServerUrl`, 
`PublicServerUrl`, `ServerUrl.Tcp` and `PublicServerUrl.Tcp` options to match the new node location. Beyond that, keep the
configuration identical. While it is _possible_ to run with different configuration on different nodes, it is not 
recommended and too high a variance of some options will cause errors. 

One you are done with the configuration, you can start the new node. You can can connect to it and you'll see the studio
as usual, but there are no databases or documents in there, obviously. In fact, this new node is currently running in 
`passive` mode. In this mode, it is not yet determined whatever the node will be joined to an existing cluster or form
a completely new cluster.
A new cluster will be formed if you create a new database or setup client certificates. In general, anything that shows
that you are preparing this node to stand on its own. 

Once you have verified that the new node is up, running and accessible (but don't perfom any actions on the new node) go
to the _existing_ node and head to `Manage Server`, `Cluster` and then to `Add Node to cluster`. You can see how this 
will look in Figure 18.1.

![Adding a new node to an existing cluster](./Ch18/img01.png)

Figure 18.1 shows the new node screen. There isn't much there, because you don't need to do much beyond provide the URL
of the new node to add to the cluster. Once you click on the `Add` button, the cluster will connect to the new node, 
add it to the topology and start updating it with the current state of the system. During this period, the node will
show up in the `Promotable` state. 

In the `Promotable` state, the node is part of the cluster in the sense that it is getting updates, but it is not actually
taking part in votes or applicable to run for the cluster leadership. When the cluster is finished updating the new node
with the existing cluster state, the cluster will move the node into the `Member` state. At this point, the new node will
be able to become a leader and its vote will be counted by the cluster. 

At this point, the node is added to the cluster and can become the leader. However, it is still missing something. It has
no databases. RavenDB will not auto move databases between nodes when you add or remove a node from the cluster. This is
left to the discretion of the operations team. New databases created on the cluster will take the new node into account 
and the cluster may decide to migrate databases from a failed node to the new node, but these are rather exceptional
circumstances.

You'll typically wait until the node is fully joined to the cluster and then tell RavenDB to start migrating specific 
databases to the new node yourself. Remember that such a process is transparent to the client and will not impact the
usual operations in any manner.

#### Removing a node from the cluster

The flip side of adding a now to the cluster is removing a node. The _how_ of it is pretty simple, as you can see in 
Figure 18.2.

![Cluster node in the `Cluster` view, the red trash icon allows you to remove the node from the cluster](./Ch18/img02.png)

In Figure 18.2 you can see the nodes view in the studio. The red trash icon at the top right allows you to remove the node
from the cluster. Clicking on this will have cause the cluster to remove the node from the cluster. What does this mean?

At the cluster level, it means that we'll remove the node, obviously. Any database groups that contain this node will have
the node removed from the database group (and the replication factor adjusted accordingly). Any databases that reside only
on this node will be removed from the cluster entirely.

At the removed node level, it will revert back to passive mode. In this mode, the node is still accessible, but the 
databases that reside on this node are unloaded. This is because the node is in passive mode. You'll need to either re-join
the node to the cluster or let it know that it should form a new cluster before the databases on this node become available
again.
This is done to avoid clients or ETL tasks from talking to the now isolated database instance on the node that was removed
from the cluster. 

Forming a new cluster is a one way option. After the new node was moved from passive mode, you cannot add it back to the 
previous cluster (or any other, for that matter). A cluster can only add either empty nodes or nodes that were previously
attached to the same cluster.

When a new cluster is formed on the formerly passive node, the topology of all the database groups is adjusted. All the 
database groups that include the current node are shrank to include just the current node. All the database groups that
do not contain the current node are removed. 

> **Don't create a new cluster with the old URL**
>
> Forming a new cluster on the removed node is fine, but you should make sure that this is done with a _new_ URL. This
> is to avoid the case of a client using an old URL or an ETL task that hasn't been updated. The new cluster will share
> the same security configuration and you'll want to avoid existing clients and tasks talking to the newly independent
> node while thinking that they are talking to the cluster as a whole.

All other cluster wide settings, such as the certificates and database access authorization remain the same. You should
take that into account if you are intending to keep the node around after removing it from the cluster.

#### Migrate a node between machines

Node migration can happen for any number of reasons. The most obvious one is that the server IP has changed, and you need
to update the cluster topology. For this reason, among many others, it is not recommended to use raw IPs in the cluster
topology. Instead, use proper URLs and DNS to control the name resolution.

In such a way, moving node C from `10.0.0.28` to `10.0.0.32` can be done simply by updating the DNS of `c.prod.rvn` to the 
new value and forcing a DNS flush. You can do that with machine names, of course, but that would just move the problem if 
you need to change the host machine. A good example is if you setup a RavenDB node on `prodsrv1` and you need to move it to 
`prodsrv3`. At the same time, `prodsrv1` is used for other services, so you cannot just change the DNS.

> **Use dedicated DNS entries for RavenDB nodes**
>
> Avoid using IPs or hostnames for RavenDB nodes. The easiest options is to have a DNS entry for each RavenDB node that
> can be changed directly if needed. This can make changing the phyiscal topology of the cluster as easy as running a
> DNS update. This section is how to handle the case when you did _not_ do that.

I'm assuming that you have to move a node here by updating the cluster topology, not just updating the DNS (which is the 
recommended mode). There are a few things that you need to do to ensure that the transition is handled properly. Starting
from obtaining a certificate for the new node URL. Once that is done, the process is simple. 

We'll assume that you have a cluster of three nodes (A,B and C) and that you want to move node C to a different machine. 
The first thing to do is to shut down node C. Then go to one of the other nodes and remove node C from the cluster. You
can also do things in the reverse order, first remove node C and then shut it down, it doesn't really matter.

Move node C to another machine, setup the certificate and update the `ServerUrl`, `PublicServerUrl`, `ServerUrl.Tcp` and 
`PublicServerUrl.Tcp` options in the `settings.json` file. Then go to node A or B and follow the same procedure to add
a new node to the cluster. The new will be re-added (still called node C). RavenDB remembers the node and will perform
all the necessary hookups to make sure that the newly added node return to its rightful place in the cluster.

#### Replacing a node on the fly

What happens if a node in the cluster suffers a catastrophic failure? For example, let's assume that node C had a hard
disk failure and went down completely. You restore the machine, but all the cluster data and databases on node C is gone.
What is the best way to handle this scenario?

Because node C effectively suffer from amnesia, the first thing we need to do is to go to the cluster and demote it from
a full member to a watcher. We'll discuss this in more detail in the next section, but the core idea is that we want to
avoid giving node C any decision making power (which, as a full member node, it has) until it recovered from it's bout
of forgetfulness.

Once node C was demoted to a watcher, we can let the clsuter handle the rest of it. Just startup RavenDB again, since 
it has no data, it will start up as a passive node. Because it is using an existing node URL, the cluster will connect 
to it and update it with its current state. That includes any databases that should reside on node C. The other nodes 
that have databases that also reside on C will start replicating the data back to node C. 

In a short order, the cluster will make sure that node C is backup again, has all of the relevant data and is fully up
to speed and can be a contributing member of the cluster. At this point, you can promote it to full member again.
We talked about members and watchers in Chapter 7, but a refresher is probably in order. 

#### Promoting and demoting nodes in the cluster

The nodes in a RavenDB cluster can be in the following states: Member, Promotable and Watcher. A member node is a fully
functional part of the cluster, able to cast votes and be elected as the leader. Typically, a cluster will have up to 
seven memebers at any one time. A promotable member is one that is currently in the process of catching up with the state
of the cluster and is typically only seen when you add a new node to the cluster.

A watcher is a member of the cluster that is managed by the cluster (assigned work, monitored, etc) but have no way of
impacting the cluster. It isn't asked to vote on commands, nor can it be elected as the leader. Once your cluster grow
beyond seven nodes, you'll typically start adding new nodes as watchers instead of full members. This is to reduce the
latency for cluster wide operations by involving a smaller number of nodes.

In Figure 18.1 you can see the `Add node as Watcher` option, which will add a new node to the cluster already as a watcher.
You can also demote member nodes to watchers and promote watchers to be full members. Figure 18.2 shows the `Demote` 
button for node D. 

You'll typically only demote a node when you are recovering from some sort of fatal error that caused amnesia, as was 
discussed in the previous section. Alternatively, you might want to shift the responsability of the cluster to the newer,
more powerful nodes in the cluster. Each of the nodes is exactly the same, the only difference is what role they are 
assigned to. Any watcher node can become a member node and vice versa. 

#### Bring up a cluster from a single surviving node

All the cluster wide operations (adding / removing nodes) require that the cluster itself be healthy. A healthy cluster
is one where a majority of the nodes are able to communicate with one another. There are certain disasterous cases where
that doesn't hold. In a three node cluster, if you lost two of the nodes, the cluster cannot recover from this. When one
of the failed node recover, so will the cluster.

However, what happens if it _can't_ recover? Imagine that you had a hard failure on both of these machines, leaving you
with a sole surviving node. What can you do at that point?
You cannot remove the failed node from the cluster on the surviving node, because there is no majority to confirm this 
decision. 

At this point, we have a nuclear option to resolve this scenario. Unilaterally seceding from the cluster. As you can 
imagine, this is considered to be a _rude_ operation and not something to be handled lightly. This feature exists to 
handle this specific circumstances. It will forcibly update the internal cluster topology on a node, without waiting for
a majority vote on the cluster and create a new single node cluster. 

This can be achieved only throught he Admin JS Console that we discussed in the Chapter 16. Go to `Manager Server`,
`Admin JS Console` and ensure that you are running in the context of the server, then run the command in Listing 18.1.

```{caption="Emergency command to initiate cluster secession on the current node" .js}
return server.ServerStore.Engine.HardResetToNewCluster();
``` 

This command will output the new topology id. And you can now go to the `Cluster` view and see that there is only a 
single node in this cluster now, and that the current node is the leader. At this point, you'll be able to run 
cluster wide operations, such as adding a new node to the cluster.

A small wrinkle here is that RavenDB validate that a node that is added to the cluster is either a brand new node or was
previous part of the same cluster. Because the node has seceded, it is now its own cluster. This can be an issue if you lost
three out of five nodes in a cluster. 

> **Do _not_ pull the emergency handbrake**
>
> This, like anything else provided under the Admin JS Console, is provided with an important caveat. This is modifying the
> internal sturcture of RavenDB in a very invasive way. This isn't meant to be something that you use otherside of the most
> dire of emregnecies.
> 
> You need to verify that this is the only thing that is running on the system while this is going on and execute the 
> sequence of steps exactly as specified. Otherwise, this might result in bad system behavior or unepxected results. 

Losing three nodes means that you don't have a majority and must cause one of the nodes to secede from the cluster. But you
can't have two nodes secede in this manner since each will create their own single node cluster. 
Let's assume that you have only nodes A and B from a five nodes cluster remaining. We can't just move to a single node cluster
so we need to take a slightly more complex series of steps.

On node A, run the `HardResetToNewCluster` command and note the topology id that was provided. On node B, in the Admin JS 
Console you'll need to execute the command shown in Listing 18.2 (remember to update the topology id from the previous step).

```{caption="Emergency cluster secession in favor of a paritcular cluster topology" .js}
server.ServerStore.Engine.HardResetToPassive(
		// the topology id from Listing 18.1 goes here
 		"xxxxxxxx-c4d2-494e-bc36-02274ccc6e4c"
);
```

Listing 18.2 shows the command to secede from the existing cluster in favor of a new one. This will also mark
node B, where this was run, as passive. You'll need to go to node A (where `HardResetToNewCluster` was run) and add node B
to the cluster again.
If the live node aren't calls nodes A and B, by the way, this process will rename them to _be_ node A and B. 

At this point, you'll have a two nodes cluster and can start adding brand new nodes as usual. This process is only valid to
run on nodes that are part of the same cluster. It is not possible to use this to merge clusters and in general this should
be treated as an option of last resort. 

### Database recipes

#### Reordering nodes and why it matters

#### Moving a database between nodes

#### expanding and shrinking database group

#### Offline query optimization

#### Scaling database under load

#### Disconnect external replication / ETL from a remote source

### Reducing indexing load with priorities

### Controling changes to indexes with locking

