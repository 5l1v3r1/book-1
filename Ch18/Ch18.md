
## Operational recipes

This chapter is going to be a short one, mostly consisting of walk throughs of particular tasks that you might need to 
do as part of operating a RavenDB cluster. This isn't meant to replace the online documentation but complement it. 
I tried to cover most of the common scenarios that you might run into in your systems. Beyond using this as a reference
I also suggest going through the various recipes to get some insight about the manner in which such issues are resolved.

Many of the methods listed in this chapter aren't new, we have covered them in previous chapters. Here they are just 
laid out in practical terms as a set of operations to achieve a specific goal.

Before we start, I want to mention something quite improtant. All of this recipes are meant to be used to solve the 
problem in which they are introduced. However, they often include deep assumptions about what is going on and it is
_not_ safe to blindly apply the techniques described here for other situations or without following the exact same
procedures listed here.

This chapter is where we learn how to make fixes to the system while it is running. RavenDB was explicitly designed
to allow such things. However, it is still modifying a live system, often in situations that it is already stressed
or in partial / full failure mode. The recipes here will help you get out of such scenarios, but mistakes can cause
hard failures.

Now that I've finished scaring you, let's us get to the real meat of this chapter. I included specific warnings for
common issues that you might want to pay attention to and you'll probably conclude that it isn't _that_ scary. 

### Cluster recipes

The cluster is the workhorse that users of RavenDB rarely think about. The cluster is responsible for managing all the 
nodes in the cluster, assign work, monitor the health of the nodes, etc. Most of the time, you setup the cluster once
and then let it do its thing.

This section is going to talk about when you need to go in and interve. The usual reasons for that is when you want to 
modify your cluster topology. The cluster topology isn't meant to be static and RavenDB explicitly support online 
modifications of the topology, so there isn't a high level of complexity involved. There are some details thah you should
take into account, though.

#### Adding a new node to the cluster

You might have started with a single node or have few members in the cluster already, but at some point, you'll want to
add a new node to your cluster. Most of the time, you setup the cluster itself as a whole using the setup wizard, so you
might not even be aware how to do this task.

> **Prerequisites for the new node**
>
> I'm assuming that you are running in a secured mode (the only reason you aren't is if you are playing around on the 
> local machine). That means that in order to add the new node, you'll need to have mutual trust between the new node
> and the existing cluster. The easiest way to do is to use a single wildcard certificate for all the nodes in 
> the cluster.
> In this way, because the new node it using the same certificate as the rest of the cluster, it will intrisinctly
> trust the remote node from cluster and accept its addition to the cluster.
>
> If you don't have the same certificate for all the nodes, you'll need to register the cluster certificate in the new
> node using (`rvn admin-channel` and then the `trustServerCert` command). 

You'll have to take care of the certificate (it is easiest to use a single wildcard certificate for all the nodes in the 
cluster, although that is not required), DNS updates, firewall configuration, etc. I'm assuming that all of this is all
properly setup ahead of time. There is nothing special about that part of the preparation.

When you start a new node for the first time, it defaults to using `Setup.Mode=Initial` and will start the setup wizard.
This is _not_ what you want in this case. Take a look at the `settings.json` file from one of your existing cluster nodes
and use that as the template for the configuration of the new node. You'll have to change the `ServerUrl`, 
`PublicServerUrl`, `ServerUrl.Tcp` and `PublicServerUrl.Tcp` options to match the new node location. Beyond that, keep the
configuration identical. While it is _possible_ to run with different configuration on different nodes, it is not 
recommended and too high a variance of some options will cause errors. 

One you are done with the configuration, you can start the new node. You can can connect to it and you'll see the studio
as usual, but there are no databases or documents in there, obviously. In fact, this new node is currently running in 
`passive` mode. In this mode, it is not yet determined whether the node will be joined to an existing cluster or form
a completely new cluster.
A new cluster will be formed if you create a new database or setup client certificates. In general, anything that shows
that you are preparing this node to stand on its own. 

Once you have verified that the new node is up, running and accessible (but don't perfom any actions on the new node) go
to the _existing_ node and head to `Manage Server`, `Cluster` and then to `Add Node to cluster`. You can see how this 
will look in Figure 18.1.

![Adding a new node to an existing cluster](./Ch18/img01.png)

Figure 18.1 shows the new node screen. There isn't much there, because you don't need to do much beyond provide the URL
of the new node to add to the cluster. Once you click on the `Add` button, the cluster will connect to the new node, 
add it to the topology and start updating it with the current state of the system. During this period, the node will
show up in the `Promotable` state. 

In the `Promotable` state, the node is part of the cluster in the sense that it is getting updates, but it is not actually
taking part in votes or applicable to run for the cluster leadership. When the cluster is finished updating the new node
with the existing cluster state, the cluster will move the node into the `Member` state. At this point, the new node will
be able to become a leader and its vote will be counted by the cluster. 

At this point, the node is added to the cluster and can become the leader. However, it is still missing something. It has
no databases. RavenDB will not auto move databases between nodes when you add or remove a node from the cluster. This is
left to the discretion of the operations team. New databases created on the cluster will take the new node into account 
and the cluster may decide to migrate databases from a failed node to the new node, but these are rather exceptional
circumstances.

You'll typically wait until the node is fully joined to the cluster and then tell RavenDB to start migrating specific 
databases to the new node yourself. Remember that such a process is transparent to the client and will not impact the
usual operations in any manner.

#### Removing a node from the cluster

The flip side of adding a now to the cluster is removing a node. The _how_ of it is pretty simple, as you can see in 
Figure 18.2.

![Cluster node in the `Cluster` view, the red trash icon allows you to remove the node from the cluster](./Ch18/img02.png)

In Figure 18.2 you can see the nodes view in the studio. The red trash icon at the top right allows you to remove the node
from the cluster. Clicking on this will have cause the cluster to remove the node from the cluster. What does this mean?

At the cluster level, it means that we'll remove the node, obviously. Any database groups that contain this node will have
the node removed from the database group (and the replication factor adjusted accordingly). Any databases that reside only
on this node will be removed from the cluster entirely.

At the removed node level, it will revert back to passive mode. In this mode, the node is still accessible, but the 
databases that reside on this node are unloaded. This is because the node is in passive mode. You'll need to either re-join
the node to the cluster or let it know that it should form a new cluster before the databases on this node become available
again.
This is done to avoid clients or ETL tasks from talking to the now isolated database instance on the node that was removed
from the cluster. 

Forming a new cluster is a one way option. After the new node was moved from passive mode, you cannot add it back to the 
previous cluster (or any other, for that matter). A cluster can only add either empty nodes or nodes that were previously
attached to the same cluster.

When a new cluster is formed on the formerly passive node, the topology of all the database groups is adjusted. All the 
database groups that include the current node are shrank to include just the current node. All the database groups that
do not contain the current node are removed. 

> **Don't create a new cluster with the old URL**
>
> Forming a new cluster on the removed node is fine, but you should make sure that this is done with a _new_ URL. This
> is to avoid the case of a client using an old URL or an ETL task that hasn't been updated. The new cluster will share
> the same security configuration and you'll want to avoid existing clients and tasks talking to the newly independent
> node while thinking that they are talking to the cluster as a whole.

All other cluster wide settings, such as the certificates and database access authorization remain the same. You should
take that into account if you are intending to keep the node around after removing it from the cluster.

#### Migrate a node between machines

Node migration can happen for any number of reasons. The most obvious one is that the server IP has changed, and you need
to update the cluster topology. For this reason, among many others, it is not recommended to use raw IPs in the cluster
topology. Instead, use proper URLs and DNS to control the name resolution.

In such a way, moving node C from `10.0.0.28` to `10.0.0.32` can be done simply by updating the DNS of `c.prod.rvn` to the 
new value and forcing a DNS flush. You can do that with machine names, of course, but that would just move the problem if 
you need to change the host machine. A good example is if you setup a RavenDB node on `prodsrv1` and you need to move it to 
`prodsrv3`. At the same time, `prodsrv1` is used for other services, so you cannot just change the DNS.

> **Use dedicated DNS entries for RavenDB nodes**
>
> Avoid using IPs or hostnames for RavenDB nodes. The easiest options is to have a DNS entry for each RavenDB node that
> can be changed directly if needed. This can make changing the phyiscal topology of the cluster as easy as running a
> DNS update. This section is how to handle the case when you did _not_ do that.

I'm assuming that you have to move a node here by updating the cluster topology, not just updating the DNS (which is the 
recommended mode). There are a few things that you need to do to ensure that the transition is handled properly. Starting
from obtaining a certificate for the new node URL. Once that is done, the process is simple. 

We'll assume that you have a cluster of three nodes (A,B and C) and that you want to move node C to a different machine. 
The first thing to do is to shut down node C. Then go to one of the other nodes and remove node C from the cluster. You
can also do things in the reverse order, first remove node C and then shut it down, it doesn't really matter.

Move node C to another machine, setup the certificate and update the `ServerUrl`, `PublicServerUrl`, `ServerUrl.Tcp` and 
`PublicServerUrl.Tcp` options in the `settings.json` file. Then go to node A or B and follow the same procedure to add
a new node to the cluster. The new will be re-added (still called node C). RavenDB remembers the node and will perform
all the necessary hookups to make sure that the newly added node return to its rightful place in the cluster.

#### Replacing a node on the fly

What happens if a node in the cluster suffers a catastrophic failure? For example, let's assume that node C had a hard
disk failure and went down completely. You restore the machine, but all the cluster data and databases on node C is gone.
What is the best way to handle this scenario?

Because node C effectively suffer from amnesia, the first thing we need to do is to go to the cluster and demote it from
a full member to a watcher. We'll discuss this in more detail in the next section, but the core idea is that we want to
avoid giving node C any decision making power (which, as a full member node, it has) until it recovered from it's bout
of forgetfulness.

Once node C was demoted to a watcher, we can let the clsuter handle the rest of it. Just startup RavenDB again, since 
it has no data, it will start up as a passive node. Because it is using an existing node URL, the cluster will connect 
to it and update it with its current state. That includes any databases that should reside on node C. The other nodes 
that have databases that also reside on C will start replicating the data back to node C. 

In a short order, the cluster will make sure that node C is backup again, has all of the relevant data and is fully up
to speed and can be a contributing member of the cluster. At this point, you can promote it to full member again.
We talked about members and watchers in Chapter 7, but a refresher is probably in order. 

#### Promoting and demoting nodes in the cluster

The nodes in a RavenDB cluster can be in the following states: Member, Promotable and Watcher. A member node is a fully
functional part of the cluster, able to cast votes and be elected as the leader. Typically, a cluster will have up to 
seven memebers at any one time. A promotable member is one that is currently in the process of catching up with the state
of the cluster and is typically only seen when you add a new node to the cluster.

A watcher is a member of the cluster that is managed by the cluster (assigned work, monitored, etc) but have no way of
impacting the cluster. It isn't asked to vote on commands, nor can it be elected as the leader. Once your cluster grow
beyond seven nodes, you'll typically start adding new nodes as watchers instead of full members. This is to reduce the
latency for cluster wide operations by involving a smaller number of nodes.

In Figure 18.1 you can see the `Add node as Watcher` option, which will add a new node to the cluster already as a watcher.
You can also demote member nodes to watchers and promote watchers to be full members. Figure 18.2 shows the `Demote` 
button for node D. 

You'll typically only demote a node when you are recovering from some sort of fatal error that caused amnesia, as was 
discussed in the previous section. Alternatively, you might want to shift the responsability of the cluster to the newer,
more powerful nodes in the cluster. Each of the nodes is exactly the same, the only difference is what role they are 
assigned to. Any watcher node can become a member node and vice versa. 

#### Bring up a cluster from a single surviving node

All the cluster wide operations (adding / removing nodes) require that the cluster itself be healthy. A healthy cluster
is one where a majority of the nodes are able to communicate with one another. There are certain disasterous cases where
that doesn't hold. In a three node cluster, if you lost two of the nodes, the cluster cannot recover from this. When one
of the failed node recover, so will the cluster.

However, what happens if it _can't_ recover? Imagine that you had a hard failure on both of these machines, leaving you
with a sole surviving node. What can you do at that point?
You cannot remove the failed node from the cluster on the surviving node, because there is no majority to confirm this 
decision. 

At this point, we have a nuclear option to resolve this scenario. Unilaterally seceding from the cluster. As you can 
imagine, this is considered to be a _rude_ operation and not something to be handled lightly. This feature exists to 
handle this specific circumstances. It will forcibly update the internal cluster topology on a node, without waiting for
a majority vote on the cluster and create a new single node cluster. 

This can be achieved only throught he Admin JS Console that we discussed in the Chapter 16. Go to `Manager Server`,
`Admin JS Console` and ensure that you are running in the context of the server, then run the command in Listing 18.1.

```{caption="Emergency command to initiate cluster secession on the current node" .js}
return server.ServerStore.Engine.HardResetToNewCluster();
``` 

This command will output the new topology id. And you can now go to the `Cluster` view and see that there is only a 
single node in this cluster now, and that the current node is the leader. At this point, you'll be able to run 
cluster wide operations, such as adding a new node to the cluster.

A small wrinkle here is that RavenDB validate that a node that is added to the cluster is either a brand new node or was
previous part of the same cluster. Because the node has seceded, it is now its own cluster. This can be an issue if you lost
three out of five nodes in a cluster. 

> **Do _not_ pull the emergency handbrake**
>
> This, like anything else provided under the Admin JS Console, is provided with an important caveat. This is modifying the
> internal sturcture of RavenDB in a very invasive way. This isn't meant to be something that you use otherside of the most
> dire of emregnecies.
> 
> You need to verify that this is the only thing that is running on the system while this is going on and execute the 
> sequence of steps exactly as specified. Otherwise, this might result in bad system behavior or unepxected results. 

Losing three nodes means that you don't have a majority and must cause one of the nodes to secede from the cluster. But you
can't have two nodes secede in this manner since each will create their own single node cluster. 
Let's assume that you have only nodes A and B from a five nodes cluster remaining. We can't just move to a single node cluster
so we need to take a slightly more complex series of steps.

On node A, run the `HardResetToNewCluster` command and note the topology id that was provided. On node B, in the Admin JS 
Console you'll need to execute the command shown in Listing 18.2 (remember to update the topology id from the previous step).

```{caption="Emergency cluster secession in favor of a paritcular cluster topology" .js}
server.ServerStore.Engine.HardResetToPassive(
		// the topology id from Listing 18.1 goes here
 		"xxxxxxxx-c4d2-494e-bc36-02274ccc6e4c"
);
```

Listing 18.2 shows the command to secede from the existing cluster in favor of a new one. This will also mark
node B, where this was run, as passive. You'll need to go to node A (where `HardResetToNewCluster` was run) and add node B
to the cluster again.
If the live node aren't calls nodes A and B, by the way, this process will rename them to _be_ node A and B. 

> **Which node should be made passive?**
> 
> In such a disaster scenario, it is not uncommon to have the different surviving node each with their own specific
> view of the state of the cluster and the command log. It is imperative that you'll select the node with the most
> up to date command log to be the one to reset to a new topology. 
> 
> You can use the `/admin/cluster/log` endpoint to check the status of the command log on each of the nodes. The node
> with the highest `CommitIndex` and the latest log in the `Entries` array is the one you should run `HardResetToNewCluster`
> on and `HardResetToPassive` should be run on the other(s).

At this point, you'll have a two nodes cluster and can start adding brand new nodes as usual. This process is only valid to
run on nodes that are part of the same cluster. It is not possible to use this to merge clusters and in general this should
be treated as an option of last resort. 

### Database recipes

So far we talked a lot about all the things that happen to RavenDB at the cluster level. This is important to understand but
it is not something that you'll use on a day to day basis. It is far more common to have to deal with operations at the 
individual database level.

To clarify the terminology, a database in RavenDB can refer to a database group (a named database in the cluster, which resides
on one or more nodes) or a database instance (a named databased on a particular node in the cluster). We don't usually have to
distinguish between them because the process is transparent. We can look at the databases from any node and we don't need to
specify which node we are talking to for each database from the client side.

This is all thanks to the fact that RavenDB stores the database group topology on all the nodes in the cluster. This is what
allows any node in the cluster to tell a client where to find the database.

#### Reordering nodes and why it matters

You can see the database group topology in the studio by clicking on the `Manage group` button on the `Databases` page. Doing 
this will take you to the `Manage Database Group` page, as you can see in Figure 18.3.

![Examining the database group topology in the Studio](./Ch18/img03.png)

If you look closely at Figure 18.3 you might notice something odd. The order of the nodes there is _wrong_. It goes `[A, C, B]`
but _obviously_ it should be sorted, no? What is going on?

The order of the elements in the topology matters, and RavenDB allows you to control that using the `Reorder nodes` button. But
what is so important about the order of the nodes? Put simply, this is the priority list that the clients will use when deciding
which node in the database group they will talk to. 

Usually, RavenDB will manage the list on its own, deciding the order in which nodes should talk to the different nodes as well
as the other tasks that are assigned to this database group. If the cluster detects that a node is down, it will drop it to the
bottom of the list and let clients know about this.

Clients, for their part, will use the list to decide what node to call whenever they need to query RavenDB. If the node they 
chose is down, they will automatically failover to the next node in the list. Note that in this case, both cluster and the 
clients are working cooperatively and independently of one another. The cluster gives its opinion on the best node to use at
any given point. The client will use that list, but may face different conditions. If the cluster is able to reach a node and
the client isn't, the client can still proceed to failover to the other nodes in the topology.

Clients will update their topology of the database group on the next request after the topology has changed. This is done by
the server setting a header in the response of a request that let the client know that the server topology have changed. In 
practice, this means that changing topologies are usually visible to all clients within a very short amount of time.

That, in turn, leads to interesting options. For example, you can use this feature to shape the way the clients will talk to
a particular node. You can move it to the bottom of the list to have the clients avoid talking to it (assuming no round robin
or fastest node options are in place). You can also move it to the top so the clients will prefer to use that particular node.

This is useful if you want to take a node down and want to gracefully move traffic away from it instead of having clients fail
(and then recover by failing over to another node in the database group). 

#### Moving a database between nodes

A database instance is the term used to refer to a specific database inside a particular node. Sometimes, for whatever reasons,
you want to move a database instance between nodes. There are a few ways of doing this, from the easy (letting RavenDB do that
for you) to the manual (when you do everything). We'll discuss the easy way to do things next, right now I want to focus on 
the manual mode.

If there is an easy way, why should we go to the trouble of doing this manually? Mostly because in some _specific_ cases (by
no means all of them, mind) it can be faster to do things directly. It is also a really exposure to how RavenDB is actually
managing databases internally and may be helpful in other places.

The way RavenDB is actually managing your databases across the cluster is interesting. At the cluster level, RavenDB coordinate
between the various nodes to achieve consensus on the `Database Record`. The `Database Record` is a JSON document that describe
that database itself. You can see it by going into one of the databases then to `Settings`, `Database Record`. Listing 18.3 
shows a (simplified) example of such a database record.

```{caption="A (somewhat simplified for space reasons) database record defining a database in RavenDB" .json}
{
    "DatabaseName": "Northwind.Orders",
    "Disabled": false,
    "Encrypted": false,
    "Topology": {
        "Members": [
            "A",
            "B"
        ],
        "DynamicNodesDistribution": false,
        "ReplicationFactor": 2
    },
    "Indexes": {
        "Orders/ByCompany": {
            "Name": "Orders/ByCompany",
            "Priority": "Normal",
            "LockMode": "Unlock",
            "Maps": [
                "from order in docs.Orders /* redacted */"
            ],
            "Reduce": "from result in results /* redacted */",
            "Type": "MapReduce"
        }
    },
    "Revisions": {
        "Collections": {
            "Orders": {
                "Disabled": false,
                "PurgeOnDelete": false
            }
        }
    },
    "PeriodicBackups": [],
    "ExternalReplications": [],
    "RavenEtls": [],
    "SqlEtls": [],
    "Etag": 113
}
```

What you see in Listing 18.3 is what goes on behind the scenes. You have the database topology (the members for the Database
Group), you have the index definitions, revisions configurations, and you can see where we would define tasks for the database.
When I talk about the cluster managing the database, what ends up happening is that the cluster mutate this document and ensures
that all the nodes in the cluster have a consistent view of it.

> **Pay no attention to the man behind the curtain**
> 
> There is no magic. RavenDB uses the `Database Record` to tell where database needs to go, and a node will create an instance
> of the database when it is assigned to the node. A database instance is the set of files, threads and in memory data structures
> required to handle queries and operations.

The database instance managed at the node level is, to use a very crude resolution, just the set of files that make up the database
data on that node. How does all of this relates to moving a database between nodes? Well, let's see how we can take advantage of this
behavior to move a database manually between nodes.

The first thing to do is to actually remove the database from the origin node. You can either soft delete the database (if it exists
on only a single node) or remove it (using soft delete) from the Database Group. This can be done in the `Manage Database Group` view
under the database's `Settings`. You can see how this would look like in Figure 18.4.

![Removing a database from a node can be done using soft-delete, leaving the database files untouched](./Ch18/img04.png)

This soft delete measure leave the database files on the disk. Further more, it also means that the RavenDB server on the origin node
will close all the file handles and release any resources associated with this database. At this point (and at this point _only_) we
can take the database folder and move it to another machine.

> **Don't make a habit of schlepping databases around manually**
>
> It is _not_ safe to generally much about in the RavenDB directory. That is true for users in general as well as other things.
> Stuff like anti viruses, file system indexing, file monitoring, etc. RavenDB firm ideas about how it interacts in the disk 
> and it expects things to match what is expected. Interfering with this can cause issues.
>
> In this case, we have started off by explicitly shutting down the database (by soft deleting it). This gives RavenDB the 
> chance to do graceful shutdown, close all the file handles and free all related resources. That is the only reason it is OK
> for us to muck about in RavenDB's data directory.

The "move it to another machine" is the key reason why we went to all this trouble. This presupposes that you have some fast way to
get the data from one machine to the other more quickly than sending it over the network. A common scenario where this is actually
achivable is when you have a large database and it is faster to literally move the disk from one machine to another.

Another such scenario is when your "disk" is actually a cloud storage volume. This allow you to detach it from one machine and the
attach it to another easily enough. In that case, it might be worth this hassle. Otherwise, just use RavenDB's builtin replication
for this scenario.

> **This is _not_ going to work to clone a database**
>
> You might be tempted to use this approach to quickly clone a new database when you want to add a new node to the database group.
> This will not work because the internal database id will be the same across multiple nodes, something that is not allowed and 
> can cause complex issues down the road. RavenDB will not be able to tell where exactly a specific change happen, so it will not
> be able to tell if documents should be replicated to the other nodes or not. 
> 
> This approach is only valid if you remove the data from the old node entirely, so there is just one copy of the database with
> the same database id. 

Once you have the database directory on the other machine, make sure that it is in the expected path on the destination and just 
add it to the new node. On update of the `Database Record` RavenDB will open the directory and find the pre-existing files 
there. From that point on, it will just proceed normally. 

#### Renaming a database

Given the previous topic, can you imagine how you'll rename a database with RavenDB? 

This seems quite easy to do. All you need to do is soft delete the database and then re-create the database with a new name
and the same path. 

There are a few things that you need to pay attention to. Only the _database level_ data is going to be preserved in this 
scenario. Tasks and data at the cluster level will _not_ be retained in this scenario. This includes tasks such as subscriptions,
backups, ETLs, etc. It also include database _data_ that is stored at the cluster level such as identities and compare exchange values.

In short, don't do this. If you want to rename a database, the proper way to do this is to backup and restore it under a different name.

#### Expanding and shrinking the Database Group

I mentioned that there is an easy way to move a database between nodes earlier. This way is quite simple, first expand the Database
Group to include the new node, wait for replication to complete and then remove the node you want from the group. 

You can do this from the `Settings`, `Manage Database Group` view. Expanding the Database Group to include a new node is as simple
as clicking on `Add node to group` and selecting which node you want to add. Once that is done, the RavenDB cluster will create the
database on the new node and assign one of the existing nodes in the Database Group for the initial seeding of data. 

The `Manage Database Group` view will allow you to monitor the status of the initial seeding until the cluster detects that the 
new node is up to speed and promote it to a full member status. At this point, you can remove another node from the Database Group
and the cluster will adjust. 

Following up what we already know, you'll probably first move the node that is about to be removed from the Database Group lower
in the priority and wait a bit to give clients the chance to learn about the new topology and connect to another server. This way,
when you remove the node, no client will be talking to it.

> **Removing the last node in the group will delete the database**
> 
> Removing a node will can be done as either soft delete (keep the files, as we previously) discussed or hard delete. Even if you 
> chose a soft delete, if this is the last node in the Database Group, the database will be removed from the cluster. The same rules
> about the cluster level data apply as when we talked about the wrong way to rename a database. 
> 
> A soft delete of the last node in a Database Group will keep the database level data intact on that node, but all the cluster data
> (tasks, identities, comcpare exchange values, etc) will be removed from the cluster. 

The advantage of this method for the operations team is obvious. There is very little hand holding that is actually required. 
Exapnding the group means that RavenDB will handle all the details of moving the data to the other node(s) and setting everything up.

The downside is that it requires RavenDB to do a fair amount of work (effectively, read the entire database and send it over the
network) and on the other side RavenDB will have to write out the entire database. This is intentionally handled in such a way that
will not impact overall operations too much. The idea is that the stability of the cluster is more important than raw speed. 

This might sound like we intentionally throttle things down, but nothing could be further from the truth. It is just that we are
careful to ensure that adding a new node isn't going to consume so many resources that it is prohibitively expensive. After all,
the most common reason to want another node is that we want to share an already high load.

#### Scaling database under load

RavenDB contains several features that are intended to allow you to scale your systems easily. Among them we have the distribution of
tasks across the nodes in the Database Group and the ability to do load balancing between the nodes. 

Assuming that you start seeing very high load on your system, what options do you have? The first thing to do is to look at the kind 
of client configuration you have. This is available in `Settings`, `Client Configuration`, as shown in Figure 18.5.

![The client configuration allow you to change the load balancing behavior of clients on the fly](./Ch18/img05.png)

Client configuration, just like the cluster topology, is defined on the cluster and disseminated to clients automatically. This means
that if this isn't set, the first thing you can do is to set the `Read balance behavior` to `Round Robin` and wait a bit. You'll see
how clients start distributing their reads across the entire Database Group.

If you do have this value already setup and you seem RavenDB struggling even with the load split among the different nodes in 
the Database Group, the obvious next step is to add new nodes to the group. This specific scenario is why RavenDB favors stability
when you add a new node to the database group. Only a single node in the group will be in change of updating the new node and even on
that node we'll ensure that we send the data at a rate that both sender and reciever can handle without choking. 

So it is possible to do this under load, but quite obviously not recommended. If you have a large database, the initial seeding may 
take a while. During this time, you don't have any extra capacity and in fact the replication is actually taking (some limited amount)
of system resources. If at all possible, try doing this sort of things ahead of time or during idle periods.

If you do find yourself in this situation, remember that beause the initial seeding of a new node happens from a single designated node, 
you can actually add _multiple_ nodes to the Database Group at the same time and they will be assigned to different nodes. In some 
cases, that can significantly help reduce the time to increase the overall capacity.

### Miscellaneous operational tasks

#### Offline query optimization

#### Disconnect external replication / ETL from a remote source

### Reducing indexing load with priorities

### Controling changes to indexes with locking

