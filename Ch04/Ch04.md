
# Deep dive into the RavenDB Client API

In this chapter we are going to take a deep dive into how the client API works. We are going to show mostly C# code examples, but the 
same concepts apply to any of the RavenDB Client APIs, regardless of platforms, with minor changes to make it fit the platform.

There are still some concepts that we haven't gotten around to (clustering or indexing, for example) which will be covered in 
their own chapters. But the Client API is very rich and has a lot of useful functionality on its own, quite aside from the server
side behavior. 

We already looked into the document store and the document session, the basic building blocks of CRUD in RavenDB. But in this chapter we
are going to look beyond the obvious and into the more advanced features. One thing we'll _not_ talk about in this chapter is querying.
We'll talk about that extensively in [Chapter 9](#map-indexes), so we'll keep it there. You already know the basic of querying in RavenDB
but there is a _lot_ more power waiting for you to discover there.

This chapter is going to contain a _lot_ of code examples and discuss the nitty gritty details of using the client. It is also divided into 
brief sections that each deal with a specific feature or behavior. I suggest reading over this to note what the capabilities of RavenDB are
and coming back to it as needed in your application.

For the rest of this chapter, we'll use the classes shown in Listing 4.1 as our model, using a simplified help desk as our example. 

```{caption="Simplified Help Desk sample model" .cs}
public class Customer
{
	public string Id { get; set; }
	public string Name { get; set; }
}

public class SupportCall
{
	public string Id { get; set; }
	public string CustomerId { get; set; }
	public DateTime Started { get;set; }
	public DateTime? Ended { get;set; }
	public string Issue { get; set; }
	public int Votes { get; set; }
	public List<string> Comments { get; set; }
}
```

## Writing documents

Writing documents in RavenDB is easy, as we saw in [Chapter 2](#zero-to-ravendb). If we want to create a new support call, we can use the code in 
Listing 4.2 to do so.

```{caption="Creating a new support call using the session" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);
	session.SaveChanges();
}
```

This is basic behavior of RavenDB, and how you would typically work with saving data. But there are lot of additional things that we can do when
writing data. For example, the user might have sent us some screen shots that we want to include in the support call. 

### Working with attachments

You can add attachments to a RavenDB document to store binary data relating to that document. Let us assume that the user have sent us a screen shots
of the problem along with the call. Listing 4.3 shows how we can store and retrieve the attachments.

```{caption="Saving attachments to RavenDB as part of opening the support call" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);

	foreach(var file in attachedFiles)
	{
		session.Advanced.StoreAttachment(call, file.Name, 
			file.OpenStream());
	}

	session.SaveChanges();
}
```

Note that we are using the session to store both the support call document and any attachments that the user might have sent. An attachment is basically
a file name and a stream that will be sent to the server (with an optional content type). When the call to `SaveChanges` is made, the RavenDB Client API
will send both the new document and all of its attachments to the server in a single call, which will be treated as a transaction. Both the document and
the attachments will be saved, or both will fail.

That was easy enough, but how do we get those back? The list of attachments for a particular document is accessible via the document metadata, as shown
in Listing 4.4. 

```{caption="Getting the list of attachments for a support call" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  var attachments = session.Advanced.GetAttachmentNames(call);

  // render the call and the attachment names
}
```

Calling `GetAttachmentNames` is cheap, since the attachments on a document are already present in the document metadata, which we loaded as part of
getting the document. There is no server side call involved. Note that the result of `GetAttachmentNames` does not include the _content_ of the 
attachments. To get the attachment itself, and not just its name, you need to make a separate call, as shown in Listing 4.5.

```{caption="Getting an attachment content" .cs}
using(var session = store.OpenSession())
{
	var call = session.Load<SupportCall>("SupportCalls/238-B");
	var attachments = session.Advanced.GetAttachmentNames(call);

 	using(var stream = session.Advanced.GetAttachment(call, 
 		attachments[0].Name))
 	{
		// process the content of the attachment 	
 	}
}
```

Each call to `GetAttachment` will make a separate call to the server to fetch the attachment, if you have a lot of attachments, be aware that 
fetching all of their information can be expensive due to the number of remote calls that are involved. 

### Working with the document metadata

In the attachments section, we noted that attachment information is stored in the document metadata. RavenDB used the metadata for a lot of things,
most of them you don't generally care about (etag, change vector, etc). But the document metadata is also available to you for your own needs and 
use. 

Actual use case for direct use of the document metadata are actually pretty rare. If you want to store some information, you'll typically want to 
store it in the document itself, not throw it to the sidelines in the metadata. Typical use cases for storing data in the medata are corss cutting
concerns. The preeminent one is auditing (who editted this document, for example). 

In order to demonstrate working with the metadata, we'll consider the case that creating a support call is a complex process that has to go through
several steps. In this case, while we saved the document to RavenDB, it is still in draft status. Typical modeling advice would be to model this 
explicitly in the domain (so you'll have a `IsDraft` or `Status` property on your model), but for this example, we'll use the metadata. You can see
the code for setting a draft status in the metadata in Listing 4.6.

```{caption="Setting a metadata flag as part of creating a new support call" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);

	var metadata = session.Advanced.GetMetadataFor(call);
	metadata["Status"] = "Draft";
	
	session.SaveChanges();
}
```

We can call `GetMetadataFor` on any document that has been associated with the session. A document is associated with the session either by loading
it from the server or by calling `Store`. After the document has been associated with the session, we can get its metadata and manipulate it.

Changes to the metadata count as changes to the document and will cause the document to be saved to the server when `SaveChanges` is called. 

### Change tracking and `SaveChanges`

The document session implements change tracking on your documents as you can see in Listing 4.7.

```{caption="Setting a metadata flag as part of creating a new support call" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  
  call.Ended = DateTime.UtcNow;

  session.SaveChanges();
}
```

The change tracking (and identity map) by the session means that you don't have to keep track of what changed and manually call `Store`. Instead, when
you call `SaveChanges` all you changes will be sent to the server in a single request. 

You have a few knobs available to tweak the process. `session.Advacned.HasChanges` will let you know if calling `SaveChanges` will result in a call to the 
server and `session.Advanced.HasChanged(entity)` will tell you whatever a particular `entity` has changed. You can also take it up a notch and ask RavenDB
to tell you _what_ changed using `session.Advanced.WhatChanged()`, which will give you all the changes that happened in the session. The `WhatChanged` 
feature can be very nice if you want to highlight changes for user approval, for example, or just want to see what modification happened in your model
after a certain operation.

You can also tell RavenDB to not update a particular instance by calling `session.Advanced.IgnoreChangesFor(entity)`. The document will remain attached to
the session, and will be part of any identity map operations, but it won't be saved to the server when `SaveChanges` is called. Alternatively, you can call
`session.Advanced.Evict(entity)` to make the session completely forget about a document. 

All of those operations tend to be very rare and are usuallly used only in very specific cases, but they are very powerful when utilized properly. 

### Optimistic concurrency

We covered optimistic concurrency in [Chapter 3](#modeling), but only in the most general terms. Now, let us take a look at see how we can use optimistic
concurrent in practice. Listing 4.8 shows two simultaneous sessions modifying the same support call. 

```{caption="Concurrent modifications of a support call" .cs}
using(var sessionOne = store.OpenSession())
using(var sessionTwo = store.OpenSession())
{
  var callOne = sessionOne.Load<SupportCall>("SupportCalls/238-B");
  var callTwo = sessionTwo.Load<SupportCall>("SupportCalls/238-B");
  
  callOne.Ended = DateTime.Today;
  callTwo.Ended = DateTime.Today.AddDays(1);
  
  sessionOne.SaveChanges();
  sessionTwo.SaveChanges();
}
```

In the case of the code in Listing 4.8, we are always going to end up with the support call end date set to tomorrow. This is because by default, RavenDB
uses the `Last Write Win` model. You can control that by setting `store.Conventions.UseOptimisticConcurrency` to true, which will affect all sessions, or
on a case by case basis by setting `session.Advanced.UseOptimisticConcurrency` to true on the session directly.

In either case, when this flag is set, when `SaveChanges` is called, we'll send the modified documents to the server alongside their etag at the time we
read them from the server. This allows the server to reject any stale write. If the flag was set to true, the code in Listing 4.8 will result in a 
`ConcurrencyException` on the `sessionTwo.SaveChanges()` call.

This ensures that you cannot overwrite changes that you didn't see and if you set `UseOptimisticConcurrency` you need to handle this error in some manner.

#### Pessimistic locking

Optimistic locking handle the issue of changes to the document we modified that happened behind our backs. Pessimistic locking prevents them entirely. 
RavenDB does _not_ support pessimistic locking, but while you really need support from the database engine to properly implment it, we can pretend to
have something that is near enough to be interesting to talk about, if not actually use. The following is recipe for using
pessimistic locking in RavenDB, not so much because it is a good idea, but because it allows us to explore several different features and see how they
all work together.

Using pessimistic locking we lock a document for modification until we release the lock (or a certain timeout has gone by). We can build a pessimistic 
lock in RavenDB by utilizing the document metadata and optimistic concurrency. It is easier to explain with code and you can find the `Lock` and `Unlock`
implementations in Listing 4.9.

> **The locks are opt in**
>
> In RavenDB, both the pessimistic lock explored in this section and the optimistic lock in the previous section are opt in. That means that you have to
> explicitly participate in the lock. If you are using `UseOptimisticConcurrency` and another thread isn't, that thread will get the `Last Write Wins`
> behavior (and might overwrite the changes made by the thread using optimistic conccurency).
>
> In the same manner, the pessimistic lock recipe described here is depedant on all parties following it, if there is a thread that isn't, the lock will
> not be respected.
>
> In short, when using concurrency control, make sure that you are using it in all acrodd the board, or it may not hold.

```{caption="Extension method to add pessimistic locking to the session" .cs}
public static IDisposable Lock(
	this IDocumentSession session, 
	string docToLock)
{
  var doc = session.Load<object>(docToLock);
  if (doc == null)
      throw new DocumentDoesNotExistException("The document " + 
      	docToLock + " does not exists and cannot be locked");
  var metadata = session.Advanced.GetMetadataFor(doc);
  if (metadata.GetBoolean("Pessimistic-Locked"))
  {
      // the document is locked and the lock is still value
      var ticks = metadata.GetNumber("Pessimistic-Lock-Timeout");
      var lockedUntil = new DateTime(ticks);
      if (DateTime.UtcNow <=  lockedUntil)
      	throw new ConcurrencyException("Document " + 
      		docToLock + " is locked using pessimistic");
  }
  
  metadata["Pessimistic-Locked"] = true;
  metadata["Pessimistic-Lock-Timeout"] = 	
  	DateTime.UtcNow.AddSeconds(15).Ticks;
  
  // will throw if someone else took the look in the meantime
  session.Advanced.UseOptimisticConcurrency = true;
  session.SaveChanges();
  return new DisposableAction(() =>
  {
      metadata.Remove("Pessimistic-Locked");
      metadata.Remove("Pessimistic-Lock-Timeout");
      Debug.Assert(session.Advanced.UseOptimisticConcurrency);
      session.SaveChanges();
  });
}
```

There is a quite a bit of code in 4.9, but there isn't actually a lot that gets done. We load a document, and check if its metadata contains the
`Pessimistic-Locked` value. If it does, we check whatever the timeout for the lock has expired. If it isn't locked, we update the document metadata,
enable _optimistic_ concurrency and then call `SaveChanges`. If no one else modified the document in the meantime, we will succuessfully mark the 
document as ours, and any other call to `Lock` will fail.

The `Lock` method returns an `IDisposable` instance which handle releasing the lock. This is done by removing the metadata values and then calling 
`SaveChanges` again. If the lock has timed out, and someone took the lock, we'll fail here with concurrency exception as well.

> **Avoid your own distributed pessimistic locks**
>
> There is a reason why RavenDB does not include a pessimistic lock feature, and I strongly recommend to avoid using the recipe above. It is here 
> because it shows how to use several different features at once to achieve a goal. 
>
> Actually handling distributed lock is a non trivial issue. Consider a RavenDB cluster with multiple nodes. If two lock requests will go to two
> distinct nodes at the same time, _both_ of them will succeed^[We'll discuss how RavenDB cluster work in the next chapter in detail.]  The two nodes
> will quickly discover the conflicting updates on both the other side and generate a conflict. But it isn't guaranteed that they will do that inside
> the lock / unlock period. 
> 
> Another issue is the subject of timing, if two clients have enough of a clock skew, a client might consider a lock to have expired even though it is
> valid. Proper distributed locking require a consensus protocol of some kind and aren't trivial to build or use. RavenDB _have_ a consensus protocol
> but pessimistic locking is usually a bad fit for OLTP environment, and we decided to not implement it.

Typical uses for pessmistic locks are to lock a document while a user is editing it. That might sound like a good idea, but experience has shown that 
in most cases, it lead to a lot of trouble. Consider for example version control systems. If you are reading this book, you have likely used a SCM of
some kind. If you haven't, please stop reading this book and pick up a book about source control of _some_ kind, that is far more important.

Early source control systems (SourceSafe as a good example) used locks as their concurrency model, and that led to a lot of problems. Joe taking a lock
on the file and then leaving for vacation is a typical problem that is brought up in such cases. The same happens whenever you have pessimistic locks.
Implementing pessimistic locks also require you to implement forced lock release, "who is locking this document" feature and a whole bunch of management
functions around it. It is typically easier to implement optimistic concurrent or merging, and it matches most users expectations a lot more.

#### Offline optimistic concurrency

We looked at online optimistic concurrency in Listing 4.8, when we load a document into the session, modify it, and then save. In that time frame, if 
there was a change, we'll get a concurrency exception. But most software doesn't work like that. In a web application, you aren't going to keep the 
document session open for as long as the user is on the site. Instead, you'll use a session per request model, most likely. And the user will first 
load a page with the document's content and modify it in another request. There isn't a shared session in sight, so how can we implement optimistic
concurrency.

All you need to do is to send the etag of the document to the user, and accept it back when the user want to save the document. Listing 4.10 shows an
example of using two separate session with concurrency handling between them.

```{caption="Concurrent modifications of a support call" .cs}
long etag;
Supportcall callOne;

using(var sessionOne = store.OpenSession())
using(var sessionTwo = store.OpenSession())
{
  callOne = sessionOne.Load<SupportCall>("SupportCalls/238-B");
  etag = sessionOne.Advanced.GetEtagFor(callOne);
  
  var callTwo = sessionTwo.Load<SupportCall>("SupportCalls/238-B");
  
  callTwo.Ended = DateTime.Today.AddDays(1);
  
  sessionTwo.SaveChanges();
}

using(var sessionThree = store.OpenSession())
{
  sessionThree.Advanced.UseOptimisticConcurrency = true;

  callOne.Ended = DateTime.Today;

  sessionThree.Store(callOne, etag, callOne.Id);

  sessionThree.SaveChanges(); // will raise ConcurrencyException
}
```

The code in Listing 4.10 first load the support call in `sessionOne`, then load it again in `sessionTwo`, modify the support call and save it to the server. 
Both sessions are then closed, and we open a _new_ session. We call `Store`, passing the entity instance, the etag that we got from the first session as 
well as the document id. 

This give the RavenDB Client API enough information so we can do an optimistic concurrency check from the time we loaded `callOne` in the first session. In
web scenario you'll typically send the etag alongside the actual data, and get it back from the client to do the check. You might also want to checkout the 
`Changes API`, which is covered a little later in this chapter, which might be of help to get early change notifications when you need to implement offline
optimistic concurrency.

### Patching documents and concurrent modifications

Typical workflow with RavenDB is to open a session, load some document, run some business logic and then call `SaveChanges`. When you follow those steps, the
session will figure out what documents have changed and will send them to the server. That model is simple, easy to follow and the recommended method to work.

However, there are a few scenarios where we don't want to send the entire document back to the server. If our document is very large, and we want to make a 
small change, we can avoid that cost. Another reason to want to avoid the full document save is because we have a scenario that calls out for concurrent work
on a document.

Let us consider the `SupportCall.Votes` property, two users may very well want to vote on the same support call at the same time. One way to handle that is to
load the support call, increment the `Votes` property and call `SaveChanges`. In order to handle concurrent modifications, we will utilize optimistic 
concurrency and retries. But that is quite a lot of work to write, and if the document is large, it is also a lot of data that goes back and forth over the 
network for very little reason. Listing 4.11 shows how we can do much better.

```{caption="Incrementing a property using the Patch API" .cs}
using(var session = store.OpenSession())
{
  session.Advanced.Increment<SupportCall>("SupportCalls/238-B", c => c.Votes, 1);

  session.SaveChanges();
}
```

What the code in Listing 4.11 does is to generate a patch request, rather then load and save the full document. That request is stored in the session, and when
`SaveChanges` is called, it will be sent to the server (alongside any other changes / operations made on the session, as usual). On the server side, we'll apply
this operation to the document. This patch request is safe to call concurrently, since there is no loss of data as a result of executing the patches. 

> **What is fater, patching or load/save?**
>
> The obvious answer is that obviously patching is faster, we send a lot less data, and we need one less roundtrip to do so. Winning all around, right?
>
> The answer is that this is a bit more complex. The patch request is actually a JavaScript function that we send, which means that we need to parse and run
> it on the server side, potentially marshal values into the script environment and then marshal it back. Conversely, the code path for loading and saving
> documents in RavenDB is _very_ well trodden and had a lot of optimizations. That means that in many cases it might be easier to just load and modify the
> document directly, rather than use a patch.
>
> Patching _aren't_ expensive, I want to emphasis, but at the same time, I have seen codebases where _all_ writes had to be made using patching, because of 
> percieved performance benefits. That resulted in extremely hard to understand system that was resistent to change. The general recommendation is that you'll
> utilize patching only when you need to support concurrnet modifications.
>
> Note that in most cases, concurrent modifications of the same document is _not_ the default. A properly modeled document should have a single reason to change
> but it is very common that documents have additional data on them (like the `Votes` property, or `Comments`) which are important to save, but don't have any
> real business logic attached to them. 
> 
> That kind of change is fine to do using patching. If you find yourself trying to run serious business logic in patch scripts (we'll see exactly how to do this
> in a bit), you should move that into your own business logic. 
>
> Another important consideration for patching, you don't need to worry about concurrency between patches on the same document. There is no guarantee about the
> order in which the patches will run, but there aren't going to be concurrent / interleaved executing of the scripts on the same document.

A slightly more complex example of the use of patching is adding a comment to a `SupportCall`. Just like before, we want to support adding a comment 
concurrnetly, but to make things a bit more interesting, we'll add the business rule that a call that has been ended cannot have additional comments added to
it. Listing 4.12 shows the obvious way to do so.

```{caption="Adding a comment to a support call using patch" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  if(call.Ended != null)
  	throw new InvalidOperationException("Cannot comment on closed call");

  session.Advanced.Patch(call, c => c.Comments, 
  	comments => comments.Add("This is important stuff!"));

  session.SaveChanges();
}
```

In Listing 4.12 you can see how we moved from using the simple `Increment` call to the `Patch` which allow us to either replace a property value completely or
to add an item to a collection. If you look closely at the code in Listing 4.12 you'll find that there is a hidden race condition there. If the business rule
is that we cannot add comments to a closed call, we have the possibility that we'll load the call, check and see that its `Ended` property is null, and then 
send the patch request to the server. However, in the meantime, another client could have closed the call and yet we'll still add the comment.

How serious an issue this would be is entirely dependant on the domain and the model. It is possible that you _do_ want to add comments during that period, 
or it is possible that allowing it could break important business invariants. 

> **Patch requests are sent as part of `SaveChanges`**
>
> It is probably obvious, but I wanted to spell it out explicitly. Calls to `Patch`, `Increment` or `Defer` do not go to the server immediately, instead, they
> are added to the list of operations the session need to execute and will be sent to the server in a single batch (along with any modified documents) when
> `SaveChanges` is called. 
> If you have multiple patch operations in the same session on the same document, they'll be merged into a single patch, and if there
> are multiple patches on different documents they will all be execute on the same transaction, as a single unit.

There are two ways to avoid this race condition. We can send an etag to the server asking it to fail with a concurrency exception if the document has been 
modified since we last saw it. That _works_, but it defeats the whole _point_ of using patches for concurrent modification of the document. The second alternative
is to move the invariant check into the script itself. Calls to `Increment` and `Patch` are actually just wrapper around the `Defer` call, which allows you to
add work to the session to be sent to the server when `SaveChanges` is called.

In Listing 4.13 we are dropping down to using `Defer` directly to manipulate the patch request ourselves, with no wrappers. As you can see, this is a bit invovled
but overall it is pretty straightforward.

```{caption="Using a patch script to maintain call invariants" .cs}
using(var session = store.OpenSession())
{
  session.Advanced.Defer(new PatchCommandData(
    id: "SupportCalls/238-B",
    etag: null, 
    patch: new PatchRequest
    {
      Script = @"
if(this.Ended == null)
    throw 'Cannot add a comment to a closed call';
this.Comments.push(comment);
",
      Values =
      {
        ["comment"] = "This is important stuff!!"
      } 
   },
   patchIfMissing: null));

  session.SaveChanges();
}
```

The code in Listing 4.13 pass a `PatchCommandData` to `Defer`, and pass the relevant document. The key part is in the `PatchRequest` itself. We do the check on
the document, and fail if the call has already been closed, otherwise, we add the comment to the call. You can also see that we don't have to deal with string
concatenation here, since we can pass arguments to the scripts directly. There is also the option to run a script if the document does not exists. This gives you 
the option to do a "modify or create" style of operations.

Using this method, we can be sure that we will never violate the rule about adding comments to a closed call. But this is a bit more complex than anything we had
before, and I would only recommend doing so if you really must. A cautionary word, though. This is a very powerful technique, but it is also open to abuse. 
Running logic in this manner inside the database is usually a very bad idea. If you find yourself doing something like this frequently, stop and reconsider. 

This is the sharp end of the stick, and abusing this feature can cause problems. In particular, the scripts are run under a lock, and can slow or prevent the 
database from completing transactions quickly. 

### Deferring commands

In the previous section, we used the `Defer` method to register a `PatchCommandData` on the session, to be executed when `SaveChanges` is called. But `Defer` is
more generic then this. It is a general mechanism for the user to register arbitrary commands that will take part of the same transaction as the `SaveChanges`. 

> **RavenDB API is like an ogre, it has layers**
>
> The RavenDB API is composed of layers. At the top, you have the document store and the document session. Those are built using Operations concept, which you
> will typically not use directly except in rare cases. And the Operations are handled via the request executer, which allows you to generate requests directly
> to the server (and take advantage of RavenDB's authentication and automatic failover). 
>
> The case of `Defer` is a good example. Instead of forcing you to drop down all the way, we expose an extension point in the session so you can plug in a 
> command of your own and piggyback on the session handling of transactions.

The available commands range from putting and deleting documents and attachments, applying patches and deleting documents using a prefix. Aside from the patch
operation, the rest are only ever useful in very rare cases. The most common use of `Defer` beyond patch is when you need fine grained control over the operations
that will be executed in a single transaction, to the point where you want to control the ordering of the operations.

RavenDB doesn't allow changing a document collection, so if you need to do this^[And this should be a very rare thing indded] you cannot just update the 
`@collection` metadata. Instead, you have first delete the old document and create a new one with the appropriate collection. The session doesn't allow to both
delete and modify a document and for the purpose of discussion, let us say that we _have_ to make this in a single transaction, so no other client may see a 
point in time where the document was deleted.

To be perfectly clear, this is a really strange situation, dreamt up specifically to showcase a feature that should be only used for very special circumstances.
This escape hatch in the API is intended specifically so you'll not be blocked if you need something that we didn't foresee, but I can't emphasis enough that 
this is probably a bad idea. The emergency exit is very important, but you don't want to make it the front door.

Another reason to typically want to avoid using `Defer` is that it is lowered in the RavenDB Client API layers, instead of dealing with high level concepts 
like entity, you'll be dealing with the direct manner in which RavenDB is representing JSON, the blittable format. That format is meant to be high performance
and things like developer convienance were secondary in its design. 

### Bulk inserting documents to RavenDB

RavenDB is fast, _really_ fast, but it still needs to face operational realities. The 
[Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing) still apply, and I/O takes a non trivial amount of time.
That means that when you want to get the best speed out of RavenDB, you need to help it achieve it. 

Listing 4.14 shows the absolutely _slowest_ way to write 10,000 documents into RavenDB.

```{caption="Writing 10,000 documents, one at a time" .cs}
var sp = Stopwatch.StartNew();
for (int i = 0; i < 10_000; i++)
{
 using (var session = store.OpenSession())
 {
     session.Store(new Customer
     {
         Name = "Customer #" + i
     });
     session.SaveChanges();
 }
}
Console.WriteLine(sp.Elapsed);
```

For fun, I decided to run the code in Listing 4.14 against the live test instance that we have. That instance is in San Francisco, and I'm testing this from
Israel. The test instance is also running as a container inside an AWS t2.medium machine (2 cores & 4 GB of memory, with burst only mode). In other words, this
is a performance test that is heavily biased against RavenDB. And the results aren't really great. In fact, they are _bad_.

> **Stacking the deck**
>
> I'm going to be talking about a lot of performance numbers in this section, and I wanted to be clear that I have choosen (intentionally) the worst possible
> situation for RavenDB and then compounded the issue by using the wrong apporaches to be able to show off the real costs in a manner that is visible.
>
> I'll refer you again to the [Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing). I'm trying to select
> a scenario that would break as many of those fallacies as I can, and show how RavenDB is able to handle them.

This is because we are running each write as an independent operation, and we have to wait for the previous operation to complete before we can start the new one.
What is more, the database server handles just a single request concurrently, which means that we have no way to amortize I/O costs across multiple requests. This
is the absolutely worst way you can write large amount of documents into RavenDB, because the problem is that most of the time is spent just going back and forth
between the client and the application. On each request we have to make another REST call, send a packet to the server, etc. On the other side, the server accept
a new request, process it (and commit it to disk). During the entire process, it is effectively idle, since most of the time is spent in waiting for I/O. That is a big waste all around.

You can see the various very nicely when looking at the Fiddler^[The [Fiddler web proxy](http://www.telerik.com/fiddler) is a great debugging tool in general, and 
very useful to peek into the communication between RavenDB server & clients.] statistics. Each request takes about 220 - 260 millseconds to run. Writing the first
1,000 documents took 4 minutes and 6 seconds and 2,000 requests took 8 minutes on the clock. The full 10,000 documents would take about 40 minutes or so. 

Granted, we are intentionally going to a remote server, but still... What happens when we are running the writes in parallel? The code in Listing 4.15 will show
up how to do this.

```{caption="Writing 10,000 documents, with a bit of parallelism thrown in" .cs}
var sp = Stopwatch.StartNew();
Parallel.For(0, 10_000, i =>
{
  using (var session = store.OpenSession())
  {
      session.Store(new Customer
      {
          Name = "Customer #" + i
      });
      session.SaveChanges();
  }
});
Console.WriteLine(sp.Elapsed);
```

Using the method in Listing 4.15, I was able to write 1,000 documents in 56 seconds, and we got to 2,000 in a minute and a half, 3,000 in a minute and 50 seconds, 
etc. The reason for the speed up is actually related to thread pool handling on the client side. Since we make a lot of blocking requests, the thread pool figure out
that we have a lot of blocking work, and creates more threads. That means that we have the chance to do more concurrent work, so as times goes by, more threads are
created and we make additional concurrent requests to RavenDB. 

The total time to write 10,000 documents in this setup was 2 minutes and 52 seconds. So we are just over half the time to write a tenth of this number when using 
sequential writes. The code in Listing 4.15 is still using syncronous calls, which means that the client side is spinning threads to handle the load and we are
limited by the rate of new threads creation on the client. 

RavenDB also supports async API, which is much more suitable for scale out scenarios, because we aren't holding a thread the duration of the connection. Listing 
4.16 shows how we can write all those documents in parallel and using the async API. The code is a bit complex, because we want to control the number of concurrent
requests we make. Spinning 10,000 concurrent requests will likely load the network and require careful attention to how they are managed, which is out of scope
for this book. Instead, I limited the number of concurrent connections to 128.

```{caption="Writing 10,000 documents, using async API" .cs}
var sp = Stopwatch.StartNew();
var semaphore = new SemaphoreSlim(128);

async Task WriteDocument(int i)
{
    using (var session = store.OpenAsyncSession())
    {
        await session.StoreAsync(new Customer
        {
            Name = "Customer #" + i
        });
        await session.SaveChangesAsync();
    }
    semaphore.Release();
}

var tasks = new List<Task>();
for (int i = 0; i < 10_000; i++)
{
    semaphore.Wait();
    tasks.Add(WriteDocument(i));
}

Task.WaitAll(tasks.ToArray());

Console.WriteLine(sp.Elapsed);
```

The code in Listing 4.16 is also using a local method, which is a new C# 7.0 feature, it allows to package a bit of behavior quite nicely, and is very useful for
small demos and internal async code. This code writes 1,000 documents in a bit under 10 seconds, and complete the full 10,000 writes in under 30 seconds (29.6, on
my machine). The speed difference is again, related to the client learning our pattern of behavior and adjusting itself accordingly (creating enough buffers, threads
and other resources needed, warming up the TCP connections^[TCP Slow Start can be a killer on benchmarks]).

However, we really had to make an effort, write explicit async code and manage it, rate limit our behavior and jump through several hoops to get a more reasonable
performance. Note that we went from over 40 minutes to less than 30 seconds in the span of a few pages. Note that we haven't actually modified _what_ we are doing,
we only changed how we are talking to the server, but it had a huge impact on performance.

You can take it as a given that RavenDB is able to process as much data as you can feed it, and the typical problem in handling writes is how fast we can get the 
data to the server, not how fast it can handle it. 

RavenDB contains dedicated API and behavior that make it easier to deal with bulk loading scenarios. The Bulk Insert API uses a single connection to talk to the
server and is able to make much better usage of the network. The entire process is carefully orchestrated by both client and server to optimize performance. 
Let us look at the code in Listing 4.17 first and then discuss the details.

```{caption="using bulk insert to write 100,000 documents, quickly" .cs}
var sp = Stopwatch.StartNew();

using (var bulkInsert = store.BulkInsert())
{
 for (int i = 0; i < 100_000; i++)
 {
     bulkInsert.Store(new Customer
     {
         Name = "Customer #" + i
     });
 }
}

Console.WriteLine(sp.Elapsed);
```

The code in Listing 4.17 took 2 minutes and 10 seconds to run on my machine. Which is interesting, because it seems slower from the async API usage sample, right?
Except that I made a typo when writing the code and wrote a _hundred_ thousands documents, instead of _ten_ thousands. If I was writing merely 10,000 documents, it
would complete in about 18 seconds or so. The code is fairly trivial to write, similar to our first sample in Listing 4.14, but the performance is many times higher.

To compare the costs, I run the same code against a local machine, giving me a total time to insert a 100,000 documents of 11 seconds (instead of 2 minutes 
remotely). If we want to compare apples to apples, then the cost for writing 10,000 documents are showing in Table. 4.1.


|                |   Remote    |    Local    |
|----------------|-------------|-------------|
| Session        | 41 *minutes*| 20 seconds  |
| Bulk Insert    | 18 seconds  | 6.5 seconds |

Table:  Bulk insert costs locally and remotely

You can see that while bulk insert is significantly faster in all cases, being over three times faster than the session option (Listing 4.14) locally is losing 
its meaning when you consider that it is over 130 times faster in the remote case. The major difference as you can imagine is the cost of going over the network, 
but even on the local machine (let alone the local network) there is a significant performance benefit for bulk insert.

Amusingly enough, using bulk insert still doesn't saturate the server, and for large datasets, it is advisable to have parallel bulk insert operations going at
the same time. This give the server more work to do, and allow us to do a lot of optimizations that increase the ingest rate of the server.

The way bulk insert work is by opening a TCP connection to the server and writing the raw data directly into the database. That means that we don't need to go 
back and forth between the client and server and can rely on a single connection to do all the work. The server, for its part, will read the data from the network
and write it to disk when it is best to do so. In other words, bulk inserts are _not_ transactional. A bulk insert is actually comopsed on many smaller transactions
whose size and scope is decided on by the server based on its own determination, in order to maximize performance.

When the bulk insert is completed, you can rest assure that all the data has been safely committed to disk properly, but during the process, the database will 
incrementally commit the data instead of going with a single big bang mode.

In general, RavenDB performance is ruled mostly by how many requests you can send it. The more requests, the higher the degree of parallelism and the more efficent 
RavenDB can work. In our internal tests, we routinely bumped into hardware limits (the network card cannot process packets any faster, the disk I/O is saturated, 
etc), not software ones. 

## Reading documents

We just spent a lot of time learning how we can write documents to RavenDB in all sorts of interesting ways. For reading, however, is there really that much
to know? We already know how to load a document, and how to query them. We covered that in [Chapter 2](#zero-to-ravendb). We also covered `Include` and how
to use it to effortlessly get referenced documents from the server. What else is there to talk about? As it turns out, quite a bit. 

In most applications, reads are far more numerous then writes. Typically by an order of magnitude. That means that RavenDB needs to be prepared to handle a
_lot_ of reads, and that applications typically have a lot of ways in which they access, shape and consume the data. RavenDB needs to be able to provide an
answer to all those needs.

The first feature I want to present actually allows you to dramatically increase your overall performane by being a little lazy.

### Lazy requests

In Section 4.1.7 about Bulk Insert, we saw how important the role of the network is. Running the same code on the local network vs. the public Internet 
results in speed differences of 20 seconds to 41 _minutes_, just because of network latencies. On the other hand, moving from many requests to a single
request in bulk insert is the primary reason we cut our costs by two thirds on the local machine and over two orders of magnitude in the remote case.

I talked about this a few times already, but this is _important_. The latency of going to the server, making a remote call, is often much higher than the
cost of actually processing the request on the server. One the local machine, you'll probably not notice it too much. That is typical for running in a
development enviornment. When you go to production, your database is typically going to run on a dedicate machine^[In fact, a database cluster will be
typically used, on a set of machines.] so you'll have to go over the network to get it. And that dramatically increase the cost of going to the database.

This problem is typically called out as the [Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing) and is
quite well know. RavenDB handles that in several ways. A session has a budget on the number of remote calls it can make (this is controlled by
`session.Advanced.MaxNumberOfRequestsPerSession`), if it goes over that limit, an exception will be thrown. We had that feature from the get go, and that
led to a lot of thinking about how we can reduce the number of remote calls.

`Include` is obviously one such case, instead of going to the server multiple times, we let the server know that we'll need additional information to 
follow up on this request and it send it to us immediately. But we can't always do that. Let us take a look at Listing 4.18, showing two queries that we
cannot optimize using `Include`.

```{caption="Loading a Customer and the count of support calls for that customer" .cs}
using (var session = store.OpenSession())
{
  var customer = session.Load<Customer>("customers/8243-C");
  var countOfCalls = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"))
  	.Count();

  // show the customer and the number of calls 
  // to the user
}
```

A scenario like the one outlined in Listing 4.18 is incredibly common. We have many cases where we need to show the user information from multiple sources
and that is a concern. Each of those calls turn out to be a _remote_ call, requiring us to go over the network. There are ways to optimize this specific
scenario. We can define a map/reduce index and run a query and `Include` on that. We haven't yet gone over exactly what this means^[We'll cover this technique
when we'll discuss map/reduce indexes, in [Chapter 13](#map-reduce)], but this is a pretty complex solution, and isn't relevant when you have different 
type of queries. If we wanted to also load the logged in user, for example, that wouldn't work.

The solution RavenDB has for this issue is the notion of Lazy Requests. A lazy request isn't actually being executed when you are making it. Instead, it is 
stored in the session and you get a `Lazy<T>` instance back. You can make multiple lazy requests one after another, and no network activity will occur. However,
as soon as you access the _value_ of one of those lazy instances, all the lazy requests that are held up by the session will be sent to the server as a single
unit.

All those requests will be processed by the server and all the replies will be sent as a single unit. So no matter how many lazy requests you have, there will
only ever be a single network roundtrip to the server. You can see the code sample in Listing 4.19.

```{caption="Lazily loading a Customer and their count of support calls" .cs}
using (var session = store.OpenSession())
{
  Lazy<Customer> lazyCustomer = session.Advanced.Lazily
  	.Load<Customer>("customers/8243-C");
  Lazy<int> lazyCountOfCalls = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"))
  	.CountLazily();

  // no network call have been made so far

  // force execution of pending lazy operations explicitly
  session.Advanced.Eagerly.ExecuteAllPendingLazyOperations(); 

  // if ExecuteAllPendingLazyOperations wasn't call, it 
  // will be implicitly called here.
  int countOfCalls = lazyCountOfCalls.Value;
  Customer customer = lazyCustomer.Value;

  // show the customer and the number of calls 
  // to the user
}
```

As the code in Listing 4.19 shows, we can define multiple lazy operations. At that stage, they are pending. They are stored in the session, but haven't been
sent to the server yet. We can either call `ExecuteAllPendingLazyOperations` to force all pending operations to execute, or we can have that happen implicitly
by access the `Value` property on any of the lazy instances we got.

> **Why do we need ExecuteAllPendingLazyOperations**
>
> The existence of ExecuteAllPendingLazyOperations is strange. It is doing explicitly something that will happen implicitly anyway, so whey is this needed?
> This method is here to allow users to have fine grained control over the execution of requests. In particular, it allows to setup a stage in your pipeline
> that will request all the data it is going to need, and then call ExecuteAllPendingLazyOperations to fetch this explicitly. 
>
> The next stage is supposed to operate on the pure in memory data inside the session, and not require any calls to the server. This is important is advanced
> scenarios when you need this level of control, and want to prevent the code from making unexpected remote calls in performance critical sections of your 
> code.

The performance gain from `Lazy` are directly corelated to the number of lazy requests that it is able to batch, and how far away the actual server is. The more
requests can be batched and the further away the database server is, the faster this method become. On the local machine, this is rarely enough to be a bother,
but once we go to production that can get you some real benefits.

Note that as useful as `Lazy` is, it is limited to requests that you can make with the information that you have on hand. If you need to make query based on the
results of another query, you will not be able to use `Lazy` for that. For most of those scenarios, you can use `Include`. And, of course, `Lazy` and `Include`
can work together, so that usually suffice.

### Streaming data

When dealing with large amount of data, the typical API we use to talk to RavenDB is not really suitable for the task. Let us consider the case of the code in
Listing 4.20.

```{caption="Query all support calls for a customer" .cs}
using (var session = store.OpenSession())
{
  List<SupportCall> calls = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"))
  	.ToList();
}
```

What will happen if this is a particularly troublesome customer, that opened a _lot_ of calls? If this is customer that had just 30 calls, it is easy to see
that we'll get them all in the list. But what will happen if this customer has 30,000 calls? Figure 4.1 shows how a query is processed on the server in this 
case. 

![A query using request buffering](./Ch04/img01.png)

The server will accept the query, find all matches, prepare the results to send and then send them all over the network. On the client side of things, we'll 
read all the results from the network and batch them all into the list we'll return to the application. Eventually, we'll read all the results and return the
full list to the application.

If there are 30 results in all, that is great, but if we have 30,000, we'll likely suffer from issues. Sending 30,000 results over the network, reading 30,000
results _from_ the network and then populating a list of 30,000 (potentially complex) objects is going to take some time. In terms of memory usage, we'll need
to hold all those results in memory, possibly for an extended period of time.

Due to the way memory management work in .NET^[The behavior on the JVM is the same, other clients environment have different policies], allocating a list with
a lot of objects over a period of time (because we are reading them from the network) will likely push the list instance (and all of its contents) into a higher
generation, which means that when you are done using it, the memory will not be collected without a more expensive `Gen1` or even `Gen2` round. 

In short, for large number of results, the code in Listing 4.20 will take more time, consume more memory and force more expensive GC in the future. In previous
versions of RavenDB, we actually had guards in place to prevent this scenario entirely. It is very easy to start writing code like the one in Listing 4.20 and
over time have more and more results come in. The logic was that at some point, we need to have a cutoff point and throw before this kind of behavior will 
poision your application.

As it turn out, our users _really_ didn't like this behavior. In many cases, they would rather the application do more work (typically uneccesarily) then have it
throw an error. That allows them to fix a performance problem rather than a "system is down" issue. As a result of this feedback, we removed this limitation. But
we still recommend to always use a `Take` clause in all queries, to prevent just this kind of issue. 

> **All queries should have a `Take` clause**
>
> A query that doesn't have a take clause can be a poision pill for your application. As data size grows, the cost of making this query also grow, until the
> entire thing goes down.
>
> The RavenDB Client API contains a convention setting called `ThrowIfQueryPageSizeIsNotSet`, which will force all queries to specify a `Take` clause and will 
> error otherwise. We recommend that during development, you'll set this value to true, to ensure that your code will always be generating queries that have
> a limit to the number of results they get.

Very large queries are bad, it seems, but that isn't actually the topic of this section. Instead, this is just the preface explaining why _buffered_ large 
queries are a bad idea. RavenDB also supports the notion of _streaming_ queries. You can see how that would look like in Figure 4.2.

![A query using request streaming](./Ch04/img01.png)

Unlike the previous example, with streaming, neither client nor server need to hold the full response in memory. Instead, as soon as the server has a single
result, it send that to the client. The client will read the result from the network, materialie the instance and hand it off to the application immediately.
In this manner, the application can start processing the results of the query before the server is done sending it, and doesn't have to wait. You can see the
code for that in Listing 4.21.

```{caption="Stream all support calls for a customer" .cs}
using (var session = store.OpenSession())
{
  var callsQuery = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"));

  using(var stream = session.Advanced.Stream(callsQuery))
  {
  	while(stream.MoveNext())
  	{
  		SupportCall current = stream.Current;
  		// do something with this instance
  	}
  }
}
```

Instead of getting all the results in one go, the code in Listing 4.21 will pull them from the stream one at a time. This way, the server, the Client API and 
the application can all work in paralle with one another to process the results of the query. This technique is suitable for processing very large number of 
results (in the millions).

The use of streaming queries requires you to hold a few things in mind: 

* The results of a streaming query are _not_ tracked by the session. Changes made to them will not be sent to the server when `SaveChanges` is called. This is
  because we expect streaming queries to have a very high number of results, and we don't want to hold all the references for them in the session. Otherwise,
  we'll ensure that the GC will not collect them.

* Since streaming is happening as a single large request, there is a limit to how long you can delay before you call `MoveNext` again. If you wait too long, 
  it is possible for the server to give up sending the rest of the request to you (since you didn't respond in time) and abort the connection. Typically, you'll
  be writing the results of the stream somewhere (to a file, to the network, etc). 

* If you want to modify all the results of the query, do not call `session.Store` on each as they are pulled from the stream. You'll just generate a lot of work 
  for the session and eventually end up with a truly humongous batch to send to the server. Typically, if you want to read a lot of results and modify them, you'll
  use a stream and a `Bulk Insert` at the same time. You'll read from the stream and call `Store` on the bulk insert for each. This way, you'll have both streaming
  for the query as you read and streaming via the bulk insert on the write. 

When you need to select what option to use a regular (batched) query or a streaming query, consider the number of items that you expect to get from the query, and
what you intend to do with them. If the number is small, you will likely want to use a query for its simple API. If you need to process a lot of results, you 
should use the streaming API.

Note that the streaming API is _intentionally_ a bit harder to use. We made sure that the API expose the streaming nature of the operation. You should strive to 
avoid wrapping that. Streaming should be used on the edge of your system, where you are doing something with the results and passing them directly to the outside 
in some manner.

## Caching

An important consideration with speeding up an application is caching. In particular, we can avoid expensive operations if we cache the results of the last time
we did that operation. Unforutnately, caching is _hard_. Phil Karlton [said](https://martinfowler.com/bliki/TwoHardThings.html):

	There are only two hard things in Computer Science: 
	 cache invalidation and naming things.

Caching itself is pretty trivial to get right, but the really hard part is how you are going to handle cache invalidation. If you are serving stale information 
from the cache, the results can range from nothing much to criticial, depending on exactly what you are doing. 

With RavenDB, we decided early on that caching was a complex topic, so we have better handle it properly. This is done in two parts. The server side will generate
an etag for all read operations. That etag is computed by the server and can be used by the client later on. The client, on the other hand, is able to cache the 
request from the server internally, and next time a similar request is made, it will send the cached etag to the server alongside the request.

When the server get a request with an etag, it has a dedicated code path, optimized specifically for that, to check whatever the results of the operation have 
changed. If they didn't change, the server can return to the client immediately, letting it know that it is safe to use the cached copy. In this manner, we save computation costs on the server and network transfer cost between the server and the client. 

Externally, from the API consumer point of view, there is no way to tell that caching happened. Consider the code in Listing 4.22.

```{caption="Query caching in action" .cs}
using (var session = store.OpenSession())
{
  var calls = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"))
  	.ToList();
}

using (var session = store.OpenSession())
{
  // this query will result in the server
  // returning "this is cached" notification
  var calls = session.Query<SupportCall>()
  	.Where(c => c.CustomerId == "customers/8243-C"))
  	.ToList();
}
```

The client code doesn't need to change in any way to take advantage of this feature. This is on by default and is always there to try to speed up your requests. 
Note that while the example in Listing 4.22 is with queries, caching is prevelant in RavenDB. Loading a document will also use the cache, as will most other 
read operations.

The cache that is kept on the client side is the already parsed results, so we saved not just the network roundtrip time, but also the parsing costs. We keep the 
data in _unmanaged_ memory, because it is easier to keep track of the size of the memory and avoid promoting objects into Gen2 just because they have been in the 
cache for a while. The cache is scoped to the document store, so all sessions from the same document store will share the cache and its benefits. 

> **Time to skip the cache**
>
> Caching by default can be a problem with particular set of queries. Those who use the notion of the current time. Consider the case of the follwing query:
>
> ```
>    session.Query<SupportCall>()
>      .Where(c => 
>	       c.StartedAt >= DateTime.Now.AddDays(-7) 
>	       && c.Ended == null);
> ```
> This query is asking for all opened support calls that are over a week old.
>
> This kind of query is quite innocent looking, but together with the cache, it can result in surprising results. Because the query uses `DateTime.Now`, on every
> call, it will generate a different query. That query will never match any previously cached results, so it will always have to be processed on the server side.
> What is worst, every instance of this query will sit in the cache, waiting to be evicted, never to be used. 
>
> A much better alternative will be to use the following:
>
> ```
>  c.StartedAt >= DateTime.Today.AddDays(-7)
> ```
>
>  By using `Today`, we ensure that we can reuse the cached entry for multiple calls. Even if you need smaller granularity then that, just truncating the current 
> time to a minute / hour interval can be very beneficial. 

The cache works by utilizing HTTP caching behavior. Whenever we make a `GET` request, we check the cache if we previously made a request with the same url and query
string parameters. If we did, we fetch the cached etag for that request and send it to the server. The server will then use that etag to check if the results have
changed and if they didn't, we return back a `304 Not Modified` response. The client will then just use the cached response that it already have.

While most read requests are cached, there are a few that aren't. Anything that will always be different, such as stats calls, will never be cached. Since stats and
debug endpoints must return fresh information any time that the are called. Attachments are also not cached, because they can be very large and they are typically
already handled very differently by the application.

### Aggresive caching

Caching is great, it seems. But in order to utilize caching by RavenDB we still need to go back to the server. That ensure that we'll get the server confirmation 
that the data we are going to return for that request is indeed fresh and valid. However there are a lot of cases where we don't care too much about the freshness
of the data.

In a heavily used page, showing data that might be stale for a few minutes is absolutely fine, and the performance benefits from not having to go to the server 
can be quite nice to gain. In order to support this scenario, RavenDB support the notion of aggresive caching. You can see an example of that in Listing 4.23.

```{caption="Aggressive caching in action" .cs}
using (var session = store.OpenSession())
using (session.Advanced.DocumentStore.AggressivelyCache())
{
    var customer = session.Load<SupportCall>(
    	"customers/8243-C");
}
```

The code in Listing 4.23 uses aggresive caching. In this mode, if the request is in the cache, the RavenDB Client API will never even ask the server if it is 
up to date. Instead, it will immediately serve the request from the cache, skipping all network traffic. This can significantly speed up operations where you
can handle a certain period of stale view of the world. 

However, just having this mode will be pretty bad if you'll never see new results. This lead us back to the problem of cache invalidation. Aggressive caching 
isn't just going to blindly cache all the data for all time. Instead, the first time it encounters an instruction to aggressively cache, the client is going
to open a connection to the server and ask the server to let it know whenever something changes in the database. This is done using the `Changes API`, which 
is covered in the next section in this chapter.

//TODO: This isn't wired in the 4.0 client yet

> **Caching behavior in a cluster**
>
> Typically RavenDB is deployed in a cluster, and a since database will reside on multiple machines. How does caching work in this context? 
> The caching is built on the full URL of the request, and that takes into account the particular server that we'll be asking. That means that the cache
> will store a separate result for each server, even if the request is exactly the same. This is done because the etags generated by each server are
> local to that server, and doesn't make sense globally.

Whenever the server let the client know that something have changed, the client will ensure that the next cached request will actually hit the server for the 
possibly updated value. Note that because you might be caching a lot of different queries, documents, etc. The client isn't asking specifically for changes that
it has in its cache. Instead, it is asking for all changes in the server.

In this way, whenever the server let the client know that a change happened, we'll force a check with the server the next time that cached response will be used
for all the cached responses that are earlier then the last notification. When we do this, we still send the cached etag, so if that particular response hasn't 
changed, the server will still respond with a `304 Not Modified` and we'll use the cached value (and update its timestamp).

> **Why isn't aggressive caching on by default?**
>
> Aggressive caching isn't on by default because it may violate a core constraint of RavenDB, that a request will always give you the latest information. With
> the requirement that aggressive caching must be turned on explicitly, you are aware that there is a period of time where the response you got might have
> diverged from the result on the server.

The idea is that with this behavior, you get the best of both worlds. You have immediate caching, without having to go to the server if nothing have changed, but
if something might have changed, we'll check the server fo the most up to date response. Given typical behavioral patterns for application, a lot of the time, we
will be able to use the aggresive cache for quite some time before a write will come in and make us check with the server.

## Changes API

We mentioned the `Changes API` in the previous section, since the aggressive caching is using it. The `Changes API` is a way for us to connect to the server and
ask it to let us know when a particular event has happened. Listing 4.24 shows how we can ask the server to let us know when a particular document have changed.

```{caption="Getting notified when a document changes" .cs}
var subscription = store.Changes()
	.ForDocument("customers/8243-C")
	.Subscribe(change => {
		// let user know the document changed
	});

// dispose to stop getting future notifications
subscription.Dispose(); 
```

Typically, we use the code in Listing 4.24 when implementing an edit page for a document. When the user starts editing the document, we register for notifications
on changed on this document, and if it does change, we let the user know immediately. That allow us to avoid having to wait for the save action to discover that
we need to redo all our work.

The `Changes API` work by opening a websocket to the server, and letting the server know exactly what kind of changes it is interested on. We can register for
a particular document, a collection, documents that start with a particular key or even global events, such as operations, indexes and transformers. 

> **Changes API in a cluster**
>
> In a cluster, the `Changes API` will always connect to one node, and changes must first flow to that node before the changes will be sent to the client. The
> failure of a node will cause us to reconnect (possibly to a different server) and resume waiting for the changes that we are interested in.

The `Changes API` is meant to get non critical notifications. It is cheap to run and is pretty simple, but it is possible that a failure scenario will cause you
to miss updates. If the connection has been reset, you might lose notifications that happened while you were reconnecting. It is recommended that you'll use 
the `Changes API` for enrichment purposes, and not rely on it. For example, to tell if a document have changed so you can give early notification, but also ensure
that you have optimistic concurrency turned on, so it will catch the change on save anyway. 

Another example is with aggressive caching, if we missed a single notification, that isn't too bad. The next notification will put us in the same state, and we'll
be fine because the user explcitly chose performance over getting the latest version in this case. Yet another case can be monitoring, where you want to have an
idea about what is going on in the server, but it is fine to lose something if there is an error, because you are interested in what is happening now, not the full
and complete history of actions on the server.

For criticial operations, where you cannot afford to miss even a single change, you can use `Subscriptions`, which are covered at the end of this chapter. They are 
suited for such a scenario, since they gurantee that all notifications will be properly sent and acknowledged. 

## Transformers

In the previous chapter, we talked a lot about modeling, how we should structure our documents to be indepndent, isolated and coherent. That make for a really good
system for transaction processing (OLTP) scenarios. But there are quite a few cases where even in a business application, we have to look at the data differently.
Let us take a look at our support case example. If I'm a help desk engineer, and I'm looking at the list of open support calls. I want to see all the recent support
calls, and I want to also see the customer that opened them. 

Based on what we know so far, it would be trivial to write the code in Listing 4.25.

```{caption="Recent orders and their customers" .cs}
using (var session = store.OpenSession())
{
  List<SupportCall> recentOrders = session.Query<SupportCall>()
  	.Include(c => c.CustomerId) // include customers
  	.Where(c => c.Ended == null) // open order
  	.OrderByDescending(c => c.Started) // get recent
  	.Take(25) // limit the results
  	.ToList();
}
```

The code in Listing 4.25 is doing a _lot_. It gets us the 25 most recent opened support calls, and it is also including their customers. We can now show this to
the user quite easily, and we were able to do that in a single request to the server. Under most circumstance, this is exactly what you will want to do. 

However, this does mean that in order to pull the few fields we need to show to the user, we need to pull the full documents for the support calls and the customers
to the application. If those documents are large, that can be expensive. Another way to handle that is by letting the server know exactly what we need to get back
and then doing the work on the server side. 

This is the role of transformers. A transformer is a bit of code that is going to run on the server and transform the result that we'll get back. Listing 4.26 
shows a transformer that will give us just the right data to show to the user.

```{caption="Transformer to get the recent order details" .cs}
public class RecentOrdersTransformer : 
	AbstractTransformerCreationTask<SupportCall>
{
	public RecentOrdersTransformer()
	{
		TransformResults = calls =>
			from call in calls
			let customer = LoadDocument<Customer>(call.CustomerId)
			select new 
			{
				CustomerName = customer.Name,
				call.Started,
				call.Issue,
				call.Votes
			};
	}

	public class Result
	{
		public string CustomerName;
		public DateTime Started;
		public string Issue;
		public int Votes;
	}
}
```

The transformer is going over the list of calls it was provided and load the associated customer, then it project out a new object, with the properties from both
the call and the associated customer. You should also note that we have a `Result` class there, which match the result of the output of the transformer. This will
help us to consume the output of the transformer on the client side.

It is important to understand that the `RecentOrdersTransformer` in Listing 4.26 isn't the actual transformer, it is the transformer _definition_. It is a way
for us to create a transformer on the client side in a type safe manner. The RavenDB Client API can use that class to generate the transformer on the server.
This is done by executing the following code:

```
new RecentOrdersTransformer().Execute(store);
```

Once that line is executed, we know that the transformer has been properly sent to the server. Multiple calls to this method, without changing the tranfromer 
defintion will have no affect on the server. This is typically done as part of the document store initialization in your application. For example, assuming your
application uses the approach outlined in Chpater 2, Listing 2.10, you'll add this line as the last thing in the `CreateDocumentStore` method.

> **Transformers and databases**
>
> Transfomers are created on the server side, and they are always scope to the database on which they are created. It is safe to have two different
> transformers with the same name on two distinct databases, although that might be confusing for the consumer. 
>
> Once created, transformers exists on their database and are available for use. Unless an operation ask for a transformer to be applied, a transformer has 
> no impact on the server.

After we created the transformer, we can now use it, as you can see in Listing 4.27.

```{caption="Recent orders and their customers, using transformer" .cs}
using (var session = store.OpenSession())
{
 List<RecentOrdersTransformer.Result> recentOrders = session
  .Query<SupportCall>()
  .Where(c => c.Ended == null) // open order
  .OrderByDescending(c => c.Started) // get recent
  .Take(25) // limit the results
  .TransformWith<RecentOrdersTransformer, 
  		RecentOrdersTransformer.Result>()
  .ToList();
}
```

There are a few things to note here. First, we no longer include the `Customer` in the query. We don't need that, because the result of this query isn't a list
of `SupportCall` but `RecentOrdersTransformer.Result`, which already include the `CustomerName` that we want. The key part for us is the call to 
`TransformWith` method. This let RavenDB know that we want to apply the `RecentOrdersTransformer` transformer on the server side to the result of the query. 

In terms of order of operations, this call should typically be last. This isn't actually required, but given that this is the order of operations on the server it 
allows us to have more intuitive understanding of what is going on^[Changing the order of the calls on the client side will _not_ change the order of operations
on the server side, to be clear.]. The transformer is applied on the query after the query has been filtered, sorted and had paging applied to it. Clasues such
as `order by`, `where`, `group by` and the like on transfromers typically have no real impact on the results.

> **Transformers in a cluster**
> 
> Calling `new RecentOrdersTransformer().Execute(store)` will cause the client to send the tranformer definition to one of the servers in the cluster. However,
> the transformer definition will be accessible on all the nodes in the cluster for that particular database. 

The last generic parameter of `TransformWith` is `RecentOrdersTransformer.Result`, which is the type that the client will use to deserialize the results from the 
server. This is a pure convention of having an inner class named `Result` with the same output of the transformer, it isn't required. But having the `Result` class
as an inner class inside the transformer make it clear that they are closely tied and typically changed together.

As you saw, a transformer can do quote a lot. It can modify the output of the query, it can load additional document, perform computation, etc. They can also be 
applied on a single load operation, not just on queries, as you can see in Listing 4.27.

```{caption="Transformer using load" .cs}
RecentOrdersTransformer.Result recentOrder = session
	.Load<RecentOrdersTransformer, 
		RecentOrdersTransformer.Result>(
			"SupportCalls/238-B");
```

It is common to use transformers in high value pages, to get just the right information from the server without having to send full documents over the wire. They
are a powerful tool to have handy. 

The output of the transformer is not a document, and as a result of that, it isn't tracked by the session. That means that changes to the result of a transformer
will not be saved back to the server when `SaveChanges` is called. You _can_ call `Store` on the result of a transformer, but be aware that this will create a 
new document, in a separate collection. There is usually little reason to want to do that.

## Cross cutting concerns on the client

The RavenDB Client API is quite big. We already discussed the notion of layers in the design of the Client API, allowing you to select at what level you want to
work with at any given point in time. There is quite a lot that you can configure and customize in the client behavior. The most common way to do that is to 
change the conventions that the client is using and to register listeners to change the behavior. 

Most of the decisions that the RavenDB Client API is making are actually controlled by the `DocumentConvnetions` class. This class allow you to modify all sorts
of behaviors. From how RavenDB should treat complex values in queries to selecting what property to use as the document id in entities. 

If you need fine grained control over the serialization and deserialization of your documents, this is the place to look. And it holds important configuration 
such as the maximum number of request to allow per server or whatever we should allow queries without a `Take` clause. Listing 4.28 shows as example of controlling
what collection an object will find itself at. 

```{caption="Customize the collection to allow polymorphism" .cs}
store.Conventions.FindCollectionName = type =>
    typeof(Customer).IsAssignableFrom(type)
        ? "customers"
        : DocumentConventions.DefaultGetCollectionName(type);
```

In Listing 4.28, we are letting RavenDB know that `Customer` or any derived type should be in the "customers" collection. That means that we can create a class 
called `VIPCustomer` which will have additional properties, but it will be treated as a `Customer` by anything else (indexing, queries, etc). Such options allow
you to have absolute control over how RavenDB willw ork within your environment.

### Listeners

Alongside the conventions, which are typically reactive. You can use listeners to perform various operations during the execution of requests by RavenDB. The 
listeners are available as events that can be subscribed to at the document store or the individual session levels. 

The following events are available:

* OnBeforeStore
* OnAfterStore
* OnBeforeDelete
* OnBeforeQueryExecuted

This allow us to register to be called whenever a particular event happens, and that in turn give us a lot of power. Listing 4.29 shows how we can implement
auditing with the help the `OnBeforeStore` event.

```{caption="Implementing auditing mechanism" .cs}
store.OnBeforeStore += (sender, args) =>
{
	args.DocumentMetadata["Modified-By"] = 
		RequestContext.Principal.Identity.GetUserId();
};
```

Listing 4.29 will ensure that whenever the document is modified, we'll register in the metadata which user made the modification. You can also use this event
to handle validation in a cross cutting fashion. Throwing an error from the event will abort the `SaveChanges` operation and raise the error to the caller. 

Another example can be to ensure that we always include a particular clause in our queries, as you can see in Listing 4.30.

```{caption="Never read inactive customer" .cs}
store.OnBeforeQueryExecuted+= (sender, args) =>
{
    if (args.QueryCustomization is IDocumentQuery<Customer> query)
    {
        query.WhereEquals("IsActive", true);
    }
}
```

The code in Listing 4.30 will apply to all queries operating on `Customer` and will ensure that all the results returned have the `IsActive` property set to true. 
This can also be used in multi tenancy situation, where you want to add the current tenant id to all queries. 

The full details of what you can do with conventions and listeners are in the online documentation, and I encourage you to browse that and consider whatever it
would sense to use those in your application. Listeners in particular can be quite a time saver because they can be applied globally at ease. 

## Versioning and revisions

In the previous section we briefly mentioned auditing. In this one, we are going to take notion of auditing and dial it up a few times, just to see what will 
happen. Certain classes of application have very strict change control requirements for their data. For example, most medical, banking payroll and insurance 
applications, to name the first few domains that pop to mind, have strict "never delete, we need to be able to see all changes on the system". One particular
system I worked with had the requirement that we'll keep all changes for a minimum period of seven years, for example. 

With RavenDB, this kind of system is a lot easier since RavenDB has builtin support for versioning. Allow me to walk you through setting up such as system.
Go into the RavenDB Studio in the browser, create a new database, as we have seen in [Chapter 2](#zero-to-ravendb). Now, go into the database, and on the
left side menu click on `Settings` and then `Versioning`. 

You can configure versioning globally for all collections, or for a single particular collection. For the purpose of this exercise, we'll define versioning
for all collections, as see in Figure 4.3.  Enable versioning by and click on `Save`. 

![Setting up versioning for all documents](./Ch04/img03.png)

Now that we have enabled versioning, let us see what this means. Go ahead and create a simple customer document, as you can see in Figure 4.4.

![Versioned customer document](./Ch04/img04.png)

You can see that this document has a `@flags` metadata property that is set to `Versioned`. And if you'll look at the right hand side you'll see a revisions
tab that you can select that will show you to previous revisions of this document. Play around with this document for a bit, modify it and save and see how
revisions are recorded on each change.

Revisions in RavenDB are created whenever you have versioning enabled and a document is modified. As part of saving the new document, we create a snapshot 
of the document (and its metadata, attachments, etc) and store it as a revision. This allows us to go back in time and look at a previous revisions of a document. 
In a way, that is  very similar to how we work with code. Every change create a new revision, and we can go back in time and compare the changes 
between revisions.

> **Versions in a cluster**
>
> In a cluster, revisions are going to be replicated to all the nodes in the database. Conflicts cannot occur with revisions, since each revision have
> its own distinct signature, and one of the most common usages of revisions is to store the entire document history when you have automatic conflict
> resolution. We'll cover this behavior in depth in [Chapter 5](#clustering-setup).

As part of configuring versioning on the database, you can select how many revisions we'll retain, and for what period of time. For exmple, you may choose to
keep around 15 revisions, for 7 days. Under those conditions, we'll delete all revisions which are _both_ older than 7 days and have more than 15 revisions 
after them. In other words, if you have made 50 changes to a document in the span of a week, we'll keep all of them, and only delete the earliest of them 
when it is over 7 days old.

From the client side, you don't really need to consider revisions at all. This behavior is happening purely on the server side and require no involvement from
the client. But you can access the revisions through the Client API, as you can see in Listing 4.31.

```{caption="Getting revisions of a document" .cs}
List<SupportCall> revisions = session.Advanced
	.GetRevisionsFor<SupportCall>("SupportCalls/238-B");
```

The code in Listing 4.31 will fetch the most recent revisions (you can also do page through the revision history) and provide you with the revisions of the 
document as it was changed. 

> **Revisions from an older version of your software**
>
> One thing that you should note when using the revisions feature is that over time, as your software evolve, you might see revisions from previous
> versions of your application. As such, the revision document might be missing properties, have properties that have been removed or had their 
> type changed.
>
> Because revisions are immutable, it isn't possible to run migration on them, and you need to take that into account. When working with revisions
> you might want to consider working with the raw document, rather than turn that into an instance of an object in your model. 

Once a revision has been created, it cannot be changed. This is done to ensure compliance with strict regulatory requirements, since it means that you can
treat the data in the revision as safe. It cannot be manipulated by the Client API or even by the admin. 

I found that some applications, without the regulations requiring versioning, can make use of revisions just because they give the users an easy way to look at
the changes on an entity over time. That was especially true when the user in question was the business expert who was in charge of the whole system. This 
feature was very valuable to her since it allowed here to see exactly what happened by whom (the application utilized a listener to implent audits, as shown
in Listing 4.29). It was quite interesting to see how much value she got out of this feature.

In terms of cost, versioning obviously increase the amount of disk space required, but given today's disk sizes, that isn't usually a signficant concern. Aside
from disk space utilization, revisions don't actually have any impact on the system. 

## How RavenDB uses JSON

The RavenDB Server and the RavenDB C# Client API use a dedicated binary format to represent JSON in memory. The whole of [Chapter 27](#blittable)
is dedicted to this format, but it is worth understanding a bit about how RavenDB think about JSON even at this stage. Typically you'll work
with JSON documents in their stringified form, a set of UTF8 characters with the JSON format. That is human readable, cheap to parse and quite
easy to read and work with.

But JSON parsing requires you to work in a streaming manner, which means that to pull up just a few values from a big document, you still need to
parse the full document. As it turns out, once a document is inside RavenDB, there are a _lot_ of cases where we want to just get a few values 
from it. Indexing a few fields is very common, and parsing the JSON each and every time can be incredibly costly. Instead, RavenDB accept the JSON
string on write and turn it into an internal format, called Blittable^[I don't like the name, but we couldn't come up with anything better]. 

A blittable json document is a format that allows RavenDB random access to any piece of information in the document without having to parse the 
document with property traversal cost of (amortised) O(1). Over the wire, RavenDB is sending JSON strings, but internally, it is all blittable.
The C# client is also using blittable format internally, since that helps a _lot_ with memory consumption and control. You generally won't see
that in the public API, but certain very low level operations may expose you to it.

Blittable documents are immutable once created and _must_ be disposed after you are done with them. Since the document session will typically hold
such blittable objects, the session _must_ also be disposed to make sure that all the memory it is holding is released. An important consideration
for the overall performance of RavenDB is that blittable documents always reside in native memory. This is done because it allows RavenDB fine grained
control over where and how the memory is used, reused and its life cycle. 

On the client side, using the blittable format means that we have to deal with reduced memory consumption and reduced fragmentation, and it also reduce
the speed of caching significantly.

## Data Subscriptions

The final topic of this chapter is also one of my favorites. In many applications, you have the need to process the data inside the database. It can be
because you need to run analytics on your orders, take steps at various steps in a workflow or run business processes over the documents stored inside
RavenDB. The common thing that is shared among all of those requirements is that we need to process all documents matching a particular criteria and
we must do that on an ongoing basis.

In other words, subscriptions are a good way to handle any scenario in which I want to run a business process on a set of data, and continue to run the 
process on all incoming data that also match my criteria. Typically, you would implement such a process by issuing a query, remembering your most recently
read value and going from there. But those kind of systems require polling, rather complex to build and scale and in general require a lot of upkeep around 
them.

In contrast, RavenDB Subscriptions allow you to create a subscription for a set of documents matching a given criteria and the database itself will manage
the subscription and make sure that you have process all the documents matching your subscription. This also include error handling, recovery and retries,
on the fly updates, etc. 

Working with subscriptions is divided into two distinct operations. Creating the subscription and opening it. In Listing 4.32 you can see how we create a 
subscription to process customers. 

```{caption="Creating a subscription to process customers" .cs}
var options = new SubscriptionCreationOptions<Customer>();
string subsId = store.Subscriptions.Create(options);
```

The result of the `Create` call is a subscription id, which we can pass to the `Open` call. Only a single client can have a subscription opened at any given
point in time. The code in Listing 4.32 will create a subscription that will match all the `Customer` documents in the database. Listing 4.33 shows how we
open the subscription and register to get notifications. 

```{caption="Opening and using a subscription to process customers" .cs}
var options = new SubscriptionConnectionOptions(subsId);
var subscription = store.Subscriptions.Open<Customer>(options));
subscription.Subscribe(/* redacted for brevity */);
subscription.Start();

// wait until the subscription is done
// typically, a subscription lasts forever
subscription.SubscriptionLifetimeTask.Wait();
```

There isn't much in Listing 4.33. We open a subscription using the previous gotten `subsId`, then we `Subscribe`... something, I redacted it for now, and 
we'll see what it is in the nexte section. We then start the subscription and it is the next bit that is interesting. We wait on the `SubscriptionLifetimeTask`,
why do we do that?
In general, a subscription will live for a very long time, typically, the lifetime of the process that is running it. In fact, typically you'll have a process
dedicated just to running subscriptions (or even a process per subscription). 

The actual handling of the incoming documents is done by the subscribers, and it is done on a background thread. On the other hand, a subscription will typically
remained subscribed to the server until it is deleted, another subscription client took over its connection or an error occured in the processing of the 
subscription that it cannot recover from. Either way, we need to wait until any of those happen, and that is handled by waiting on `SubscriptionLifetimeTask`. 

In Listing 4.33 we used the `subsId` from Listing 4.32. This is typical to how we are using subscriptions. The subscription id should be persistent and survive
restat of the process or machine, becuase it is used to represent the state of the subscription and what doucments have already been processed by that particular
subscription.

You can set your own subscription id during the `Create` call, which give you well known subscription name to use, or you can ask RavenDB to choose one for you, 
as we have done in Listing 4.32. Note that even if you use a hard coded subscription id, it still needs to be created before you can call `Open` on it. 

Why do we have all those moving parts? We have the creation of the subscription, opening it, tracking its lifetime and we haven't even gotten to the part where
we are actually processing documents using it. 

The reason for this is that subscriptions are very long lived processes, which are resilient to failure. One a subscription is created, a client will open it 
and then keep a connection open to the server, getting fed all the documents that match the subscription criteria. This is true for all existing data in the
database. 

Once we have gone over all the documents currently in the database, the subscription will go to sleep, but will remain connected to the server. Whenever a new
or updated document will match the subscription criteria, it will be sent again. Errors during this process, either in the network, the server or the client are
tolerated and recoverable. Subscriptions will ensure that a client will recieve each matching document at least once^[Although errors may cause you to recieve the 
same document multiple times, you ara guranteed to never miss a document]. 

Typically, on a subscription that already processed all existing documents in the database, the lag time between a new document coming in and the subscription 
recieving it is a few milliseconds. Under load, when there are many such documents, we'll use batches of documents and send them to the client for processing
as fast as it can process them.

After every successfully completed batch, the subscription will acknowledge to the server that it completed processing this batch, at which point the next batch
will be sent. It is the process of acknowledging the processing of the batch that make the process reliable. Until the client confirmed reciept of the batch,
we'll not move forward and send the next one. 

> **Subscription in a cluster**
>
> The subscription will connect to a server in the cluster, which may redirect the subscription to a more suited server for that particular subscription. Once
> the subscription found the appropriate server, it will open the subscription on that server and start getting documents from it. A failure of the client will
> result in a retry (either from the same client or possibly another one that was waiting to take over). A failure of the server will cause the client to 
> transparently switch over to another server. 
>
> The entire process is highly available on both client and server sides. The idea is that once you setup your subscriptions, you just need to make sure that the
> processes that open and process the subscription is running, and the entire system will hum along, automatically recovering from any failures along the way.

Okay, that is about as much as we can talk about subscriptions without actually showing what they are _doing_. Let us go to handle the actual document processing.

### Processing documents via subscriptions.

We previously seen the `Subscribe` method, in Listing 4.33. But we haven't seen yet what is actually subscribing. The `Subscribe` method is simply taking an 
`IObserver<Customer>` argument, we can also pass a lambda there, with the help of Reactive Extensions. Listing 4.34 shows the code to handle the subscription
that we redacted from Listing 4.33. 

```{caption="Processing customers via subscription" .cs}
subscription.Subscribe((SubscriptionResult<Customer> result) =>
{
	Customer customer = result.Document;
	// do something with this customer
});
```

After all this build up, the actual code in Listing 4.34 is pretty boring. What subscriptions gives us is an observer over documents. In this case, we register
a lambda to be called with a customer. This code will first go through all the customers in the database, invoking the lambda one at a time. Once we have gone
through all the customer documents, this subscription will wait, and whenever a new customer comes in, or an existing customer is modifed, we'll be called with
that document. 

What can we do with this? Well, quite a lot, as it turns out. We can use this to run all sort of business processes. For example, we may want to check if this
customer have a valid address, and if so, record the GPS coordinates so we can run spatial queries on it. Because of the way subscriptions work, we get a full
batch of documents from the server, and we can run heavy processing on them, we aren't limited by the data streaming over the network, unlike streaming, we won't 
run out of time here.

> **Subscriptions are background tasks**
>
> It may be obvious, but I wanted to state this explicitly, subscriptions are background tasks for the server. There is no requirement that a subscription 
> will be opened at any given point in time, and a subscription that wasn't opened will simply get all the documents that it needs to since the last 
> acknowledged batch.
> 
> That means that if a document was modified multiple times, it is possible that the subscription will only be called upon it once. See the section about
> Versioned Subscription if you care about this scenario.

One of the things we can do here is to open a new session, modify the document we got from the subscription, `Store` the document and call `SaveChanges` on it, 
right from the subscription lambda itself. But note that doing so will also typically put that document right back on the path to be called again with this 
subscription, so you need to be aware of that and protect against infinite loops like that. 

### Error handling with subscriptions

What happens when there is an error in the processing of a document? Imagine that we had code inside the lambda in Listing 4.34 and that code threw an exception.
Unless you set `SubscriptionConnectionOptions.IgnoreSubscriberErrors`^[You probably shouldn't do that.], we will abort processing of the subscription and the
`SubscriptionLifetimeTask` will raise an error. Typical handling in that scenario is to dispose of the subscription and immediately open it again. 

Assuming the error is transient, we'll start processing from the last batch we got and continue forward from there. If the error isn't transient, for example,
some `NullReferenceException` because the code isn't check for it, the error will obviously repeat itself. You might want to set an upper limit to the number
of errors you'll try to recover from in a given time period, and just fail completely afterward. This depend heavily on the kind of error reporting and 
recovery you are using in your applications. 

Note that this apply only to errors that came from the code processing the document. All other errors (connection to server, failover between servers, etc) are
already handled by RavenDB. The reason that we abort the subscription in the case of subscriber error is that there really isn't anything else that we can do.
We don't want to skip processing the document, and just logging the error is possible (in fact, that is exactly what we do if `IgnoreSubscriberErrors` is set)
but no one ever reads the log until the problem was already discovered, which is typically very late in the game. 

### Conditional Subscriptions

Subscriptions so far are useful, but not really something to get excited about. But the fun part starts now. Subscirptions aren't limited to just fetching all
the documents in a particular collection. We can do much better than this. Let us say that we want to send a survey for all customers where we had a complex
support call. The first step for that is to created a subscription using the code in Listing 4.35.

```{caption="Creating a subscription for complex calls" .cs}
var options = new SubscriptionCreationOptions<SupportCall>(
	call => 
	   call.Comments.Count > 25 
	&& call.Votes > 10 
	&& call.Survey == false);
string subsId = store.Subscriptions.Create(options);
```

We are registering for subscriptions on support calls that have more than 10 votes and over 25 comments, and we add a flag to denote that we already sent the
survey. This is just part of the work, of course, we still need to handle the subscription itself. This is done in Listing 4.36.

```{caption="Taking surveys of complex calls" .cs}
subscription.Subscribe((SubscriptionResult<SupportCall> result) =>
{
	SupportCall call = result.Document;
	if( (DateTime.Today - call.Started) > DateTime.FromDays(14))
		return; // not need to send survey for old stuff


	using(var session = store.OpenSession())
	{
		var customer = session.Load<Customer>(
			call.CustomerId);

		call.Survey = true;		

		session.Store(call, result.Etag, result.Id);

		try
		{
			session.SaveChanges();
		}
		catch(ConcurrenyException)
		{
			// will be retried by the subscription
			return;
		}	

		SendSurveyEmailTo(customer, call);
	}
});
```

A lot of stuff is going on in Listing 4.36. Even though we removed the code for actually opening the subscription (this is identical to Listing 4.33) there is 
still a lot going on. First, we fetch the customer for this support call, then we mark the call as having sent the survey. Then we call `Store` and pass it
not just the instance that we got from the subscription, but also the etag and id for this document. This ensures that when we call `SaveChanges`, if the 
document has changed in the meantime on the server side, we'll get an error.

In this case, this is an expected error, and if we got a concurrency exception, we can just ignore it and skip processing this document. There is a bit of a 
trickery involved here. Because the document have changed, the subscription will get it again anyway, so we'll skip sending it now, but we'll be sending it 
later.

Finally, we send the actual email. Note that in real production code, I'll also need to decide what to do if sending the email failed. In this case, I'm 
assuming that it cannot fail, and favor skipping sending the email rather then sending it twice. Typical mail system have options to ignore duplicate emails
in a certain time period, which is probably how I would solve this in production.

#### Complex conditionals 

### Complex scripts

Load customer as part pf returning it.


### Versioned Subscriptions

## Summary