
# Deep dive into the RavenDB Client API

In this chapter we are going to take a deep dive into how the client API works. We are going to show mostly C# code examples, but the 
same concepts apply to any of the RavenDB Client APIs, regardless of platforms, with minor changes to make it fit the platform.

There are still some concepts that we haven't gotten around to (clustering or indexing, for example) which will be covered in 
their own chapters. But the Client API is very rich and has a lot of useful functionality on its own, quite aside from the server
side behavior. 

We already looked into the document store and the document session, the basic building blocks of CRUD in RavenDB. But in this chapter we
are going to look beyond the obvious and into the more advanced features. One thing we'll _not_ talk about in this chapter is querying.
We'll talk about that extensively in [Chapter 9](#map-indexes), so we'll keep it there. You already know the basic of querying in RavenDB
but there is a _lot_ more power waiting for you to discover there.

This chapter is going to contain a _lot_ of code examples and discuss the nitty gritty details of using the client. It is also divided into 
brief sections that each deal with a specific feature or behavior. I suggest reading over this to note what the capabilities of RavenDB are
and coming back to it as needed in your application.

For the rest of this chapter, we'll use the classes shown in Listing 4.1 as our model, using a simplified help desk as our example. 

```{caption="Simplified Help Desk sample model" .cs}
public class Customer
{
	public string Id { get; set; }
	public string Name { get; set; }
}

public class SupportCall
{
	public string Id { get; set; }
	public string CustomerId { get; set; }
	public DateTime Started { get;set; }
	public DateTime? Ended { get;set; }
	public string Issue { get; set; }
	public int Votes { get; set; }
	public List<string> Comments { get; set; }
}
```

## Writing documents

Writing documents in RavenDB is easy, as we saw in [Chapter 2](#zero-to-ravendb). If we want to create a new support call, we can use the code in 
Listing 4.2 to do so.

```{caption="Creating a new support call using the session" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);
	session.SaveChanges();
}
```

This is basic behavior of RavenDB, and how you would typically work with saving data. But there are lot of additional things that we can do when
writing data. For example, the user might have sent us some screen shots that we want to include in the support call. 

### Working with attachments

You can add attachments to a RavenDB document to store binary data relating to that document. Let us assume that the user have sent us a screen shots
of the problem along with the call. Listing 4.3 shows how we can store and retrieve the attachments.

```{caption="Saving attachments to RavenDB as part of opening the support call" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);

	foreach(var file in attachedFiles)
	{
		session.Advanced.StoreAttachment(call, file.Name, 
			file.OpenStream());
	}

	session.SaveChanges();
}
```

Note that we are using the session to store both the support call document and any attachments that the user might have sent. An attachment is basically
a file name and a stream that will be sent to the server (with an optional content type). When the call to `SaveChanges` is made, the RavenDB Client API
will send both the new document and all of its attachments to the server in a single call, which will be treated as a transaction. Both the document and
the attachments will be saved, or both will fail.

That was easy enough, but how do we get those back? The list of attachments for a particular document is accessible via the document metadata, as shown
in Listing 4.4. 

```{caption="Getting the list of attachments for a support call" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  var attachments = session.Advanced.GetAttachmentNames(call);

  // render the call and the attachment names
}
```

Calling `GetAttachmentNames` is cheap, since the attachments on a document are already present in the document metadata, which we loaded as part of
getting the document. There is no server side call involved. Note that the result of `GetAttachmentNames` does not include the _content_ of the 
attachments. To get the attachment itself, and not just its name, you need to make a separate call, as shown in Listing 4.5.

```{caption="Getting an attachment content" .cs}
using(var session = store.OpenSession())
{
	var call = session.Load<SupportCall>("SupportCalls/238-B");
	var attachments = session.Advanced.GetAttachmentNames(call);

 	using(var stream = session.Advanced.GetAttachment(call, 
 		attachments[0].Name))
 	{
		// process the content of the attachment 	
 	}
}
```

Each call to `GetAttachment` will make a separate call to the server to fetch the attachment, if you have a lot of attachments, be aware that 
fetching all of their information can be expensive due to the number of remote calls that are involved. 

### Working with the document metadata

In the attachments section, we noted that attachment information is stored in the document metadata. RavenDB used the metadata for a lot of things,
most of them you don't generally care about (etag, change vector, etc). But the document metadata is also available to you for your own needs and 
use. 

Actual use case for direct use of the document metadata are actually pretty rare. If you want to store some information, you'll typically want to 
store it in the document itself, not throw it to the sidelines in the metadata. Typical use cases for storing data in the medata are corss cutting
concerns. The preeminent one is auditing (who editted this document, for example). 

In order to demonstrate working with the metadata, we'll consider the case that creating a support call is a complex process that has to go through
several steps. In this case, while we saved the document to RavenDB, it is still in draft status. Typical modeling advice would be to model this 
explicitly in the domain (so you'll have a `IsDraft` or `Status` property on your model), but for this example, we'll use the metadata. You can see
the code for setting a draft status in the metadata in Listing 4.6.

```{caption="Setting a metadata flag as part of creating a new support call" .cs}
using(var session = store.OpenSession())
{
	var call = new SupportCall
	{
		Started = DateTime.UtcNow,
		Issue = customerIssue,
		CustomerId = customerId
	};
	session.Store(call);

	var metadata = session.Advanced.GetMetadataFor(call);
	metadata["Status"] = "Draft";
	
	session.SaveChanges();
}
```

We can call `GetMetadataFor` on any document that has been associated with the session. A document is associated with the session either by loading
it from the server or by calling `Store`. After the document has been associated with the session, we can get its metadata and manipulate it.

Changes to the metadata count as changes to the document and will cause the document to be saved to the server when `SaveChanges` is called. 

### Change tracking and `SaveChanges`

The document session implements change tracking on your documents as you can see in Listing 4.7.

```{caption="Setting a metadata flag as part of creating a new support call" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  
  call.Ended = DateTime.UtcNow;

  session.SaveChanges();
}
```

The change tracking (and identity map) by the session means that you don't have to keep track of what changed and manually call `Store`. Instead, when
you call `SaveChanges` all you changes will be sent to the server in a single request. 

You have a few knobs available to tweak the process. `session.Advacned.HasChanges` will let you know if calling `SaveChanges` will result in a call to the 
server and `session.Advanced.HasChanged(entity)` will tell you whatever a particular `entity` has changed. You can also take it up a notch and ask RavenDB
to tell you _what_ changed using `session.Advanced.WhatChanged()`, which will give you all the changes that happened in the session. The `WhatChanged` 
feature can be very nice if you want to highlight changes for user approval, for example, or just want to see what modification happened in your model
after a certain operation.

You can also tell RavenDB to not update a particular instance by calling `session.Advanced.IgnoreChangesFor(entity)`. The document will remain attached to
the session, and will be part of any identity map operations, but it won't be saved to the server when `SaveChanges` is called. Alternatively, you can call
`session.Advanced.Evict(entity)` to make the session completely forget about a document. 

All of those operations tend to be very rare and are usuallly used only in very specific cases, but they are very powerful when utilized properly. 

### Optimistic concurrency

We covered optimistic concurrency in [Chapter 3](#modeling), but only in the most general terms. Now, let us take a look at see how we can use optimistic
concurrent in practice. Listing 4.8 shows two simultaneous sessions modifying the same support call. 

```{caption="Concurrent modifications of a support call" .cs}
using(var sessionOne = store.OpenSession())
using(var sessionTwo = store.OpenSession())
{
  var callOne = sessionOne.Load<SupportCall>("SupportCalls/238-B");
  var callTwo = sessionTwo.Load<SupportCall>("SupportCalls/238-B");
  
  callOne.Ended = DateTime.Today;
  callTwo.Ended = DateTime.Today.AddDays(1);
  
  sessionOne.SaveChanges();
  sessionTwo.SaveChanges();
}
```

In the case of the code in Listing 4.8, we are always going to end up with the support call end date set to tomorrow. This is because by default, RavenDB
uses the `Last Write Win` model. You can control that by setting `store.Conventions.UseOptimisticConcurrency` to true, which will affect all sessions, or
on a case by case basis by setting `session.Advanced.UseOptimisticConcurrency` to true on the session directly.

In either case, when this flag is set, when `SaveChanges` is called, we'll send the modified documents to the server alongside their etag at the time we
read them from the server. This allows the server to reject any stale write. If the flag was set to true, the code in Listing 4.8 will result in a 
`ConcurrencyException` on the `sessionTwo.SaveChanges()` call.

This ensures that you cannot overwrite changes that you didn't see and if you set `UseOptimisticConcurrency` you need to handle this error in some manner.

#### Pessimistic locking

Optimistic locking handle the issue of changes to the document we modified that happened behind our backs. Pessimistic locking prevents them entirely. 
RavenDB does _not_ support pessimistic locking, but while you really need support from the database engine to properly implment it, we can pretend to
have something that is near enough to be interesting to talk about, if not actually use. The following is recipe for using
pessimistic locking in RavenDB, not so much because it is a good idea, but because it allows us to explore several different features and see how they
all work together.

Using pessimistic locking we lock a document for modification until we release the lock (or a certain timeout has gone by). We can build a pessimistic 
lock in RavenDB by utilizing the document metadata and optimistic concurrency. It is easier to explain with code and you can find the `Lock` and `Unlock`
implementations in Listing 4.9.

> **The locks are opt in**
>
> In RavenDB, both the pessimistic lock explored in this section and the optimistic lock in the previous section are opt in. That means that you have to
> explicitly participate in the lock. If you are using `UseOptimisticConcurrency` and another thread isn't, that thread will get the `Last Write Wins`
> behavior (and might overwrite the changes made by the thread using optimistic conccurency).
>
> In the same manner, the pessimistic lock recipe described here is depedant on all parties following it, if there is a thread that isn't, the lock will
> not be respected.
>
> In short, when using concurrency control, make sure that you are using it in all acrodd the board, or it may not hold.

```{caption="Extension method to add pessimistic locking to the session" .cs}
public static IDisposable Lock(
	this IDocumentSession session, 
	string docToLock)
{
  var doc = session.Load<object>(docToLock);
  if (doc == null)
      throw new DocumentDoesNotExistException("The document " + 
      	docToLock + " does not exists and cannot be locked");
  var metadata = session.Advanced.GetMetadataFor(doc);
  if (metadata.GetBoolean("Pessimistic-Locked"))
  {
      // the document is locked and the lock is still value
      var ticks = metadata.GetNumber("Pessimistic-Lock-Timeout");
      var lockedUntil = new DateTime(ticks);
      if (DateTime.UtcNow <=  lockedUntil)
      	throw new ConcurrencyException("Document " + 
      		docToLock + " is locked using pessimistic");
  }
  
  metadata["Pessimistic-Locked"] = true;
  metadata["Pessimistic-Lock-Timeout"] = 	
  	DateTime.UtcNow.AddSeconds(15).Ticks;
  
  // will throw if someone else took the look in the meantime
  session.Advanced.UseOptimisticConcurrency = true;
  session.SaveChanges();
  return new DisposableAction(() =>
  {
      metadata.Remove("Pessimistic-Locked");
      metadata.Remove("Pessimistic-Lock-Timeout");
      Debug.Assert(session.Advanced.UseOptimisticConcurrency);
      session.SaveChanges();
  });
}
```

There is a quite a bit of code in 4.9, but there isn't actually a lot that gets done. We load a document, and check if its metadata contains the
`Pessimistic-Locked` value. If it does, we check whatever the timeout for the lock has expired. If it isn't locked, we update the document metadata,
enable _optimistic_ concurrency and then call `SaveChanges`. If no one else modified the document in the meantime, we will succuessfully mark the 
document as ours, and any other call to `Lock` will fail.

The `Lock` method returns an `IDisposable` instance which handle releasing the lock. This is done by removing the metadata values and then calling 
`SaveChanges` again. If the lock has timed out, and someone took the lock, we'll fail here with concurrency exception as well.

> **Avoid your own distributed pessimistic locks**
>
> There is a reason why RavenDB does not include a pessimistic lock feature, and I strongly recommend to avoid using the recipe above. It is here 
> because it shows how to use several different features at once to achieve a goal. 
>
> Actually handling distributed lock is a non trivial issue. Consider a RavenDB cluster with multiple nodes. If two lock requests will go to two
> distinct nodes at the same time, _both_ of them will succeed^[We'll discuss how RavenDB cluster work in the next chapter in detail.]  The two nodes
> will quickly discover the conflicting updates on both the other side and generate a conflict. But it isn't guaranteed that they will do that inside
> the lock / unlock period. 
> 
> Another issue is the subject of timing, if two clients have enough of a clock skew, a client might consider a lock to have expired even though it is
> valid. Proper distributed locking require a consensus protocol of some kind and aren't trivial to build or use. RavenDB _have_ a consensus protocol
> but pessimistic locking is usually a bad fit for OLTP environment, and we decided to not implement it.

Typical uses for pessmistic locks are to lock a document while a user is editing it. That might sound like a good idea, but experience has shown that 
in most cases, it lead to a lot of trouble. Consider for example version control systems. If you are reading this book, you have likely used a SCM of
some kind. If you haven't, please stop reading this book and pick up a book about source control of _some_ kind, that is far more important.

Early source control systems (SourceSafe as a good example) used locks as their concurrency model, and that led to a lot of problems. Joe taking a lock
on the file and then leaving for vacation is a typical problem that is brought up in such cases. The same happens whenever you have pessimistic locks.
Implementing pessimistic locks also require you to implement forced lock release, "who is locking this document" feature and a whole bunch of management
functions around it. It is typically easier to implement optimistic concurrent or merging, and it matches most users expectations a lot more.

#### Offline optimistic concurrency

We looked at online optimistic concurrency in Listing 4.8, when we load a document into the session, modify it, and then save. In that time frame, if 
there was a change, we'll get a concurrency exception. But most software doesn't work like that. In a web application, you aren't going to keep the 
document session open for as long as the user is on the site. Instead, you'll use a session per request model, most likely. And the user will first 
load a page with the document's content and modify it in another request. There isn't a shared session in sight, so how can we implement optimistic
concurrency.

All you need to do is to send the etag of the document to the user, and accept it back when the user want to save the document. Listing 4.10 shows an
example of using two separate session with concurrency handling between them.

```{caption="Concurrent modifications of a support call" .cs}
long etag;
Supportcall callOne;

using(var sessionOne = store.OpenSession())
using(var sessionTwo = store.OpenSession())
{
  callOne = sessionOne.Load<SupportCall>("SupportCalls/238-B");
  etag = sessionOne.Advanced.GetEtagFor(callOne);
  
  var callTwo = sessionTwo.Load<SupportCall>("SupportCalls/238-B");
  
  callTwo.Ended = DateTime.Today.AddDays(1);
  
  sessionTwo.SaveChanges();
}

using(var sessionThree = store.OpenSession())
{
  sessionThree.Advanced.UseOptimisticConcurrency = true;

  callOne.Ended = DateTime.Today;

  sessionThree.Store(callOne, etag, callOne.Id);

  sessionThree.SaveChanges(); // will raise ConcurrencyException
}
```

The code in Listing 4.10 first load the support call in `sessionOne`, then load it again in `sessionTwo`, modify the support call and save it to the server. 
Both sessions are then closed, and we open a _new_ session. We call `Store`, passing the entity instance, the etag that we got from the first session as 
well as the document id. 

This give the RavenDB Client API enough information so we can do an optimistic concurrency check from the time we loaded `callOne` in the first session. In
web scenario you'll typically send the etag alongside the actual data, and get it back from the client to do the check. You might also want to checkout the 
`Changes API`, which is covered a little later in this chapter, which might be of help to get early change notifications when you need to implement offline
optimistic concurrency.

### Patching documents and concurrent modifications

Typical workflow with RavenDB is to open a session, load some document, run some business logic and then call `SaveChanges`. When you follow those steps, the
session will figure out what documents have changed and will send them to the server. That model is simple, easy to follow and the recommended method to work.

However, there are a few scenarios where we don't want to send the entire document back to the server. If our document is very large, and we want to make a 
small change, we can avoid that cost. Another reason to want to avoid the full document save is because we have a scenario that calls out for concurrent work
on a document.

Let us consider the `SupportCall.Votes` property, two users may very well want to vote on the same support call at the same time. One way to handle that is to
load the support call, increment the `Votes` property and call `SaveChanges`. In order to handle concurrent modifications, we will utilize optimistic 
concurrency and retries. But that is quite a lot of work to write, and if the document is large, it is also a lot of data that goes back and forth over the 
network for very little reason. Listing 4.11 shows how we can do much better.

```{caption="Incrementing a property using the Patch API" .cs}
using(var session = store.OpenSession())
{
  session.Advanced.Increment<SupportCall>("SupportCalls/238-B", c => c.Votes, 1);

  session.SaveChanges();
}
```

What the code in Listing 4.11 does is to generate a patch request, rather then load and save the full document. That request is stored in the session, and when
`SaveChanges` is called, it will be sent to the server (alongside any other changes / operations made on the session, as usual). On the server side, we'll apply
this operation to the document. This patch request is safe to call concurrently, since there is no loss of data as a result of executing the patches. 

> **What is fater, patching or load/save?**
>
> The obvious answer is that obviously patching is faster, we send a lot less data, and we need one less roundtrip to do so. Winning all around, right?
>
> The answer is that this is a bit more complex. The patch request is actually a JavaScript function that we send, which means that we need to parse and run
> it on the server side, potentially marshal values into the script environment and then marshal it back. Conversely, the code path for loading and saving
> documents in RavenDB is _very_ well trodden and had a lot of optimizations. That means that in many cases it might be easier to just load and modify the
> document directly, rather than use a patch.
>
> Patching _aren't_ expensive, I want to emphasis, but at the same time, I have seen codebases where _all_ writes had to be made using patching, because of 
> percieved performance benefits. That resulted in extremely hard to understand system that was resistent to change. The general recommendation is that you'll
> utilize patching only when you need to support concurrnet modifications.
>
> Note that in most cases, concurrent modifications of the same document is _not_ the default. A properly modeled document should have a single reason to change
> but it is very common that documents have additional data on them (like the `Votes` property, or `Comments`) which are important to save, but don't have any
> real business logic attached to them. 
> 
> That kind of change is fine to do using patching. If you find yourself trying to run serious business logic in patch scripts (we'll see exactly how to do this
> in a bit), you should move that into your own business logic. 
>
> Another important consideration for patching, you don't need to worry about concurrency between patches on the same document. There is no guarantee about the
> order in which the patches will run, but there aren't going to be concurrent / interleaved executing of the scripts on the same document.

A slightly more complex example of the use of patching is adding a comment to a `SupportCall`. Just like before, we want to support adding a comment 
concurrnetly, but to make things a bit more interesting, we'll add the business rule that a call that has been ended cannot have additional comments added to
it. Listing 4.12 shows the obvious way to do so.

```{caption="Adding a comment to a support call using patch" .cs}
using(var session = store.OpenSession())
{
  var call = session.Load<SupportCall>("SupportCalls/238-B");
  if(call.Ended != null)
  	throw new InvalidOperationException("Cannot comment on closed call");

  session.Advanced.Patch(call, c => c.Comments, 
  	comments => comments.Add("This is important stuff!"));

  session.SaveChanges();
}
```

In Listing 4.12 you can see how we moved from using the simple `Increment` call to the `Patch` which allow us to either replace a property value completely or
to add an item to a collection. If you look closely at the code in Listing 4.12 you'll find that there is a hidden race condition there. If the business rule
is that we cannot add comments to a closed call, we have the possibility that we'll load the call, check and see that its `Ended` property is null, and then 
send the patch request to the server. However, in the meantime, another client could have closed the call and yet we'll still add the comment.

How serious an issue this would be is entirely dependant on the domain and the model. It is possible that you _do_ want to add comments during that period, 
or it is possible that allowing it could break important business invariants. 

> **Patch requests are sent as part of `SaveChanges`**
>
> It is probably obvious, but I wanted to spell it out explicitly. Calls to `Patch`, `Increment` or `Defer` do not go to the server immediately, instead, they
> are added to the list of operations the session need to execute and will be sent to the server in a single batch (along with any modified documents) when
> `SaveChanges` is called. 
> If you have multiple patch operations in the same session on the same document, they'll be merged into a single patch, and if there
> are multiple patches on different documents they will all be execute on the same transaction, as a single unit.

There are two ways to avoid this race condition. We can send an etag to the server asking it to fail with a concurrency exception if the document has been 
modified since we last saw it. That _works_, but it defeats the whole _point_ of using patches for concurrent modification of the document. The second alternative
is to move the invariant check into the script itself. Calls to `Increment` and `Patch` are actually just wrapper around the `Defer` call, which allows you to
add work to the session to be sent to the server when `SaveChanges` is called.

In Listing 4.13 we are dropping down to using `Defer` directly to manipulate the patch request ourselves, with no wrappers. As you can see, this is a bit invovled
but overall it is pretty straightforward.

```{caption="Using a patch script to maintain call invariants" .cs}
using(var session = store.OpenSession())
{
  session.Advanced.Defer(new PatchCommandData(
    id: "SupportCalls/238-B",
    etag: null, 
    patch: new PatchRequest
    {
      Script = @"
if(this.Ended == null)
    throw 'Cannot add a comment to a closed call';
this.Comments.push(comment);
",
      Values =
      {
        ["comment"] = "This is important stuff!!"
      } 
   },
   patchIfMissing: null));

  session.SaveChanges();
}
```

The code in Listing 4.13 pass a `PatchCommandData` to `Defer`, and pass the relevant document. The key part is in the `PatchRequest` itself. We do the check on
the document, and fail if the call has already been closed, otherwise, we add the comment to the call. You can also see that we don't have to deal with string
concatenation here, since we can pass arguments to the scripts directly. There is also the option to run a script if the document does not exists. This gives you 
the option to do a "modify or create" style of operations.

Using this method, we can be sure that we will never violate the rule about adding comments to a closed call. But this is a bit more complex than anything we had
before, and I would only recommend doing so if you really must. A cautionary word, though. This is a very powerful technique, but it is also open to abuse. 
Running logic in this manner inside the database is usually a very bad idea. If you find yourself doing something like this frequently, stop and reconsider. 

This is the sharp end of the stick, and abusing this feature can cause problems. In particular, the scripts are run under a lock, and can slow or prevent the 
database from completing transactions quickly. 

### Deferring commands

In the previous section, we used the `Defer` method to register a `PatchCommandData` on the session, to be executed when `SaveChanges` is called. But `Defer` is
more generic then this. It is a general mechanism for the user to register arbitrary commands that will take part of the same transaction as the `SaveChanges`. 

> **RavenDB API is like an ogre, it has layers**
>
> The RavenDB API is composed of layers. At the top, you have the document store and the document session. Those are built using Operations concept, which you
> will typically not use directly except in rare cases. And the Operations are handled via the request executer, which allows you to generate requests directly
> to the server (and take advantage of RavenDB's authentication and automatic failover). 
>
> The case of `Defer` is a good example. Instead of forcing you to drop down all the way, we expose an extension point in the session so you can plug in a 
> command of your own and piggyback on the session handling of transactions.

The available commands range from putting and deleting documents and attachments, applying patches and deleting documents using a prefix. Aside from the patch
operation, the rest are only ever useful in very rare cases. The most common use of `Defer` beyond patch is when you need fine grained control over the operations
that will be executed in a single transaction, to the point where you want to control the ordering of the operations.

RavenDB doesn't allow changing a document collection, so if you need to do this^[And this should be a very rare thing indded] you cannot just update the 
`@collection` metadata. Instead, you have first delete the old document and create a new one with the appropriate collection. The session doesn't allow to both
delete and modify a document and for the purpose of discussion, let us say that we _have_ to make this in a single transaction, so no other client may see a 
point in time where the document was deleted.

To be perfectly clear, this is a really strange situation, dreamt up specifically to showcase a feature that should be only used for very special circumstances.
This escape hatch in the API is intended specifically so you'll not be blocked if you need something that we didn't foresee, but I can't emphasis enough that 
this is probably a bad idea. The emergency exit is very important, but you don't want to make it the front door.

Another reason to typically want to avoid using `Defer` is that it is lowered in the RavenDB Client API layers, instead of dealing with high level concepts 
like entity, you'll be dealing with the direct manner in which RavenDB is representing JSON, the blittable format. That format is meant to be high performance
and things like developer convienance were secondary in its design. 

### Bulk inserting documents to RavenDB

RavenDB is fast, _really_ fast, but it still needs to face operational realities. The 
[Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing) still apply, and I/O takes a non trivial amount of time.
That means that when you want to get the best speed out of RavenDB, you need to help it achieve it. 

Listing 4.14 shows the absolutely _slowest_ way to write 10,000 documents into RavenDB.

```{caption="Writing 10,000 documents, one at a time" .cs}
var sp = Stopwatch.StartNew();
for (int i = 0; i < 10_000; i++)
{
 using (var session = store.OpenSession())
 {
     session.Store(new Customer
     {
         Name = "Customer #" + i
     });
     session.SaveChanges();
 }
}
Console.WriteLine(sp.Elapsed);
```

For fun, I decided to run the code in Listing 4.14 against the live test instance that we have. That instance is in San Francisco, and I'm testing this from
Israel. The test instance is also running as a container inside an AWS t2.medium machine (2 cores & 4 GB of memory, with burst only mode). In other words, this
is a performance test that is heavily biased against RavenDB. And the results aren't really great. In fact, they are _bad_.

> **Stacking the deck**
>
> I'm going to be talking about a lot of performance numbers in this section, and I wanted to be clear that I have choosen (intentionally) the worst possible
> situation for RavenDB and then compounded the issue by using the wrong apporaches to be able to show off the real costs in a manner that is visible.
>
> I'll refer you again to the [Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing). I'm trying to select
> a scenario that would break as many of those fallacies as I can, and show how RavenDB is able to handle them.

This is because we are running each write as an independent operation, and we have to wait for the previous operation to complete before we can start the new one.
What is more, the database server handles just a single request concurrently, which means that we have no way to amortize I/O costs across multiple requests. This
is the absolutely worst way you can write large amount of documents into RavenDB, because the problem is that most of the time is spent just going back and forth
between the client and the application. On each request we have to make another REST call, send a packet to the server, etc. On the other side, the server accept
a new request, process it (and commit it to disk). During the entire process, it is effectively idle, since most of the time is spent in waiting for I/O. That is a big waste all around.

You can see the various very nicely when looking at the Fiddler^[The [Fiddler web proxy](http://www.telerik.com/fiddler) is a great debugging tool in general, and 
very useful to peek into the communication between RavenDB server & clients.] statistics. Each request takes about 220 - 260 millseconds to run. Writing the first
1,000 documents took 4 minutes and 6 seconds and 2,000 requests took 8 minutes on the clock. The full 10,000 documents would take about 40 minutes or so. 

Granted, we are intentionally going to a remote server, but still... What happens when we are running the writes in parallel? The code in Listing 4.15 will show
up how to do this.

```{caption="Writing 10,000 documents, with a bit of parallelism thrown in" .cs}
var sp = Stopwatch.StartNew();
Parallel.For(0, 10_000, i =>
{
  using (var session = store.OpenSession())
  {
      session.Store(new Customer
      {
          Name = "Customer #" + i
      });
      session.SaveChanges();
  }
});
Console.WriteLine(sp.Elapsed);
```

Using the method in Listing 4.15, I was able to write 1,000 documents in 56 seconds, and we got to 2,000 in a minute and a half, 3,000 in a minute and 50 seconds, 
etc. The reason for the speed up is actually related to thread pool handling on the client side. Since we make a lot of blocking requests, the thread pool figure out
that we have a lot of blocking work, and creates more threads. That means that we have the chance to do more concurrent work, so as times goes by, more threads are
created and we make additional concurrent requests to RavenDB. 

The total time to write 10,000 documents in this setup was 2 minutes and 52 seconds. So we are just over half the time to write a tenth of this number when using 
sequential writes. The code in Listing 4.15 is still using syncronous calls, which means that the client side is spinning threads to handle the load and we are
limited by the rate of new threads creation on the client. 

RavenDB also supports async API, which is much more suitable for scale out scenarios, because we aren't holding a thread the duration of the connection. Listing 
4.16 shows how we can write all those documents in parallel and using the async API. The code is a bit complex, because we want to control the number of concurrent
requests we make. Spinning 10,000 concurrent requests will likely load the network and require careful attention to how they are managed, which is out of scope
for this book. Instead, I limited the number of concurrent connections to 128.

```{caption="Writing 10,000 documents, using async API" .cs}
var sp = Stopwatch.StartNew();
var semaphore = new SemaphoreSlim(128);

async Task WriteDocument(int i)
{
    using (var session = store.OpenAsyncSession())
    {
        await session.StoreAsync(new Customer
        {
            Name = "Customer #" + i
        });
        await session.SaveChangesAsync();
    }
    semaphore.Release();
}

var tasks = new List<Task>();
for (int i = 0; i < 10_000; i++)
{
    semaphore.Wait();
    tasks.Add(WriteDocument(i));
}

Task.WaitAll(tasks.ToArray());

Console.WriteLine(sp.Elapsed);
```

The code in Listing 4.16 is also using a local method, which is a new C# 7.0 feature, it allows to package a bit of behavior quite nicely, and is very useful for
small demos and internal async code. This code writes 1,000 documents in a bit under 10 seconds, and complete the full 10,000 writes in under 30 seconds (29.6, on
my machine). The speed difference is again, related to the client learning our pattern of behavior and adjusting itself accordingly (creating enough buffers, threads
and other resources needed, warming up the TCP connections^[TCP Slow Start can be a killer on benchmarks]).

However, we really had to make an effort, write explicit async code and manage it, rate limit our behavior and jump through several hoops to get a more reasonable
performance. Note that we went from over 40 minutes to less than 30 seconds in the span of a few pages. Note that we haven't actually modified _what_ we are doing,
we only changed how we are talking to the server, but it had a huge impact on performance.

You can take it as a given that RavenDB is able to process as much data as you can feed it, and the typical problem in handling writes is how fast we can get the 
data to the server, not how fast it can handle it. 

RavenDB contains dedicated API and behavior that make it easier to deal with bulk loading scenarios. The Bulk Insert API uses a single connection to talk to the
server and is able to make much better usage of the network. The entire process is carefully orchestrated by both client and server to optimize performance. 
Let us look at the code in Listing 4.17 first and then discuss the details.

```{caption="using bulk insert to write 100,000 documents, quickly" .cs}
var sp = Stopwatch.StartNew();

using (var bulkInsert = store.BulkInsert())
{
 for (int i = 0; i < 100_000; i++)
 {
     bulkInsert.Store(new Customer
     {
         Name = "Customer #" + i
     });
 }
}

Console.WriteLine(sp.Elapsed);
```

The code in Listing 4.17 took 2 minutes and 10 seconds to run on my machine. Which is interesting, because it seems slower from the async API usage sample, right?
Except that I made a typo when writing the code and wrote a _hundred_ thousands documents, instead of _ten_ thousands. If I was writing merely 10,000 documents, it
would complete in about 18 seconds or so. The code is fairly trivial to write, similar to our first sample in Listing 4.14, but the performance is many times higher.

To compare the costs, I run the same code against a local machine, giving me a total time to insert a 100,000 documents of 11 seconds (instead of 2 minutes 
remotely). If we want to compare apples to apples, then the cost for writing 10,000 documents are showing in Table. 4.1.


|                |   Remote    |    Local    |
|----------------|-------------|-------------|
| Session        | 41 *minutes*| 20 seconds  |
| Bulk Insert    | 18 seconds  | 6.5 seconds |

Table:  Bulk insert costs locally and remotely

You can see that while bulk insert is significantly faster in all cases, being over three times faster than the session option (Listing 4.14) locally is losing 
its meaning when you consider that it is over 130 times faster in the remote case. The major difference as you can imagine is the cost of going over the network, 
but even on the local machine (let alone the local network) there is a significant performance benefit for bulk insert.

Amusingly enough, using bulk insert still doesn't saturate the server, and for large datasets, it is advisable to have parallel bulk insert operations going at
the same time. This give the server more work to do, and allow us to do a lot of optimizations that increase the ingest rate of the server.

The way bulk insert work is by opening a TCP connection to the server and writing the raw data directly into the database. That means that we don't need to go 
back and forth between the client and server and can rely on a single connection to do all the work. The server, for its part, will read the data from the network
and write it to disk when it is best to do so. In other words, bulk inserts are _not_ transactional. A bulk insert is actually comopsed on many smaller transactions
whose size and scope is decided on by the server based on its own determination, in order to maximize performance.

When the bulk insert is completed, you can rest assure that all the data has been safely committed to disk properly, but during the process, the database will 
incrementally commit the data instead of going with a single big bang mode.

In general, RavenDB performance is ruled mostly by how many requests you can send it. The more requests, the higher the degree of parallelism and the more efficent 
RavenDB can work. In our internal tests, we routinely bumped into hardware limits (the network card cannot process packets any faster, the disk I/O is saturated, 
etc), not software ones. 

## Reading documents



### Lazy requests

### Streaming data

## Cross cutting concerns on the client

### Conventions

### Listeners

metadata, auditing

## Versioning and revisions

### Changes API


## How RavenDB uses JSON

The RavenDB Server and the RavenDB C# Client API use a dedicated binary format to represent JSON in memory. The whole of [Chapter 27](#blittable)
is dedicted to this format, but it is worth understanding a bit about how RavenDB think about JSON even at this stage. Typically you'll work
with JSON documents in their stringified form, a set of UTF8 characters with the JSON format. That is human readable, cheap to parse and quite
easy to read and work with.

But JSON parsing requires you to work in a streaming manner, which means that to pull up just a few values from a big document, you still need to
parse the full document. As it turns out, once a document is inside RavenDB, there are a _lot_ of cases where we want to just get a few values 
from it. Indexing a few fields is very common, and parsing the JSON each and every time can be incredibly costly. Instead, RavenDB accept the JSON
string on write and turn it into an internal format, called Blittable^[I don't like the name, but we couldn't come up with anything better]. 

A blittable json document is a format that allows RavenDB random access to any piece of information in the document without having to parse the 
document with property traversal cost of (amortised) O(1). Over the wire, RavenDB is sending JSON strings, but internally, it is all blittable.
The C# client is also using blittable format internally, since that helps a _lot_ with memory consumption and control. You generally won't see
that in the public API, but certain very low level operations may expose you to it.

Blittable documents are immutable once created and _must_ be disposed after you are done with them. Since the document session will typically hold
such blittable objects, the session _must_ also be disposed to make sure that all the memory it is holding is released. An important consideration
for the overall performance of RavenDB is that blittable documents always reside in native memory. This is done because it allows RavenDB fine grained
control over where and how the memory is used, reused and its life cycle. 

On the client side, using the blittable format means that we have to deal with reduced memory consumption and reduced fragmentation, and it also reduce
the speed of caching significantly.

## Caching

### Aggresive caching

## Data Subscriptions

### Versioned Subscriptions
