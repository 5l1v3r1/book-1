
## Backups and restores

Backups are _important_. I don't really think that I need to tell you how much. At the same time, we need to discuss what is the
point of having backups. There are two reasons to have backups:

* Restore the database after losing the data.
* Recover data from an earlier point in time.

Let's consider each one of them independently. Restoring after data loss can happen because an operator accidently deleted the
wrong database, the hard disk or the entire server died, etc. 
I intentionally picked these two examples, because they represent very different scenario. In the later case, a hardware failure
resulting in the loss of a node, the other members in the cluster will just pick up the slack. You can setup a new node or 
recover the old one and let the cluster fill in any missing details automatically^[Remember that any single node system cannot
have any uptime SLA, since a single node failure will bring it all down.].

The case of accidental deletion of the database is more worrying. In this case, the database is gone from all the nodes in the
cluster. At this point, what can save you is an offsite replica. By that I mean a database to which you had an exteral 
replication setup. Because it isn't part of the same database group, it will not be impacted by the deletion, and you can 
manually fail over to it while you replicate the data back. 

> **Restoring databases can take time**
> 
> The scenario of deleting a database, or losing a whole cluster seems like the perfect reason why you'll want to have a backup.
> Why wouldn't we just restore from backup at this point? The answer is simple, _time_.
>
> Imagine that we have a decent size database, 250GB in size. Just copying the raw data from the backup destination to the machine
> on which we want to restore can take a long time. Let's assume that we have a Provisioned IOPS SSD on Amazon Web Services 
> (high speed hard disk recommended for demanding database loads). Ignoring any transport / decompression costs, the raw speed in 
> which you can write to a disk like that means that it will take about 10 minutes just to copy the backup to the local disk, and 
> another 10 minutes (with just I/O costs, ignoring everything else) for the actual restore. 
>
> That gives us a minimum of 20 minutes for the restore, assuming we are using a high end disk and are only limited by the speed
> of the disk itself. However, the I/O costs aren't the only thing to consider and the actual restore time can be higher. Most
> of the cost for restoring a database is actually spent it getting the data to the machine, by the way. Backups are often 
> optimized for long term storage and speed of access is not prioritized (tape storage, for example).
>
> Because of this, if you are interested in minimizing downtime in such scenarios, you would have a separate offsite replica that
> you can use. We discussed this at length in the previous chapter. There is a balance between how much protection you want and
> how much you are willing to pay for. If your threat scenario does not include an admin deleting a database by mistake or losing
> the entire cluster in one go, you probably don't need this.
> Alternatively, you may decide that for such a scenario, the time to restore from backup is acceptable.

The option of restoring the database to a particular point in time is used a lot more often than restoring after data loss. This
can be very helpful in many cases. You might want to restore from backup on an independent machine, to try to troubleshoot a 
particular problem, or to see what was in the database at that time. In many cases, there are regulatory requirements that backups
should be kept for a certain period of time (often a minimum of 7 years).

In short, backups are _important_, but I said that already. This is why I'm dedicating a full chapter for this topic and why 
RavenDB has a whole suite of features around scheduling, managing and monitoring backups. We'll start by going over how backup
works in RavenDB, to make sure that you understand what are your options and what are the implications of the choises you make.
Only after that we'll start setting up backups and performing restores. 

### How backups work in RavenDB

Backups are often stored for long periods of time (years) and as such, their size matter quite a lot. The standard backup option
for RavenDB is a gzipped json dump of all the documents and other data (such as attachments) inside the database. This backup
option gives you the smallest possible size for your data and make it easier and cheaper to store it. On the other hand, when 
you need to restore the data, RavenDB will need to re-insert and re-index all the data. This can increase the time that it takes
to restore the database.

> **Avoid backing up at the files on disk**
>
> It can be tempting to try to backup the database at the file system level. Just copy the directory to the side and store it
> somewhere. While it seems easy, this is _not_ supported and likely to cause failures down the line. RavenDB has an involved
> set of interactions with the file system, with a carefully choreographed set of calls to ensure ACID compliance. 
>
> Copying the directory to the side will usually not capture a point in time of the data and is likely to cause issues. When
> creating backups, RavenDB ensures that there is a point in time freeze of the database at a particular point to handle
> this scenario. In short, you should be using RavenDB's own backup system, not rely on the file system for that.

An alternative to the backup option is the snapshot. A snapshot is a binary copy of the database and the journals at a 
given point in time. Like the regular backup, snapshots are compressed, but other than that, they are pretty much ready 
to go as far as the database is concerned. 
The restore process of a snapshot involves extract the data and journal files from the archive and starting the database normally.

The advantage of a snapshot is that it is _much_ faster to restore the database, but it is also typically much larger than a 
regular backup. In most cases you'll have both a regular backup defined for long term storage (where the restore speed doesn't
matter) and a snapshot backup written to immediately accessible storage (such as local SAN) for quick restores. 

Both backups and snapshots perform a full backup of the database. In other words, this is a full clone of the database at the
point in time in which the backup started. However, there are many cases where you don't want to have a full backup everytime.
You want just the changes that happened from the last backup. This is called incremental backup and is available for both
backups and snapshots.

Incremental backup is defined as the set of changes that happened since the last backup / snapshot. Regardless of whatever you
use a backup or a snapshot, an incremental backup is always using gzipped json (RavenDB doesn't do incremental snapshots). The
reason for that is that applying incremental backups to a snapshot is typically very quick and won't significantly increase the
time to restore the database, while incremental snapshots can be very big. One of the primary reasons incremental backup exists
in the first place is to reduce the cost of taking backups, after all. 
Figure 17.1 shows the interplay between snapshots, full backups and incremental backups from a real production database (my blog).

![Snapshot, full backup and following incremental backups for the `blog.ayende.com` database](./Ch17/img01.PNG)

In Figure 17.1 you can see a snapshot taken on April 17th at 2 AM on node A as well as two incremental backups after that. The
second folder shows a full backup on April 16th at 2 AM and two incremental backups after that. In both cases, the database 
is the `blog.ayende.com`, which powers my personal blog. The database size on disk is 790MB, so you can see that even for 
snapshots, we have quite a big space saving. On the other hand, this is a pretty small database. Figure 17.2 shows the same
snapshot / backup division for a database that is about 14 GB in size. 

![Snapshot and backup for a large database on S3](./Ch17/img02.PNG)

The reason why even snapshots are so much smaller than the raw data is that the backup is compressed, even for snapshots. 
The cost of decompressing the backup is far overshadowed by the cost of I/O at such sizes. However, encrypted databases
typically cannot be compressed, so you need to be prepared for snapshots that are the same size of the database (_very_ big).

In Figure 17.1 you can see that there are only a couple of incremental backups, and in Figure 17.2 we have a lot more. This
is because (while both backups were defined with roughly the same incremental backup duration) they show very different 
databases. The blog database is seeing infrequent writes, and when the incremental backup runs and see that there have been
no changes since the last time, there is nothing for it to do, so it skips a backup run. On the other hand, whenever a full
backup run, even if there have been no changes, you'll still get a full backup.

On the other hand, in Figure 17.2 we are backing up a database that is under relatively constant write load, so you'll see
an incremental backup on every run, although you can see that there are significant differences between the sizes of the 
incremental backups. 

> **Incremental backups record the current state**
> 
> An important consideration for the size of incremental backup is the fact that the _number_ of writes don't matter as
> much as the number of _documents_ that have been written to. In other words, if a single document was modified a thousand
> times, when the incremental backup runs, the latest versin of the document will be written to the backup. If a thousand
> different documents were written, we'll need to write all of them to the backup.
> That kind of difference in behavior can produce signficiant size changes between incremental backups. 

Backups record the current state of documents, but if you want to get all the changes in between, you can use revisions.
And just like documents, revisions are also included in the backup. This means that even if you store revisions inside
your database for a short period of time, you can still restore a document to any point in time by digging into the 
relevant revision from historical backups.

#### What is in the backup?

A backup (or a snapshot) contains everything that is needed to restore the database to full functionality. Table 17.1 shows all
the gory details about what exactly is being backed up. This requires us to understand a bit more about where RavenDB store
different information about a database.

|  Database   |           Cluster                    |
|-------------|--------------------------------------|
| Documents   | Database Record (including tasks)    |
| Attachments | Compare exchange values              |
| Revisions   | Identities                           |
| Tombstones  | Indexes                              |
| Conflicts   | Tasks state (snapshot only)          |
         
Table: What is backed up for a database and at what level. 

We already discussed the differences between database group and the cluster (see Chpater 7). At the database level we manage 
documents and any node in the database group can accept writes. At the cluster level, we use a consensus algortihm to ensure
consistency of changes for the database. Such operations include identities, creating indexes, etc.
These details are stored at the cluster level and are managed by the cluster as a whole, instead of independently on each node.

> **Incremental backup of cluster level state**
> 
> At the cluster level, RavenDB dumps the entire cluster level state of a database to the backup on any backup (full or 
> incremental). If you have a _lot_ of identities (very rare) or plenty of compare exchange values (more common), you might
> want to take that into account when defining the backup frequency. 

Identities and compare exchange values can be very important for certain type of usages and they are stored outside of the
database itself. When we backup a database, the cluster level values are also backed up. Another important factor is the database
tasks, such as the backup definitions and schedule, ETL tasks, subscriptions, etc. 

#### Important considerations

When the database is restored, the tasks that were defined to it are also restored. In other words, if you have an ETL task
defined for a production database and you restore the backup on a development machine, you need to disable the tasks. Otherwise,
assuming your development server can reach the ETL targets, it might start running these tasks and writing to places you don't
want it to.

The same apply to external replication, backups and any other tasks that was defined for the database. The restore includes all
these tasks, which is what you want when you restore a down node, but it is something to note if you are restoring a backup on
the side. During restore, you have the option of disabling all such tasks, so you'll have restored the database cleanly, but
have to manually select which tasks to re-enable. That option should be set when you are not restoring the bacup to
the same environment (and purpose) as before.

If you are using encrypted databases, you need to be aware that the snapshot backup has the actual data still encrypted, but all
the cluster level data are stored in plain text (even if the server store itself is encrypted). And regular backups are always
in plain text. As part of your backup strategy, you need to consider the security of the backup themselves. You can read more 
about backups and encrypted databases in Chapter 14.

#### Backing up of the cluster itself

We talked about backing up databases, but what about backing the cluster as a whole? In general, the cluster is mostly concerned
with managing databases, there isn't any persistent state beyond the database data that needs backing up. If you'll look at 
Table 17.1, you can see that all the details there, whatever they are stored at the cluster level or the database level are all
for a particular database.

The only details at the cluster level that aren't directly related to a database are about the cluster itself (nodes, topology, 
task assignments, history of health checks, etc). All of that data isn't really meaningful if you lost the entire cluster, 
so there is no real point in preserving that. 

Backups in RavenDB are always at the database level. Recreating the cluster from scratch take very little time, after all. Once that
is that you can restore the individual databases on the new cluster. 

#### Who is doing the backup?

A database in RavenDB is usually hosted on multiple nodes in the cluster. When it comes to backup, we need to ask a very important 
question, who is actually going to run the backup? Individual database instances in the database group are independent of 
each other but hold the same data. We don't want to have each of the nodes in the database group create their own backup. That 
would lead us to create duplicated backups and wasting a lot of resources.

A backup task, just like ETL tasks or subscriptions is a task that is set for the entire database group. The cluster will decide
which node in the database group is the owner of this task and that node will be the one in charge for running backups. The 
cluster know to pick a node that is up to date and will move the responsability for the backups to another node if the owner
node has failed.

For full backups, this doesn't matter. Any node will perform the backup from its current state as usual. If the cluster decdied that
a different node will execute the backup, they are clearly marked and timestamped. From an operational perspective, there is no
difference between the nodes in this regard.

For incremental backups, the situation is a bit different. You can only apply an incremental backup on top of a full backup from the 
same node that took it. When the cluster decides that incremental backup ownership should switch bcause the owner node is down, the
new node will not run an incremental backup. Intead, it will create a _full_ backup first, and only then it will start creating 
incremental backups.

> **Backups from different nodes to the same location**
> 
> A RavenDB 

Consider the case of a three nodes cluster and a database that is configured to take a full backup every midnight and an incremental
backup every four hours. Node C is the node that is usually assigned to do the backups and indeed, on midnight it took a full backup 
as well as dutifully created the incremental backups at 4 AM and 8  AM. However, at noon, node C is down and the cluster has moved the
responsability for the backups to node A. 

Node A cannot create an incremental backup on top of node C's full backup this will
trigger a full backup from A. If node C is still down at 4 PM node A will still own the backup task and create an incremental
backup since noon. When node C comes back up at 6 PM, the cluster will transfer the backup ownership back to it. At 8 PM node C will 
create an incremental backup (everything since 8 AM, the last time node C took an incremental backup). 

There are a few interesting behaviors that you might want to pay attention to here:

* The cluster ensures that at all times the minimal backup definition is respected. In other words, every 4 hours we'll have an 
  incremental backup of the node's state. 
* Only nodes that are up to date with the database state are considered candidate for backup ownership. If node C was down for 
  so long, it is probably out of date. The cluster will only transfer ownership of the backup task (as well as any other tasks)
  when the node has caught up with any changes that happened while it was down.




### Backup strategies

Snapshot vs. backup

Incremental backups

Scheduling backups

Remote backups

Backup tasks in the cluster

#### External replication as a backup strategy

Delayed

### Restoring databases

Important to practice, can take a LONG time.

How to restore to a single node
How to restore to the whole cluster.

Managing the encryption keys!

### Restoring whole clusters