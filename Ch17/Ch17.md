
## Backups and restores

Backups are _important_. I don't really think that I need to tell you how much. At the same time, we need to discuss what is the
point of having backups. There are two reasons to have backups:

* Restore the database after losing the data.
* Recover data from an earlier point in time.

Let's consider each one of them independently. Restoring after data loss can happen because an operator accidently deleted the
wrong database, the hard disk or the entire server died, etc. 
I intentionally picked these two examples, because they represent very different scenario. In the later case, a hardware failure
resulting in the loss of a node, the other members in the cluster will just pick up the slack. You can setup a new node or 
recover the old one and let the cluster fill in any missing details automatically^[Remember that any single node system cannot
have any uptime SLA, since a single node failure will bring it all down.].

The case of accidental deletion of the database is more worrying. In this case, the database is gone from all the nodes in the
cluster. At this point, what can save you is an offsite replica. By that I mean a database to which you had an exteral 
replication setup. Because it isn't part of the same database group, it will not be impacted by the deletion, and you can 
manually fail over to it while you replicate the data back. 

> **Restoring databases can take time**
> 
> The scenario of deleting a database, or losing a whole cluster seems like the perfect reason why you'll want to have a backup.
> Why wouldn't we just restore from backup at this point? The answer is simple, _time_.
>
> Imagine that we have a decent size database, 250GB in size. Just copying the raw data from the backup destination to the machine
> on which we want to restore can take a long time. Let's assume that we have a Provisioned IOPS SSD on Amazon Web Services 
> (high speed hard disk recommended for demanding database loads). Ignoring any transport / decompression costs, the raw speed in 
> which you can write to a disk like that means that it will take about 10 minutes just to copy the backup to the local disk, and 
> another 10 minutes (with just I/O costs, ignoring everything else) for the actual restore. 
>
> That gives us a minimum of 20 minutes for the restore, assuming we are using a high end disk and are only limited by the speed
> of the disk itself. However, the I/O costs aren't the only thing to consider and the actual restore time can be higher. Most
> of the cost for restoring a database is actually spent it getting the data to the machine, by the way. Backups are often 
> optimized for long term storage and speed of access is not prioritized (tape storage, for example).
>
> Because of this, if you are interested in minimizing downtime in such scenarios, you would have a separate offsite replica that
> you can use. We discussed this at length in the previous chapter. There is a balance between how much protection you want and
> how much you are willing to pay for. If your threat scenario does not include an admin deleting a database by mistake or losing
> the entire cluster in one go, you probably don't need this.
> Alternatively, you may decide that for such a scenario, the time to restore from backup is acceptable.

The option of restoring the database to a particular point in time is used a lot more often than restoring after data loss. This
can be very helpful in many cases. You might want to restore from backup on an independent machine, to try to troubleshoot a 
particular problem, or to see what was in the database at that time. In many cases, there are regulatory requirements that backups
should be kept for a certain period of time (often a minimum of 7 years).

In short, backups are _important_, but I said that already. This is why I'm dedicating a full chapter for this topic and why 
RavenDB has a whole suite of features around scheduling, managing and monitoring backups. We'll start by going over how backup
works in RavenDB, to make sure that you understand what are your options and what are the implications of the choises you make.
Only after that we'll start setting up backups and performing restores. 

### How backups work in RavenDB

Backups are often stored for long periods of time (years) and as such, their size matter quite a lot. The standard backup option
for RavenDB is a gzipped json dump of all the documents and other data (such as attachments) inside the database. This backup
option gives you the smallest possible size for your data and make it easier and cheaper to store it. On the other hand, when 
you need to restore the data, RavenDB will need to re-insert and re-index all the data. This can increase the time that it takes
to restore the database.

> **Avoid backing up at the files on disk**
>
> It can be tempting to try to backup the database at the file system level. Just copy the directory to the side and store it
> somewhere. While it seems easy, this is _not_ supported and likely to cause failures down the line. RavenDB has an involved
> set of interactions with the file system, with a carefully choreographed set of calls to ensure ACID compliance. 
>
> Copying the directory to the side will usually not capture a point in time of the data and is likely to cause issues. When
> creating backups, RavenDB ensures that there is a point in time freeze of the database at a particular point to handle
> this scenario. In short, you should be using RavenDB's own backup system, not rely on the file system for that.

An alternative to the backup option is the snapshot. A snapshot is a binary copy of the database and the journals at a 
given point in time. Like the regular backup, snapshots are compressed, but other than that, they are pretty much ready 
to go as far as the database is concerned. 
The restore process of a snapshot involves extract the data and journal files from the archive and starting the database normally.

The advantage of a snapshot is that it is _much_ faster to restore the database, but it is also typically much larger than a 
regular backup. In most cases you'll have both a regular backup defined for long term storage (where the restore speed doesn't
matter) and a snapshot backup written to immediately accessible storage (such as local SAN) for quick restores. 

Both backups and snapshots perform a full backup of the database. In other words, this is a full clone of the database at the
point in time in which the backup started. However, there are many cases where you don't want to have a full backup everytime.
You want just the changes that happened from the last backup. This is called incremental backup and is available for both
backups and snapshots.

Incremental backup is defined as the set of changes that happened since the last backup / snapshot. Regardless of whatever you
use a backup or a snapshot, an incremental backup is always using gzipped json (RavenDB doesn't do incremental snapshots). The
reason for that is that applying incremental backups to a snapshot is typically very quick and won't significantly increase the
time to restore the database, while incremental snapshots can be very big. One of the primary reasons incremental backup exists
in the first place is to reduce the cost of taking backups, after all. 
Figure 17.1 shows the interplay between snapshots, full backups and incremental backups from a real production database (my blog).

![Snapshot, full backup and following incremental backups for the `blog.ayende.com` database](./Ch17/img01.PNG)

In Figure 17.1 you can see a snapshot taken on April 17th at 2 AM on node A as well as two incremental backups after that. The
second folder shows a full backup on April 16th at 2 AM and two incremental backups after that. In both cases, the database 
is the `blog.ayende.com`, which powers my personal blog. The database size on disk is 790MB, so you can see that even for 
snapshots, we have quite a big space saving. On the other hand, this is a pretty small database. Figure 17.2 shows the same
snapshot / backup division for a database that is about 14 GB in size. 

![Snapshot and backup for a large database on S3](./Ch17/img02.PNG)

The reason why even snapshots are so much smaller than the raw data is that the backup is compressed, even for snapshots. 
The cost of decompressing the backup is far overshadowed by the cost of I/O at such sizes. However, encrypted databases
typically cannot be compressed, so you need to be prepared for snapshots that are the same size of the database (_very_ big).

In Figure 17.1 you can see that there are only a couple of incremental backups, and in Figure 17.2 we have a lot more. This
is because (while both backups were defined with roughly the same incremental backup duration) they show very different 
databases. The blog database is seeing infrequent writes, and when the incremental backup runs and see that there have been
no changes since the last time, there is nothing for it to do, so it skips a backup run. On the other hand, whenever a full
backup run, even if there have been no changes, you'll still get a full backup.

On the other hand, in Figure 17.2 we are backing up a database that is under relatively constant write load, so you'll see
an incremental backup on every run, although you can see that there are significant differences between the sizes of the 
incremental backups. 

> **Incremental backups record the current state**
> 
> An important consideration for the size of incremental backup is the fact that the _number_ of writes don't matter as
> much as the number of _documents_ that have been written to. In other words, if a single document was modified a thousand
> times, when the incremental backup runs, the latest versin of the document will be written to the backup. If a thousand
> different documents were written, we'll need to write all of them to the backup.
> That kind of difference in behavior can produce signficiant size changes between incremental backups. 

Backups record the current state of documents, but if you want to get all the changes in between, you can use revisions.
And just like documents, revisions are also included in the backup. This means that even if you store revisions inside
your database for a short period of time, you can still restore a document to any point in time by digging into the 
relevant revision from historical backups.

#### What is in the backup?

A backup (or a snapshot) contains everything that is needed to restore the database to full functionality. Table 17.1 shows all
the gory details about what exactly is being backed up. This requires us to understand a bit more about where RavenDB store
different information about a database.

|  Database   |           Cluster                    |
|-------------|--------------------------------------|
| Documents   | Database Record (including tasks)    |
| Attachments | Compare exchange values              |
| Revisions   | Identities                           |
| Tombstones  | Indexes                              |
| Conflicts   | Tasks state (snapshot only)          |
         
Table: What is backed up for a database and at what level. 

We already discussed the differences between database group and the cluster (see Chpater 7). At the database level we manage 
documents and any node in the database group can accept writes. At the cluster level, we use a consensus algortihm to ensure
consistency of changes for the database. Such operations include identities, creating indexes, etc.
These details are stored at the cluster level and are managed by the cluster as a whole, instead of independently on each node.

> **Incremental backup of cluster level state**
> 
> At the cluster level, RavenDB dumps the entire cluster level state of a database to the backup on any backup (full or 
> incremental). If you have a _lot_ of identities (very rare) or plenty of compare exchange values (more common), you might
> want to take that into account when defining the backup frequency. 

Identities and compare exchange values can be very important for certain type of usages and they are stored outside of the
database itself. When we backup a database, the cluster level values are also backed up. Another important factor is the database
tasks, such as the backup definitions and schedule, ETL tasks, subscriptions, etc. 

#### Important considerations

When the database is restored, the tasks that were defined to it are also restored. In other words, if you have an ETL task
defined for a production database and you restore the backup on a development machine, you need to disable the tasks. Otherwise,
assuming your development server can reach the ETL targets, it might start running these tasks and writing to places you don't
want it to.

The same apply to external replication, backups and any other tasks that was defined for the database. The restore includes all
these tasks, which is what you want when you restore a down node, but it is something to note if you are restoring a backup on
the side. During restore, you have the option of disabling all such tasks, so you'll have restored the database cleanly, but
have to manually select which tasks to re-enable. That option should be set when you are not restoring the bacup to
the same environment (and purpose) as before.

If you are using encrypted databases, you need to be aware that the snapshot backup has the actual data still encrypted, but all
the cluster level data are stored in plain text (even if the server store itself is encrypted). And regular backups are always
in plain text. As part of your backup strategy, you need to consider the security of the backup themselves. You can read more 
about backups and encrypted databases in Chapter 14.

#### Backing up of the cluster itself

We talked about backing up databases, but what about backing the cluster as a whole? In general, the cluster is mostly concerned
with managing databases, there isn't any persistent state beyond the database data that needs backing up. If you'll look at 
Table 17.1, you can see that all the details there, whatever they are stored at the cluster level or the database level are all
for a particular database.

The only details at the cluster level that aren't directly related to a database are about the cluster itself (nodes, topology, 
task assignments, history of health checks, etc). All of that data isn't really meaningful if you lost the entire cluster, 
so there is no real point in preserving that. 

Backups in RavenDB are always at the database level. Recreating the cluster from scratch take very little time, after all. Once that
is that you can restore the individual databases on the new cluster. 

#### Who is doing the backup?

A database in RavenDB is usually hosted on multiple nodes in the cluster. When it comes to backup, we need to ask a very important 
question, who is actually going to run the backup? Individual database instances in the database group are independent of 
each other but hold the same data. We don't want to have each of the nodes in the database group create their own backup. That 
would lead us to create duplicated backups and wasting a lot of resources.

A backup task, just like ETL tasks or subscriptions is a task that is set for the entire database group. The cluster will decide
which node in the database group is the owner of this task and that node will be the one in charge for running backups. The 
cluster know to pick a node that is up to date and will move the responsability for the backups to another node if the owner
node has failed.

For full backups, this doesn't matter. Any node will perform the backup from its current state as usual. If the cluster decdied that
a different node will execute the backup, they are clearly marked and timestamped. From an operational perspective, there is no
difference between the nodes in this regard.

For incremental backups, the situation is a bit different. You can only apply an incremental backup on top of a full backup from the 
same node that took it. When the cluster decides that incremental backup ownership should switch bcause the owner node is down, the
new node will not run an incremental backup. Intead, it will create a _full_ backup first, and only then it will start creating 
incremental backups.

> **Backups from different nodes to the same location**
> 
> RavenDB encodes the node id in the backup folder name. In such a way, even if you had a backup triggered from multiple nodes
> at the same time, you can tell which node was responsible for which backup. This scenario can happen if there is a split in 
> the network, when the owner node of the backup is unable to communicate with the cluster, but is still functional.
> 
> At this point, the cluster will assign someone else as the owner for the backup (since the original node is missing in action,
> as far as the cluster is concerned). At the same time, the original node didn't get the ownership change from the cluster,
> obviously. That can cause both the original and the new node to run the backup.
> The reason for this behavior is that we want to ensure that we always take backups and prefer to have an extra backup rather
> than go without.

Consider the case of a three nodes cluster and a database that is configured to take a full backup every midnight and an incremental
backup every four hours. Node C is the node that is usually assigned to do the backups and indeed, on midnight it took a full backup 
as well as dutifully created the incremental backups at 4 AM and 8  AM. However, at noon, node C is down and the cluster has moved the
responsability for the backups to node A. 

Node A cannot create an incremental backup on top of node C's full backup this will
trigger a full backup from A. If node C is still down at 4 PM node A will still own the backup task and create an incremental
backup since noon. When node C comes back up at 6 PM, the cluster will transfer the backup ownership back to it. At 8 PM node C will 
create an incremental backup (everything since 8 AM, the last time node C took an incremental backup). 

There are a few interesting behaviors that you might want to pay attention to here:

* The cluster ensures that at all times the minimal backup definition is respected. In other words, every 4 hours we'll have an 
  incremental backup of the node's state. 
* Only nodes that are up to date with the database state are considered candidate for backup ownership. If node C was down for 
  so long, it is probably out of date. The cluster will only transfer ownership of the backup task (as well as any other tasks)
  when the node has caught up with any changes that happened while it was down.
* The backup scheduled is shared among all members of the cluster. In other words, the fact that a node failed doesns't 
  automatically trigger a backup in another node. Only after the appropriate time passed since the last successful backup will
  the cluster actually trigger a backup on a substitue node. 
* Incremental backups apply from the last backup (incremental or full) on that particular node. This means that you can use a 
  single node backup to restore the datbase state fully.
* When the cluster moves the ownership of a backup from nodes, the new owner will first run a full backup (even if only an 
  incremental backup was scheduled to run). This is a good rule of thumb to remember, but it is actually a bit more complex. 
  On backup task reassignment, the new node will check whatever it has done a full backup in the defined duration, if it did
  then only an incremental backup will run (from the last full backup for this node).

The last two points are important for a simple reason. Even though you might have scheduled your full backup to happen during
off hours, a new owner for the backup may cause a full backup to run at a time where you expect an incremental backup. This can
be an issue because of the cost of actually doing a full backup can be high in terms of disk I/O, CPU, etc. If you are running
this at a time when you are under high load (and especially as you already lost at least one node) this can put additional 
pressure on the server. 

To handle that, RavenDB control the amount of resources that a backup can take. The I/O and CPU priority for the backup task 
equals to the normal request processing priority, which is higher than the usual indexing priority. This will likely 
only be an issue if your system is teetering on resource exhaustion as it is. 

Now that we know how backups behave in RavenDB, let's go ahead and see how we configure backups in RavenDB.

### Setting up backups

Backups in RavenDB are setup using the Studio^[You can also setup backups through the API, of course.] by going to `Settings`, 
`Manage Ongoing Tasks` clicking on `Add Task` and selecting `Backup`. You can see how the backup screen looks in Figure 17.3.

![Defining a backup schedule for every Saturday at midnight](./Ch17/img03.png)

In Figure 17.3 we have defined a Backup that is scheduled to run a full at 00:00 AM every Saturday. As you can see, RavenDB
uses cron expressions to define recurring backup schedules. In the case of the backup shown in Figure 17.3, we have defined
only a weekly full backup. The incremental backup duration is empty, so no incremental backups will be generated. 

> **Manual backups aren't a thing for RavenDB**
>
> RavenDB doesn't have a way to specify a one time backup. You can define a backup schedule, force it to run now and discard it, but
> all backup operations in RavenDB are intrinsically tied to a schedule. This is because we treat backups as not something that the
> admin needs to remember to do, but something something that should be scheduled once. At that point, RavenDB will take care of
> everything.

You can see in Figure 17.3 that we didn't select the preferred mentor node. This lets the server assign any node to this task.
On the other hand, even if we chose a preferred node, if that node is down at the time of the backup, the cluster will still
re-assign that task to another node so the backup will happen. 

> **When will my backup run?**
> 
> Time is an awkward concept. In the case of a backup schedule, we have a few conflicting definitions for time:
>
> * The server's local time.
> * The admin's local time.
> * The user's local time.
>
> In some cases, you'll have a match between at least some of them, but in many cases, that will not be the case. 
> When you define a backup schedule for RavenDB, we _always_ use the server local time.  Because this can be 
> confusing, you can see that Figure 17.3 lists the time in triplicate. Once in the server local time (what you
> actually defined), once in the admin's own time and once as a time duration for the next backup. 
>
> RavenDB also expects that your servers' clock will be (more or less) in sync. There is no actual requirement
> for this to be the case, but since we use the server's local time to schedule backups, one node's midnight
> might be middle of the business day for you. In general it is easier if all the nodes agree on the same timezone
> and sync their clock on a regular basis.

You can save the backup as it stands, but that won't really do much. This is a backup that has no destination, so it
is effectively disabled. In order to actually get the backup to do something, we need to tell it where to write the 
backups to. RavenDB supports the following destinations:

* Local / network paths
* FTP / SFTP
* Amazon S3
* Azure Storage
* Amazon Glacier

Figure 17.4 shows how you can define a backup task that will write to a local folder as well as to an Amazon S3
account. The backup will run _once_ each time it is executed, and the backup files will be sent to all the specified
destinations from that single source.

![Setting up multiple destinations for a single backup task](./Ch17/img04.png)

The backup process will run and first write the backup to the local path (or to a temporary path if there is local
path is not specified). Once that part is done, the backup runner will start uploading the backup to all the remote
destinations (in parallel). If any of the remote destinations fail, the entire backup will be considered to have 
failed. 

> **Removing old backups**
> 
> RavenDB does absolutely nothing to remove old backups. This is because there isn't a single good policty that 
> RavenDB can apply in such cases. Instead, RavenDB relies on the operations team to set things up properly. That
> can take the shape of a cron job that will delete backup folders that are too old. This can be as simple adding
> the following to your crontab:
> 
> `0 4 * * * find /path/to/backup/* -type d -mtime +14 -delete`
>
> This will run every day at 4:00 AM and delete all the directories older than 14 days. If you don't have something
> similar to this, backups will accrue until you run out of disk space. In particular, be _very_ careful about 
> setting backups to the _same_ volume as your database is using. There have been many not so humorous at the time
> incidents where the backup took the entire disk space and cause RavenDB to reject writes because there is no more
> space for it.
>
> For remote backups, you'll need to decide what is your policy. It is fairly common to never delete backups in the 
> interest of being able to go back in time. Given how cheap disk space is today, that can be a valid strategy. 

The local backup option also supports network paths (so you can mount an `NFS` volume or use `\\remote\directory`)
and it treats it as if it was a local file. If you define both a local path and a remote one (for example, Azure Storage)
RavenDB will first write to the local path and then upload from there. If the local store is actually a remote one, that
can increase the time to complete the backup.

When selecting the local path, take into account that the backup task may run on different nodes. It is usually better
to use a path that is either an absolute path (after verifying that it is valid on all machines) or a relative paths.
In the later case, the path is relative to the RavenDB executable, not the database location.

I'm going to skip going over the details of each of the backup options, the online documentation does a good job in
covering them and you can probably figure out how to use them even without reading the docs. Let's look at what 
the backups look like after we have defined them.

#### Inspecting backup state

After defining the backup schedule and the backup destinations, you can click on `Save` and the studio will take
you back to the `Manage Ongoing Tasks` page where you'll see the task distribution across the cluster, as you can see
in Figure 17.5.

![The backup tasks automatically distributed between the nodes in the database group.](./Ch17/img05.png)

Figure 17.5 shows how the studio is making it apperant where each task will run. And in Figure 17.6 you can see how
we can get full details on the backup tasks for a particular database. You will need to expand the backup 
tasks for the full details by pressing the details icon.  

![The backup status for each of the backups defined for this database.](./Ch17/img06.png)

Figure 17.6 shows the last successful full and incremental backups as well as the time of the next scheduled backup. You
can also see that you can trigger a backup immediately. Note that the backup will run on the assigned node, not on 
the source node. In this case, even if I'm access this view from node C, if I'll press `Backup now` on the first backup
(which is assigned to node A) it will be executed on node A. 

Backups tasks are always defined as scheduled tasks to run automatically and RavenDB will make it explicit if you haven't 
done so. Figure 17.7 shows how this is done. There is no backup defined for `users.northwind` so RavenDB makes sure that this
is clearly visibile to the operators.

![The backup status for each database is shown as integral part of the database details](./Ch17/img07.png)

You can also see in Figure 17.7 that the last backup status is shown right there in the databases view. This is an 
important piece of information and RavenDB makes sure that it is clearly visible for the operator. 

This concludes the backups topic. You now know everything about how to setup and schedule backups in RavenDB. But 
we aren't actually done, backups are just half the job, we now need to talk about how we are actually going to handle
restores. 

### Restoring databases

We talked about how to generate backups, but the most important parts about backups is actually utilizing them by restoring
a database from backup. RavenDB attempts to make the restore process simple and obvious. To start the restore process you'll
need to go to the `Databases` view in the Studio and click on the cheveron near the `New database` button then select 
`New database from backup`. The result of this is shown in Figure 17.8.

![How to restore a database in RavenDB](./Ch17/img08.png)

Figure 17.8 shows the basic structure of restoring a RavenDB database. You create a new database from backup, provide the name
for the new database (it is cropped from Figure 17.8 for space reasons) and the path for the backup. Once you have provided the
path RavenDB will inspect the directory and list the backups that can be restored from that path.

You can see that in this case, we have a Snapshot from node C from April 22th at 2 AM and an incremental snapshot from the same
node at 12:15 PM of the same day. The next day, April 23, we have a full backup at 2 AM from node B and an incremental backup
at 12:15 PM. 
RavenDB make it easy to decide to what point in time you will restore. RavenDB also handles such details as knowing how to start 
from a full backup or a snapshot and then apply the incremental backups up to the point you have specified. 

Figure 17.8 also contains a bit more information. Some slicing and dicing was required to make it all fit into the pages of this
book but it should be clear enough. Of great importance is the `Disable ongoing tasks after restore` flag. If this is set, RavenDB
will disable all the ongoing tasks (backups, subscriptions, ETL processes, etc) in the database. This is what you want if you are
restoring a database for a secondary purpose (you want another copy of the database, but you don't want the same behavior). You
want to keep this flag disable if you are actually restoring to replace the database. As this is the primary reason why you would
restore databases, this is disabled by default.

> **Backup locations**
> 
> RavenDB has no issue if all your databases go to the same location, it uses the database name as part of a backup folder
> to distinguish which database is responsible for which backup. On restore, you can provide the root backup path for all
> your backups and RavenDB will probe that for all the available backups, sort them by their database and date.
> 
> If you have a lot of databases (or a lot of backups) this can be awkward to search for in a big list. It is usually 
> easier to specify a separate backup folder for each database (such as `e:\backups\blog.ayende.com`, 
> `e:\backups\users.northwind`, etc). 

Figure 17.8 also shows some interesting options. If you are using a full backup option, you can specify that the new database 
will be encrypted (and setup a new key)^[This option is not available for snapshots. A snapshot is a binary copy of the database
at a given point, and either it is already encrypted and has its own key or it is unencrypted and will require an import/export
cycle for encryption.]. Figure 17.9 shows the result of the restoration process.

![Setting out to restore a database](./Ch17/img09.png)

The restore process can take a _long_ time. In Figure 17.9 you can see that the restore process took 2 seconds or so and the
database size was about 700MB (see Figure 17.1 for the kind of backup that was restored). This is a very small database, for
databases who range in the hundreds of GB or more you can expect much higher restore duration. Snapshots speed up the process
significantly, but the cost in I/O is significant and should be accounted for. 
You should practice your restores to see how much they take on your own hardware and account for that in any disaster
recovery plans you make. 

In particular, you need to account for the fact that you need to have the backup media available on the machine you want to
restore it. Consider a backup strategy that backups to a local folder (retained one week), Amazon S3 (retained one year) and 
Amazon Glacier (retained forever). 

Assuming that you want to restore from a recent backup, you can expect pretty speedy restore times. I have tested this on a
AWS machine (`t2.large` with 2 cores, 8 GB of RAM and an EBS volume with 2,000 IOPS). Restoring a full backup from a 5 GB
backup file had a restore time of about 3 hours. 
A full backup that is 5 GB in size is compressed json, which usually have _really_ good compression properties. Such a backup
expanded to a database whose size exceeded 350GB. That takes _time_ to insert to the database. A snapshot can usually
be restored more quickly. But the same 350GB database will have a snapshot of about 30GB and on the `t2.large` machine is
took just under an hour.

> **Remember to account for indexing time as well**
> 
> When measuring the restore time, you need to also take into account that indexes will need to be updated as well. A 
> snapshot restore includes the database state (including all the indexes) at the time of the snapshot, but incremental
> restore on top of that (and full / incremental backups in general) only include the index definitions, not the indexed
> data as well.
>
> RavenDB will complete the restore of the backup when all the documents have been written and will not wait for the indexes
> to complete. Indexing of the data is done normally throught async tasks at that time and may take a while to complete, 
> depending on how much there is to index and what it the initial state. This is done to give you access to the documents
> as soon as possible and enable you to start defining a replication factor for the new database as soon as possible.

I intentionally took the numbers from a relatively low powered machine, since that gives you a good idea on the lower bounds
of these numbers.
Your real production machines are likely to be more powerful, but there is still a length of time in which
the server is going to be restoring the database. And remember that this is when the backup is available on the local machine.
If you need to restore from Amazon S3 you also need to account for the
time to fetch the data from S3 (which can be significant for large databases) and using a service like Amazon Glacier has a
minimum latency of _days_. That has to play a part in your disaster recovery plans. 

#### Backups are not for high availability
 
I've emphasised the length of time restoring from backup can take a few times already, and I feel like I need to explain
exactly why this is the case. There is a (mistaken) belief in some operations teams that they can get away with having 
just a single instance, and any reliability issues they have can be handled by restoring from backup.

To a certain extent, this is true, but this is only true if you don't care about the availability of your system. In 
many cases, relying on restoring from backup may mean that you are stranded without a database for a long time. This is 
especially true if you don't have a regularly excersized process for actually restoring the data.

Conversely, if you have a cluster and a database that spans multiple nodes does not mean that you can just ignore the 
issue of backups. Spreading the database on multiple nodes ensures that any single (or even multiple) nodes going down
will not affect your ability to access (and modify) your data. However, it does absolutely nothing to protect you from 
accidental data modifications or deletes. That is what a backup gives you, the ability to roll back the clock and go
to a previous state of the database. 

Another option that you have that combine these two options is the notion of delayed external replication. You can setup
a replication target that will have a certain amount of lag builtin to it. For example, you can specify that you want a 
6 hours delay in replication. This means that any change in your system will not be reflected on the replica for 6 hours.
That gives you a hot node that you can refer to and recover lost or modified data. 

#### Restoring is done locally

When you restore a database, you always provide a file path. This is a path for the _specific_ node that you are restoring
from and it is local to this node. The database is also going to be restored only to that specific node. This has several
interesting implications.

First, you need a file path. This seems obvious, but it means that you need a way to expose the backups to RavenDB as a file
path. This is easy enough when you are backing up to a local path in the first place, but what happens if you have backed
up to the Azure Blob Storage or to Amazon S3? In that case, you will usually download the files to the local machine and
then restore them from the local path.

Alternatively, you can use projects such as `S3FS-FUSE` and `BlobFUSE` to mount cloud storage as a normal volume in your system.
That will allow RavenDB to access the data directly from these systems without having to first download it to the local machine
and it can save some time in the restoration process.

The second implication of the local restore is that if you want to restore a database to multiple nodes, the typical process
this is done is by restoring it on one node and then modifying the database group topology to spread it to the other nodes
in the system. Part of the reason that we make the database available immediately after backup, even before the indexes had
a chance to run is that it allows us to start replicating the data to other nodes in the cluster sooner.

#### Restoring to mulitple nodes at the same time

As already mentioned previously, restoring from backup can be a lengthy process. Even though we don't recommend that you'll
treat this as a time sensitive operation, you are likely to want to spend as little time on this as possible. One option 
that is usually raised is to restore the database on multiple nodes at the same time. 

The idea is that this can save the secondary step of replicating the data to the other nodes in the cluster, because while
the first node was restoring, so did all the other nodes in database group. The problem with this approach is that this 
ignores the real costs of actual data transfer. In most cases, you'll restore to the node that you have the latest backup
on because that is the one that is going to take the least amount of time to restore on.
Then, once you have the database up and running you can expand it at your leisure. 

Restoring it on other nodes will require to copy all the backup data to the other nodes and then restore there as well. This
is possible, but it rarely ends up speeding the entire process in any significant way and it usually adds overhead to the
process.
There are other issues, as well. When restoring from a full backup on multiple nodes, each instance will generate its own
separate change vectors for each document. RavenDB is able to handle that and can reconcile such changes automatically, but
it still means that each of the nodes will have to send all the data to all the other nodes to compare between them.

When using a snapshot, because this is a binary clone of the database at the specified time, restoring on multiple nodes can
be done without generation of different change vectors. This means that the databases can detect early on that the data they
have is identical and not even have to send it over the network. For this reason, I'm only going to explain how to perform
a multiple node restore for snapshots. Even so, it is probably not something that you should usually do. Here is what you'll
need to do:

* Ensure that on all the nodes in the cluster that you want to restore on you have a way to access the data. This can be 
  copying it to each machine, having it shared on the network, using mount tools such as `S3FS-FUSE` and the like. 
  If you are using some sort of shared option (network share, all nodes reading directly from the cloud) you have to 
  take into account that the nodes might compete with one another for the I/O resources to read from the shared resource.
* On the first node, restore the database using the original name (`users.northwind`, for example). On any of the other
  nodes restore the database using different names (`users.northwind.1`, `users.northwind.2`).
* Wait for the restore process to complete on all the nodes.
* _Soft_ delete the additional databases on each of the nodes. This will remove the database from the cluster, but retain
  the data file on disk. 
* On each of these nodes, rename the database folder on file to the proper database name (`users.northwind`).
* Add the database to all the other relevant nodes.

What does this process does? First, we restore the database on all the nodes in parallel, under different names. Because
snapshot is a binary copy of the database, the nodes end up with the same data on disk. We then soft delete the databases
on all but one of the nodes and rename the database folder to the proper name on disk. When we add new nodes to the 
database it will be sent to all the nodes in the database group, which will open the database at the specified path and
find the already restored data there.

For large restores, that can save quite a bit of time, at the expense of more work for the operator. You'll need to decide
whatever this is worth it for your scenario.

This process works because the change vectors across all the databases instances are the same, so there is no need to 
reconcile the data between the nodes. Since the reason you want to run this on multiple nodes in parallel is to reduce
the time to restore, I'm assuming that you have a non trivial amount of data. If you have a full backup, each node will 
generate different change vectors and each database instance will have to compare each of their documents to the others.
This will _work_, mind you, but it can take time and a _lot_ of network traffic. 

For that matter, any incremental backups that are applied to the sanpshot restore will also have the exact same problem, 
but I'm assuming that the amount of modified documents in this case is small enough to effectively not matter. The nodes
will just need to reconcile the differences between the change vectors of the documents that were changed since the last
full snapshot. That is unlikely to be a singificant number of documents, so should complete rapidly.

#### Restoring encrypted databases

We talked about encryption in detail in Chapter 14, so I'm going to focus here on just the practical details of handling
restoring of encrypted databases. In practice, there are three scenarios that interest us in this regard:

* Restoring a full backup
* Restoring an encrypted snapshot
* Restoring a non encrypted snapshot

If you'll look at Figure 17.8 you'll see that the restore dialog has an `Encryption` dialog, which is how you'll define 
the encryption options for the restored database. For a database restored from backup, the data isn't encrypted in the 
backup file and defining an encryption key will cause the newly restored database to be encrypted.

When you restore from an encrypted snapshot you _must_ provide the encryption key (in the `Encryption` dialog) during 
the restore process. The encryption key is not stored in the snapshot and you must retrieve it from your own records
(when you create a new database with encryption, it will prompt you to save the encryption key to a safe location). 
Once you provide the encryption key for the snapshot, the database will be restored normally and any incremenal 
updates^[It's important to remember, while the snapshot itself is encrypted, the incremental updates since it was
taken are _not_ encrypted and are storing the data in plain text mode.]
that happened after the snapshot was taken will be encrypted and stored in the database.

When resoting an unencrypted snapshot, you cannot provide an encryption key. The snapshot must be first restored and then
you should export/import the data to a separate encrypted database. 

### Restoring whole clusters

We talked so far about the process of backing up and restoring individual databases, what about a whole cluster? As it 
turns out, there is really rarely a need to do such a thing. outside of the databases' data, the only data that the 
cluster contains is the node topology, which will obviously change for the new cluster.

Because of this, you'll typically just setup a completely new cluster and restore the databases from the previous cluster
to the new cluster. If you have multiple databases, you can speed things up by restoring each database to a different 
node to parallelize the process, but the process of setting up new cluster from backup is rarely time sesitive, so it 
might be easier to just do everything from a single location.

