
## Production deployments

The day you deploy to production can be a very scary day. It is the culmination of months or years of work and it is 
a time when you actually get to see the results of your work actually bearing fruits. Of course, sometimes the fruit is 
unripe, because you pushed the big red button too soon. There is no riskier time in production than just after a new
version has been deployed.

I want to recommend the [Release It!](https://pragprog.com/book/mnee/release-it) book by Michael T. Nygard, I read it 
for the first time over a decade ago, and it made an _impact_ on how I think about production systems. It had a lot
of affect on the design and implementation of RavenDB itself, as well. This chapter will cover topics specific for
RavenDB, but you might want to read the Release It! book to understand how to apply some of these patterns to your 
application as a whole. 

There are quite a few concerns that you need to deal with when you are looking at your deployment plan and strategy.
These start with the constraints that you have, such as a specific budget or regulatory concerns about the data and 
how you and where may store it. The good thing about these is that they are often clearly stated and understood. 
You also have the requirements, such as how responsive the system should be, how many users are expected to use it
and what kind of load you have to plan for. Unfortunately, these requirements are often unstated, assumed or just 
kept at a high enough level to be meaningless.

> **You _need_ the numbers**
>
> In 2007, I was asked by a client to make sure that the project we were working on would be "as fast as Google". 
> I took him at his word and gave him a quote for 11.5 billion dollars (which was the Google budget for that year).
> While I didn't get that quote approved, I was able to make the point, and we were able to hammer down what we 
> actually needed from the application.
>
> Giving a quote for $11,509,586,000 is a cry for help. I don't know what _fast_ is. That is not how you measure 
> things in a meaningful way. A much better way to state this is to say something in the format of Table 15.1. 
> 
> | Reqs / sec |   %    | Max duration (ms) |
> |-----------|--------|--------------------|
> |       100 | 99%    |                100 |
> |       100 | 99.99% |                200 |
> |           |        |                    |
> |       200 | 99%    |                150 |
> |       200 | 99.9%  |                250 |
> |       200 | 99.9%  |                350 | 
> 
> Table 15.1: SLA table allowing for max response time for requests under different loads
>
> Table 15.1 is something actional. It tells us that under a load of 100 requests per second, we should complete 
> 99% of requests in under 100ms, and 99.99% requests in under 200ms. If the number of concurrent requests go up, 
> we also have an SLA set for that. We can measure that and see whatever we match the actual requirement. 

Developers typically view production as the end of the project. It is in production and it stays there, and the 
developers can turn to the next feature or maybe even a different project entirely. Operations team usually have the 
opposite view. Now that this in production, this is their role to babysit the system.
I don't intend to go over the benefits of having both operations and development insight during both the design of 
the system and its deployment, or even talk about closer colaboration in general. You can search for the term 
"devops" and read reams about it. 

What I'm going to do is to assume that you are early enough in the process that you can utilize some of the notions
and tools in this chapter to affect how your system is being design and deployed, or if you aren't at this stage, you
are able to start shifting to a point where you match the recommended practices.

And with that, let's dive in and talk about the first thing you need to figure out.

### System resources usage

You must have a 386DX, 20MHz or higher processor, 4MB of memory (8MB recommended) and 
at least 70MB of available hard disk space. Oh, wait. That is wrong. These are actually the system requirements if you
want to install Windows 95. 

I want to refer you back to Table 15.1. Before you can make any decisions, you need to know what kind of requirements 
are going to be placed on _your_ system. Is this a user facing or B2B? How much data is there to handle, how many 
requests are expected? Are there different _types_ of requests? This is important enough topic that it's worth 
repeating. There are a lot of good materials about how to go about figuring this out for your system and I'll also
point you again to the Release It! book for more details. 

There is an obvious correlation between the load on your system and the resources that it consume to handle this load.
Figure 15.1 shows a couple of graphs from a production system. This isn't peak load, but it isn't idle either.^[All
the stats and monitoring details are directly from the RavenDB studio. We'll go over where they are located and what
you can deduce from them in the next chpater.] 

![Load and resource graphs from the RavenDB dashboard on production system](./Ch15/img01.png)

This particular machine is running on Amazon EC2 `t2.large` instance with 2 cores and 8 GB of memory. The machine isn't
particularly loaded, with enough spare capacity to handle peaks of three to four times as many requests per second. Of 
course, this is just a very coarse view of what is going on, it is missing the request latencies. We can get that as
well, as you can see in Figure 15.2.

![Request latencies tracked using the RavenDB Traffic Watch feature](./Ch15/img02.png)

This seems to be fine, the average request time is low, and even the maximum isn't too bad. 
We can dive into percentiles and metrics and all sort of details, but at this point will get too specific to be of much 
relevance for this book. 

If you are already in production the RavenDB studio already surface these numbers to you, which means that you can act
upon them. For one thing, the correlation between the requests on the server and the load on the server can be 
clearly seen in Figure 15.1, and that can help you figure out what kind of system you want to run this on.

Given that you are not likely to have an infinite budget, let's go over how RavenDB uses the machine's resources 
and what kind of impact that will likely have on your system. 

#### Disk

By far the most important factor for RavenDB performance is the disk. RavenDB is an ACID database, which means that it
will not acknowledge a write until it had been properly sent to the disk and confirmed to be stored in a persistent 
manner. A slow disk can cause RavenDB to have to wait for quite a while until it has the disk confirmation, and that
will slow down writes.

RavenDB is quite proactive in this manner and will parallelize writes whenever possible, but there is a limit to how 
much we can play with the hardware. At the end of the day, the disk is the final arbiter about when RavenDB can 
actually declare a transaction successful.

If you are using physical drives, then the order of preference at this time is to use NVMe if you can, failing that
use a good SSD and only if you can't get these (and you should) go for a high end HDD. Running a production database 
load on a typical HDD is not recommended. It is _possible_, but you'll likely see high latencies and contention on 
writing to the disk, which may impact operations. This strongly relates to your actual write load. 

> **What kind of a request are you?**
>
>  It is easy to lump everything into a single value, requests per second, but not all requests are made equal. Some
> requests are to load a document by id (cheap), other may involve big projections over a result set containing dozens
> of documents (expensive). Some requests are writes, which will have to wait for the disk and some are reads which
> can be infinitely parallel. 
> 
> When doing capacity planning, it is important to try to split the different types of requests, so we can estimate
> what kind of resource usage they are going to need. In Figure 15.1, you can see that RavenDB divide requests into
> the total number (`Requests/s`) and then the number of writes out of the total. This give us some good indication
> about the split in requests and the relative costs thereof.

If you are using network or cloud disks, be sure to provision enough IOPS for the database to run. In particular, when
using a SAN, do _not_ deploy a cluster where all the nodes use the same SAN. This can lead to trouble. Even though the
SAN may have high capacity, under load, all nodes will be writing to it and they will compete for the same resources. 
Effectively, this is turning into a denial of service attack against your SAN. The write load for RavenDB is 
distributed among the nodes in the cluster. Each of them writes their own copy of the data, but it all ends up in 
the same place. 

I strongly recommend that when you deploy a RavenDB cluster, you'll use independent disks and I/O channels. RavenDB 
assumes that each node is independent from the others and the load that one node geneates shouldn't impact operations
on another. 

You might have noticed that so far I was talking about the disk and it's impact on writes, but didn't mention reads at
all. This is because RavenDB uses memory mapped I/O, so reads are usually served from the system memory directly.

#### Memory

The general principle for memory with RavenDB is that the more memory you have, the better everything is. In ideal 
cases, your entire database can fit in memory, which means that the only time that RavenDB will need to go to disk is
when it ensures that a write is full persisted. 
In more realistic scenarios, when you have a database that is larger than your memory, RavenDB will try to keep as 
much of the database in memory as you are actively using. 

In addition to memory mapped files, RavenDB also uses memory for internal operations. This can be divided into managed
memory (that goes through the .NET GC) and unmanaged memory (that is managed by RavenDB directly). Typically these will
only be large if RavenDB is busy doing heavy indexing, such as when rebuilding an index from scratch. 

For good performance and stability, it is important to ensure that the working set of RavenDB (the amount of data that 
is routinely accessed and operated on at any given time) is less than the total memory on the machine. Under low memory
conditions, RavenDB will start scaling down operations in an attempt to reduce the memory pressure and use a more 
conservative strategy for many internal operations.

If you are running on a machine with a NUMA node (Non Uniform Memory Access), it can cause issues. RavenDB doesn't use
NUMA aware addressing for requests or operations, which can cause memory to jump between NUMA nodes, cause high
CPU usage  and increase latencies. The recommendation is to configure the machine to behave in non NUMA aware fashion.
Alternatively, run multiple instances of RavenDB on the machine, each bound to a specific NUMA node.

#### CPU

Given unlimited budget, I want the fastest CPU with the most cores. Reads in RavenDB scales linearly with the number
of cores that a machine has, but sequential operations such as JSON parsing are usually bounded by the speed of the 
individual cores.

RavenDB makes larger use of async operations internally, to reduce overall number of context switches, but selecting
whatever to prefer more cores over faster cores is something that you have to determine based on your requirements.
More cores means that RavenDB can have higher concurrent number of requests, but will have higher latency. 
Fewer and faster cores means faster responses, but fewer concurrent requests. 

The question is often academic in nature. We have test RavenDB with a Raspberry Pi 3 which uses a quad core 1.2GHz 
ARM CPU. On that machine, we were able to process about 13,000 document reads per second. Those were simple document
loads, without complex queries or projections, but that should still give you some idea about the expected performance
on your production systems.

#### Network

Network usage in RavenDB is important, as you can imagine. With enough load RavenDB can saturate a 10 Gbit 
connection, sending out gigabytes of data per second. If you get to this point, however, I suggest first taking a look
at your application to see what it is doing. Very often, such network saturation is the result of the application 
asking for much more data than is required. 

A good example is wanting to load the list of orders for a customer, needing to show a grid of the date, total order
value and the status of the order. In some cases, the application will pull the full documents from the server, even
though it uses very little data from them. Changing the application to project just the relevant information is 
usually better overall than just plugging in a 20 Gbit card. 

An important consideration for network usage is that RavenDB will not compress the outgoing data by default when using
HTTPS. If you are talking to RavenDB from a nearby machine (same rack, data center), there is usually enough network
capacity to avoid spending time compressing the responses. There is also the [BREACH](http://breachattack.com/) attack 
for compressed HTTPS to consider, which is why this is off by default.

Compression on the wire is controlled via `Http.UseResponseCompression` and `Http.AllowResponseCompressionOverHttps`
settings. You can also control the compression level using `Http.GzipResponseCompressionLevel` setting, favoring 
speed over compression rate or vice versa. On a local network, it is probably best to not enable that, the CPU time
is better spent handling requests, not compressing responses.

### Common cluster topologies

RavenDB is quite flexible in the ways it allows you to setup itself up for production. In this section, we are going to
look at a few common configuration topologies for RavenDB, giving you the option to pick and chose what is best for 
your environment and needs. These aren't the only options, and you can usually mix and match between them. 

> **RavenDB Cluster and what it is good for**
>
> A cluster in RavenDB is a group of machines that are managed as a single unit. A cluster can contain any number of
> databases that are spread across the nodes in the cluster. Each database is replicated to some (or all) of the nodes
> and multiple copies of the data is held, depending on the replication factor for this database.
>
> RavenDB doesn't split the data inside a database among the various nodes, but replicate all the data in the database
> to each of the nodes in the database group.^[See Chapters 6 and 7 for the full details on how RavenDB clusters 
> behave.]

You'll typically use a dedicated database per application, potentially with some data flows (ETL and external 
replication) between them. These databases are hosted on a single cluster, which simplify management and operations.
Let's see how we actually deploy RavenDB in various scenarios and the pros and cons of each option.

#### A single node 

The single node option is the simplest one. Just have a single node and run everything on top of that. You can see an
example of this topology in Figure 15.3.

![A single node option, hosting multiple databases](./Ch15/img03.png)

In this mode, all the databases you use are all on the same node. Note that this can change over time. Any RavenDB node
is always part of a cluster. It may be a cluster that only contains itself, but it is still a cluster. In operational
terms, this means that you can expand this cluster at any point by adding more nodes to it, and then deciding how to
arrange the databases on the cluster. 

This mode is popular for development, UAT, CI and other such systems. It is possible to use this mode for production, 
but this is _not_ a recommended configuration for production, because it has a single point of failure. If this single 
node is down, there is no one else around that can take its duties, and that makes any issues with the node 
"High Priority" by definition. 

The better alternative by far is a proper cluster.

#### The classic cluster

The classic cluster has either three or five nodes. The databases are spread across the cluster, typically with a 
replication factor of two or three. Figure 15.4 shows a three node cluster with a replication factor of two. 

![A three node cluster, with each database residing on two nodes.](./Ch15/img04.png)

In the mode shown in Figure 15.4, we know that we can lose any node in the cluster, and have no issue continuing 
operations normally. This is because at the cluster level, we have a majority (2 out of 3) and we are guaranteed to 
have all the databases available as well. 

> **You don't need a majority**
>
> The topologies shown in Figure 15.4 and Figure 15.5 showcase a deployment method that ensures that as long as
> a majority of the nodes are up, there is no interruption of service. This is pretty common with distributed 
> systems, but it isn't actually required for RavenDB.
> 
> Cluster wide operations, such as creating and deleting databases, assigning tasks to nodes in the cluster or 
> creating indexes require a majority to be accessible. But these tend to be rare operations. The most common
> operations are the reads and writes to documents, and these can operate quite nicely even with just a single
> suviving node. The mesh replication between the different databases uses a gossip protocol (discussed in more
> depth in Chapter 6) with a multi-master architecture. 
>
> All reads and writes can go to any database in the database group and they will be accepted and processed 
> normally. This gives the operations team a bit more freedom with how they design the system and the ability to
> chose how much of a safety margin is needed, compared to the resources required.

In this way, we reduced the density we have from five databases per server to four databases per server. Not a hude
reduction, but it means that we have more resource available for each database, and we gained the surity of high
availability for ourselves. 

Another classic is the five node cluster, as shown in Figure 15.5. Each node in the cluster contains three databases
and the cluster can survive up to two nodes being down with no interruption in service. At this point, you need 
to consider whatever you actually need this level of redundnacy. 

![A five node cluster, with each database residing on three nodes.](./Ch15/img05.png)

There are many cases where you can live with lower redundancy level than the one shown in Figure 15.5. Having a five
node cluster with each of the databases having a replication factor of two is also an option. Of course, in this mode 
losing two specific nodes in the cluster may mean that you'll lose access to a particular database.
What exactly will happen depends on your specific failure. If you lose two nodes immediately, that will cause the
database to become inaccessible. 

> **High availability cluster**
>
> RavenDB clusters and their highly available properties were discussed at length in Chapter 6 and Chapter 7.
> Here it is important to remember that a failed node will be automatically worked around by the cluster, which
> will redirect clients to another node, re-assign its tasks and start active monitoring for its health. 
>
> If the node is down for long enough, the cluster may decide to add extra copies of the databases that resided
> on the failed node, to ensure the proper amount of replica are kept, according to the configured replication
> factor.

If there is enough time between the two nodes failures of the cluster to react, it will spread the database whose node
went down to other nodes in the cluster to maintain the replication factor, ensuring that a second node failure will
not make a database inaccessible. 

This particular feature is nice to have when you have a five node cluster, but most times you'll use a replication
factor of three and not really have to think about the cluster moving database around. That is far more likely when
the number of nodes that you have in the cluster grow much higher. 

#### The Czar's Cluster

RavenDB uses a consensus protocol to manage the cluster. This means that any cluster wide operation require 
acknowledgement from a majority of the nodes in the cluster. For a five nodes cluster, this means that each decision
requires the leader and two other confirmations to be considered accepted. As the size of the cluster grows, so does
the size of the majority. For a seven nodes cluster, you need a majority of four, for example. 

Because of this, you'll typically not just increase the size of your cluster whenever you need more capacity. 
If you have enough databases and load to need a 20 nodes cluster, each decision will require 11 confirmations. That...
is quite high. Instead, we arrange the cluster in two ranks.

At the first rank, we have the cluster members, these are nodes that can vote and become the cluster leader. You'll 
typically have three, five or seven of them, but not beyond that. The higher the number of member nodes, the more 
redundnacy you'll have in the cluster. But what about the rest of the cluster nodes?

The rest of the cluster nodes aren't going to be full members, intead, they are marked as watchers in the cluster.
You can see how this topology looks like in Figure 15.6.

![A large cluster, divided to full members and watchers](./Ch15/img06.png)

A watcher in the cluster is a fully fledged cluster node. The node hosts databases in the cluster and is managed by
the cluster, like any other node. However, it cannot be voted as the leader, and while it is informed about any 
cluster decision, the cluster doesn't count a watcher's vote toward the majority. A cluster majority is always computed
as: `floor(MembersCount / 2) + 1`, disregarding the watchers (which are just silent observers).

A watcher node can be promoted to a member node (and vice versa), so the decision to mark a node as a member or 
watcher doesn't have long term implications. If a member node is down and you want to maintain the same number of 
active members, you can promote a watcher to be a member. Such promotion is a cluster operation that requires a 
majority confirmation among the cluster members, so you cannot promote a node if a majority of the members are down.

#### Primary / secondary mode

The primary / secondary topology is a common topology to achieve high availability. A good example of how this topology
looks like can be seen in Figure 15.7. You have a primary server that does all the work, and a secondary that 
replicates the data in the primary. This is also called active/passive topology.

![A primary server, replicating to a secondary](./Ch15/img07.png)

With RavenDB, a two nodes cluster is not a good idea. The majority of two is still two, after all, so even a single 
node failure will cause the cluster to become unavailable.^[However, the _databases_ inside that cluster will 
continue to be available fom the remaining node, of course.] 
This is sometimes chosen as the selected topology when you want to manually control failover behavior for some reason.
We'll cover this option in the next section.

### Database group topologies

We talked about the topology of the cluster up until now, but most of the work done with RavenDB is at the database
level. Let's zoom in to how we should deploy our databases in production.

> **Reminder: databases groups and database instances**
>
> A database group is a set of nodes that holds individual database instances. Each one of the database instances
> will replicate all its data to any of its sibling instances on the other nodes in the database group. 
>
> The database group as a whole is being managed by the cluster, assigning tasks to various nodes in the group,
> determining the priority order for clients, etc. At the same time, each instance constantly gossips with other
> members in the group, sharing new and updated information and reconciling any changes that happened in the 
> meantime. 
> 
> This gossip is done _independently_ from the cluster behavior and continues to work even under error conditions
> that cause the cluster to become inoperable. Clients, from their end, know about the database group topology
> and will try another node if the node they tried talking to isn't responding. This lead to a robust system
> in which the cluster, the database group members and the clients are all working together to achieve maximum 
> uptime.

There a few considerations to take into account about the database topology. First, we need to decide what is the 
appropriate replication factor for the database. The more copies of the data we have, the safer we are against
catastrophe. On the other hand, at some point we are safe enough and just burning through disk space. 

Second, we need to consider what kind of access pattern should we choose to use? By default, clients will use the
databases instances in a database groups in the order they are defined on the server. You can see that in 
Figure 15.8 and on the studio under `Settings` and then `Manage Database Group`. 

![Showing the priority order for clients to access nodes in a database group](./Ch15/img08.png)

As you can see in Figure 15.8, there is a way for the admin to control this order. That is not just to comply with
some people's OCD tendencies^[No, I'll not call it CDO, even though that is the _proper_ way to alphabetize it.], the 
order of the nodes in this list matters. This forms the priority order for the clients to access the database 
instance by default. 

If a client cannot talk to the first node in the list, it will try the second, etc. Reordering the nodes wil cause 
the server to let the clients know on the next request they make, so they can quickly get up to date on the new 
topology.

Aside from the operations teams manually routing traffic, there is also the cluster itself that may decide to change
the order of nodes there based on its own cognizance, to handle failures, distribute load in the cluster, etc. You
can also ask the clients to use a round robin strategy (configured via `Settings`, `Client Configuration` and then
select the `Read balance behavior` you want) to spread the read load among the nodes.

In addition to defining a database group inside a single cluster, you also have the option of connecting databases
from _different_ clusters. This can be done as a one way or bidirectional manner. 

#### Using replication outside the cluster

A database group is a fairly rigid structure, except for determining which nodes the database will reside on, there
is not much that you configure on a per node basis. Sometimes you want to go beyond that, maybe you want different
indexes on different nodes, or to have a "firebreak" type of failover, where only if you manually switch things 
over will clients failover to a secondary node or in general need higher degree of control.

At that point, you will no longer use the cluster to manage things but take the reins yourself. This is done by 
manually defining the data flow between databases using external replication. We covered external replication 
in Chapter 7. Using external replication, you can contorl exactly where and how the data will go. You aren't limited
to just defining the data flow between different database instances in the same cluster.

When using external replication, you need to remember:

* Clients will _not_ follow external replication in the case of failure.
* Indexes are not replicated via external replication.
* The configuration and setings between the nodes can be different (impacts things like expiration, revisions, etc).
* External replication is unidirectional. If you want to have documents flowing both ways, you'll to define the 
  replication from ends.
* External replication is a database group wide task that is being managed by the cluster, with automatic failure
  handling and failover on both source and destination. 

These properties of external replication opens up several interesting deployment options for your operations team. 
Probably the most obvious option is the notion of the offsite replica. This can be used as part of your disaster 
recovery plans, to have a totally separate copy of the data that isn't directly accessed from anywhere else.

This is also where the delayed replication feature come into play. Configuring a delay into external replication gives
you time to react in case something bad happened to your database (the most common occurance is running an update
command without using a `where` clause). 

The fact that external replication is a database group wide operation and that it's target is a database group 
in a cluster (not a single node) is also very important. Consider the topology in Figure 15.9.

![Two clusters in different data centers connected via external replication](./Ch15/img09.png)

In Figure 15.9, you have two independent clusters deployed to different parts of the world. These clusters are 
connected via bidirectional external replication. Any change that happens on one of these clusters will be reflected
in the other. Your application, on the other hand, is configured to use the cluster in the same datacenter as the
application. There is no cross data center failover happening here. But the data is fully shared.

This is a good way to ensure both high availability inside the same data center, global availablity with 
geo-distributed system and good locality for all data accesses, since the application will only talk to its own 
local nodes. In this case, we _rely_ on the fact that clients will not failover across external replication. 
Instead, if we lose a whole data center, we'll simply route traffic to the surviving data center, rather than try
to connect from one data center to another for all our database calls.

#### Ideal number of databases per node

It's rare that you'll only have a single database inside your cluster. Typically, you'll have each application using
its own database, and even inside a single application there are commonly good reasons why separate pieces should 
store information separately. This leads to an obvious question, how many database can we squeeze on each node?

There is no generic way to answer this question, but we can break down what RavenDB does with each database and then
see how much load that will take. Each database that RavenDB hosts uses resources, these can be broken up into memory,
disk and threads.
It might seem obvious, but the more databases you have on a single node, the more diskspace is being used. If 
all these databases are active at once, they they are going to be using memory.

I might seem obvious, but I have seen customers that packed 800 - 900 databases into a single server and were surprised
that it was struggling to manage. Sure, each of those databases was quite small, 1 - 5 GB in size, but when they 
needed to access all of them the database was consuming a lot of resources and often failed to keep up with the 
demand. 
A good rule of a thumb is that fewer, larger, databases are better than _many_ small databases. 

Each database hold a set of resources. Dedicated threads to apply transactions, run indexes, replicate data to other
instances, etc. All of that requires CPU, memory, time and I/O. RavenDB can do quite nicely with a decent number of
databases on a single node, but do try to avoid packing them too tightly.

A better altnernative if you have a lot of databases is to simply break the node into multiple nodes and spread the
load across the cluster. In such a way, all the databases aren't competing for the same resources and you can manage
things much more easily.

#### Increasing the database group capacity

A database group is not just about having extra copies of your data, although that is certainly nice. A large part of
what a database group can do is to distribute the load on the system at large. There are many tasks that can be done
at the database group level. We already looked at `Read balance behavior` to share the query load between the nodes, 
but that is just the beginning.

Subscriptions, ETL processes, backups and external replication are all tasks that the cluster will distribute around
the database group. You can also define such tasks to be sticky, so they'll always reside in the same node. The idea
is that you can distribute the load among all the instances you have. You can see how that works in Figure 15.10.

![The database group topology and task distribution among the nodes](./Ch15/img10.png)

If needed, you can increase the number of nodes a database group resides on, giving you more resource to distribute
among the various tasks that the database group is responsible for. This can be done via the `Settings`, 
`Manage Database Group` page by clickin on `Add node to group` and select an available node from the cluster. 
This will cause RavenDB to create the database in the new node in a `promotable` state. The cluster will assign one
of the existing nodes as the mentor, replicating the current database state to the new database. 

Once the new database is all caught up (which for large databases can take a bit of time) the cluster will promote
it to a normal member and re-distribute the tasks in the cluster in a fair manner. You need to be aware that for
the duration of the expansion, the mentor node is going to be busier than usual. Not enough to impact operations
normaly, but not something that you really want to be doing if your cluster is at absolute full capacity.

### Databases, drives and directories

I already talked about the importance of good I/O for RavenDB earlier in this chapter. That was in the context of 
deploying a cluster, now I want to focus on the I/O behavior of a single node and even a single database inside that
node.

Using a database's data directory as the base, RavenDB uses the format shown in Listing 15.1.

```{caption="On disk structure of RavenDB the `Northwind` database folder" .cs}
┌── Indexes
│   ├── Orders_ByCompany
│   │   ├── Journals
│   │   │   ├── 0000000000000000001.journal
│   │   └─ Raven.voron
│   ├── Orders_Totals
│   │   ├── Journals
│   │   │   ├── 0000000000000000001.journal
│   │   │   └── 0000000000000000002.journal
│   │   └── Raven.voron
│   └── Product_Search
│       ├── Journals
│       │   └── 0000000000000000002.journal
│       └── Raven.voron
├── Journals
│   ├── 0000000000000000003.journal
│   └── 0000000000000000004.journal
└── Raven.voron
```

You can see that the most important details are `Raven.voron` (the root data file), `Journals` (where the transaction
write ahead journals are) and `Indexes` (where the indexes details are kept).
Interestingly, you can also see that the `Indexes` structure actually mimic the structure of the database as a whole. 
RavenDB uses Voron to store the database itself and its indexes, in separate locations. 

This is done intentionally, because it gives the operations team the chance to play some interesting games. This 
split allow you to define where the data will actually reside. It does
not have to all sit on the same drive. In fact, for high end systems, it probably shouldn't. 

> **Using the operating system tools**
>
> Splitting data and journals into separate phyiscal devices gives us better concurrency and avoid traffic jams
> at the storage level.
>
> Instead of having to configure separate paths for journals, data and indexes, RavenDB relies on 
> the operating system to handle that. On Linux, you can use soft links or mount points to get RavenDB 
> to write to a particular device or path. On Windows, junction points and mount points serves the same idea. 

The `Journals` directory files are typically only ever written to, this is done in a sequential manner and there is 
only ever a single write pending at any given time. This write is also always using unbuffered writes and 
direct I/O to hit make sure that the transaction is properly persisted. It is also in the hot path for transaction 
commit, since we can't confirm that the transaction has actually been committed until the disk has acknowledged it.

The `Raven.voron` file is written to in a random manner, typically using memory mapped I/O and occasionally an 
`fsync` is called on it. This is not in the hot path of any operation, but if this is too slow it can cause RavenDB
to use memory to hold the modified data in the file while waiting for it to sync to disk. 

Each of the directories inside the `Indexes` directory holds a single index, and the same rules about `Raven.voron`
and `Journals` apply to those as well, with the exception that each of them is operating independently and concurrently
with the others, which means that there may be many concurrent writes to the disk (either for the indexes' `Journals` 
or for the writes to the `Raven.voron` files for the indexes).

You can utilize this knowledge to separate the `Indexes` to a separate drive, avoiding congensting the drive that the
database is writing to. Having the `Journals` use a separate drive, maybe one that is setup to be optimal for the 
kind of access the journals have. 

The idea is that you'll split the I/O work between different drives, having journals, indexes and the main data file
all on separate phyiscal hardware and avoiding them fighting each other for I/O access. 

#### Paths in the clusters

In a cluster, you'll often use machines that are identical to one another. That means that paths, drives and setup
is identical. It make it easier to work in the cluster because you don't have to worry about the difference between
nodes.
There is nothing in RavenDB that actually demands this, and you can have a database node with a drive `D:` running
Windows and another couple running Ubuntu with different mount points configuration, all in the same cluster. 

However, when you need to define paths in the cluster, you should take into account that whatever path you define is
not only applicable for the current node but may be used on any node in the cluster. For that reason, be sure that 
if you define an absolute path it should be one that is valid on all nodes.

For relative paths, RavenDB uses the `DataDir` configuration value to determine what is the base directory it will
start computing paths from. In general, I would recommend using only relative path since this simplify managmeent 
in most cases.

### Network and firewall considerations

There isn't much that RavenDB requires from the network in order to operate successfully. All the nodes need to be
able to talk to each other, this refers to both the HTTPS port used for external communication and the TCP port 
that is mostly used for internal communication.
These are configured via `ServerUrl` and `ServerUrl.Tcp` configuration options, respectively. 

If you don't have to worry about firewall configurations, you can skip setting the `ServerUrl.Tcp` value, in which
case RavenDB will use a random port (the node negotiate how to connect to each other using the HTTPS channel first,
so as long as nothing blocks the connection, that would be fine). But for production settings, I would strongly 
recommend setting a specific port and configuring things properly. At some point, there _will_ be a firewall, and
you don't want to chase things.

Most of the communication to the outside world is done via HTTPS, but RavenDB also does a fair bit of server to 
server communication over a dedicate TCP channel. This can be between different members of the same cluster or 
external replication between different clusters or even subscriptions connection from clients. As discussed in
Chapter 13, all communication channels when running in secured mode are using TLS 1.2 to secure the traffic. 

RavenDB will also connect to `api.ravendb.net` to get notifications about version updates, check support 
and licensing status. During setup and certificate renewal we'll also contact `api.ravendb.net` to manage the 
automatic certificate generation, if you are using RavenDB's Let's Encrypt integration (see Chapter 13). 
For these reasons, you should ensure that your firewall is configured to allow outgoing connections to this
URL.

In many cases using certificates will check AIA^[Authority Information Access] and 
CRL^[Certificate Revocation List]. This is usually controlled by system level configuration and 
security policies and is basically a way to ensure trust in all levels of the certificate. These are usually
done over HTTP (_not_ HTTPS) and failing to allow them through the firewall can cause slow connections or 
failure to connect.

### Configuring the operating system

The online documentation has the full details about all the kind of options you might want to play with. 
I do want to point out a few of the more common ones and their impact on production deployments. 

On Linux, the number of open file descriptors can often be a stumbling block. RavenDB doesn't actually use
that many file descriptors.^[See Listing 15.1 to get a good idea about the number of files RavenDB will 
typically have opened.] However, network connections are also, of course, using file descriptors, and under
load it is easy to run out of them. You can configure this with `ulimit -n 10000`. 

Another common issue on Linux is if you want to bind to a port that is lower than 1024 (such as 443 for HTTPS).
In this case, since we want to avoid running as root, you'll need to use `setcap` to allow RavenDB to listen
to the port. This can be done using: `sudo setcap CAP_NET_BIND_SERVICE=+eip /path/to/Raven.Server`.

When using encryption, you may need to increase the amount of memory that the RavenDB is allow to lock into
memory. On Linux, this is handled via the `/etc/security/limits.conf` and increasing the `memlock` values. 
For Windows, you may need to give the RavenDB user the right to `Lock pages in memory`.

### Summary 

We started this chapter talking about capacity planning and most importantly, measurable SLAs. Without which
operations teams resort to hand waving, imprecise language and over provisioning of resources. It is important
to get a good idea bout what your system is expected to handle, and in what manner. Once you have these 
numbers, you can set out to plan how to actually deploy it. 

We went over the kind of resources that RavenDB uses and how we should evaluate and provision for them. We 
took a very brief look at the kind of details that we'll need for production.
We talked about the kind of disks you should run RavenDB on (and the kind you should _not_).
I/O is very important for RavenDB and we'll go back to this topic in the monitor chapter as well. 
We covered how RavenDB uses the memory, CPU and the network. In particular paying attention to some of the 
settings (such as HTTP compression) that are only really meaningful for production. 

We then talked about different cluster topologies and what they are good for. From the single node setup
that is mostly suitable for development (and _not_ suitable for production) 
to highly avaible cluster that can handle node failures without requiring any outside involvment. We went
over the considerations for replication factors in our cluster, depending on the value of the data, how
much extra disk space we can afford and the results of being offline.

We then talked about large clusters, composed of many nodes in tiers. Members nodes that form the "ruling 
council" of the cluster and can vote on decisions and become leader and the watcher nodes (the "plebs")
which cannot vote but are still being managed by the cluster. That is a typical configuration when you have
a very large number of databases and want to scale out the amount of resources that will be used.

From the cluster level we narrowed things down to the database level, talking about database group topologies
and how that relates to the behavior of the system in production. As an administrator of a RavenDB system you can
use the database topology to dictate which node the clients will prefer to talk to, whatever they will use a 
single node or spread their load across multiple ones and how tasks should be assigned in the group.

Beyond the database group, you can also use external replication to bridge different databases, including between
clusters. We looked at an example that shared data between London and New York data centers in two separate 
clusters. Such options give you better control over exactly how clients will behave when an entire data center
goes down. 

We then talked about the number of databases that we want to have per node, RavenDB can support a decent number
of them without issue, but in general fewer and larger databases are preferred. We also looked into what is
involved in expanding the database group and adding more nodes (and capacity) to the database.

Following that we dove into how RavenDB store data on disk and what access patterns are used. You can utilize 
this information during deployment to split the data, indexes and the jouranls to separate physical drives, 
resulting in lower contentions on the I/O devices and giving you higher overall performance. 

We ended the chapter by discussing some of the minor details involved in deploying to production, the kind of
firewall and network settings to watch out for and what kind of configuration changes we should make to the 
operating system. 

This chapter is meant to give you a broad overview of deploying RavenDB, and I recommend going over the 
detailed instructions in the online documention. Now that we have your RavenDB cluster up and running, we need to
go over how you are keep it up, next topic: monitoring, troubleshooting and disaster recovery.