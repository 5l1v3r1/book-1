
## Production deployments

The day you deploy to production can be a very scary day. It is the culmination of months or years of work and it is 
a time when you actually get to see the results of your work actually bearing fruits. Of course, sometimes the fruit is 
unripe, because you pushed the big red button too soon. There is no riskier time in production than just after a new
version has been deployed.

I want to recommend the [Release It!](https://pragprog.com/book/mnee/release-it) book by Michael T. Nygard, I read it 
for the first time over a decade ago, and it made an _impact_ on how I think about production systems. It had a lot
of affect on the design and implementation of RavenDB itself, as well. This chapter will cover topics specific for
RavenDB, but you might want to read the Release It! book to understand how to apply some of these patterns to your 
application as a whole. 

There are quite a few concerns that you need to deal with when you are looking at your deployment plan and strategy.
These start with the constraints that you have, such as a specific budget or regulatory concerns about the data and 
how you and where may store it. The good thing about these is that they are often clearly stated and understood. 
You also have the requirements, such as how responsive the system should be, how many users are expected to use it
and what kind of load you have to plan for. Unfortunately, these requirements are often unstated, assumed or just 
kept at a high enough level to be meaningless.

> **You _need_ the numbers**
>
> In 2007, I was asked by a client to make sure that the project we were working on would be "as fast as Google". 
> I took him at his word and gave him a quote for 11.5 billion dollars (which was the Google budget for that year).
> While I didn't get that quote approved, I was able to make the point, and we were able to hammer down what we 
> actually needed from the application.
>
> Giving a quote for $11,509,586,000 is a cry for help. I don't know what _fast_ is. That is not how you measure 
> things in a meaningful way. A much better way to state this is to say something in the format of Table 15.1. 
> 
> | Reqs / sec |   %    | Max duration (ms) |
> |-----------|--------|--------------------|
> |       100 | 99%    |                100 |
> |       100 | 99.99% |                200 |
> |           |        |                    |
> |       200 | 99%    |                150 |
> |       200 | 99.9%  |                250 |
> |       200 | 99.9%  |                350 | 
> 
> Table 15.1: SLA table allowing for max response time for requests under different loads
>
> Table 15.1 is something actional. It tells us that under a load of 100 requests per second, we should complete 
> 99% of requests in under 100ms, and 99.99% requests in under 200ms. If the number of concurrent requests go up, 
> we also have an SLA set for that. We can measure that and see whatever we match the actual requirement. 

Developers typically view production as the end of the project. It is in production and it stays there, and the 
developers can turn to the next feature or maybe even a different project entirely. Operations team usually have the 
opposite view. Now that this in production, this is their role to babysit the system.
I don't intend to go over the benefits of having both operations and development insight during both the design of 
the system and its deployment, or even talk about closer colaboration in general. You can search for the term 
"devops" and read reams about it. 

What I'm going to do is to assume that you are early enough in the process that you can utilize some of the notions
and tools in this chapter to affect how your system is being design and deployed, or if you aren't at this stage, you
are able to start shifting to a point where you match the recommended practices.

And with that, let's dive in and talk about the first thing you need to figure out.

### System resources usage

You must have a 386DX, 20MHz or higher processor, 4MB of memory (8MB recommended) and 
at least 70MB of available hard disk space. Oh, wait. That is wrong. These are actually the system requirements if you
want to install Windows 95. 

I want to refer you back to Table 15.1. Before you can make any decisions, you need to know what kind of requirements 
are going to be placed on _your_ system. Is this a user facing or B2B? How much data is there to handle, how many 
requests are expected? Are there different _types_ of requests? This is important enough topic that it's worth 
repeating. There are a lot of good materials about how to go about figuring this out for your system and I'll also
point you again to the Release It! book for more details. 

There is an obvious correlation between the load on your system and the resources that it consume to handle this load.
Figure 15.1 shows a couple of graphs from a production system. This isn't peak load, but it isn't idle either.^[All
the stats and monitoring details are directly from the RavenDB studio. We'll go over where they are located and what
you can deduce from them in the next chpater.] 

![Load and resource graphs from the RavenDB dashboard on production system](./Ch15/img01.png)

This particular machine is running on Amazon EC2 `t2.large` instance with 2 cores and 8 GB of memory. The machine isn't
particularly loaded, with enough spare capacity to handle peaks of three to four times as many requests per second. Of 
course, this is just a very coarse view of what is going on, it is missing the request latencies. We can get that as
well, as you can see in Figure 15.2.

![Request latencies tracked using the RavenDB Traffic Watch feature](./Ch15/img02.png)

This seems to be fine, the average request time is low, and even the maximum isn't too bad. 
We can dive into percentiles and metrics and all sort of details, but at this point will get too specific to be of much 
relevance for this book. 

If you are already in production the RavenDB studio already surface these numbers to you, which means that you can act
upon them. For one thing, the correlation between the requests on the server and the load on the server can be 
clearly seen in Figure 15.1, and that can help you figure out what kind of system you want to run this on.

Given that you are not likely to have an infinite budget, let's go over how RavenDB uses the machine's resources 
and what kind of impact that will likely have on your system. 

#### Disk

By far the most important factor for RavenDB performance is the disk. RavenDB is an ACID database, which means that it
will not acknowledge a write until it had been properly sent to the disk and confirmed to be stored in a persistent 
manner. A slow disk can cause RavenDB to have to wait for quite a while until it has the disk confirmation, and that
will slow down writes.

RavenDB is quite proactive in this manner and will parallelize writes whenever possible, but there is a limit to how 
much we can play with the hardware. At the end of the day, the disk is the final arbiter about when RavenDB can 
actually declare a transaction successful.

If you are using physical drives, then the order of preference at this time is to use NVMe if you can, failing that
use a good SSD and only if you can't get these (and you should) go for a high end HDD. Running a production database 
load on a typical HDD is not recommended. It is _possible_, but you'll likely see high latencies and contention on 
writing to the disk, which may impact operations. This strongly relates to your actual write load. 

> **What kind of a request are you?**
>
>  It is easy to lump everything into a single value, requests per second, but not all requests are made equal. Some
> requests are to load a document by id (cheap), other may involve big projections over a result set containing dozens
> of documents (expensive). Some requests are writes, which will have to wait for the disk and some are reads which
> can be infinitely parallel. 
> 
> When doing capacity planning, it is important to try to split the different types of requests, so we can estimate
> what kind of resource usage they are going to need. In Figure 15.1, you can see that RavenDB divide requests into
> the total number (`Requests/s`) and then the number of writes out of the total. This give us some good indication
> about the split in requests and the relative costs thereof.

If you are using network or cloud disks, be sure to provision enough IOPS for the database to run. In particular, when
using a SAN, do _not_ deploy a cluster where all the nodes use the same SAN. This can lead to trouble. Even though the
SAN may have high capacity, under load, all nodes will be writing to it and they will compete for the same resources. 
Effectively, this is turning into a denial of service attack against your SAN. The write load for RavenDB is 
distributed among the nodes in the cluster. Each of them writes their own copy of the data, but it all ends up in 
the same place. 

I strongly recommend that when you deploy a RavenDB cluster, you'll use independent disks and I/O channels. RavenDB 
assumes that each node is independent from the others and the load that one node geneates shouldn't impact operations
on another. 

You might have noticed that so far I was talking about the disk and it's impact on writes, but didn't mention reads at
all. This is because RavenDB uses memory mapped I/O, so reads are usually served from the system memory directly.

#### Memory

The general principle for memory with RavenDB is that the more memory you have, the better everything is. In ideal 
cases, your entire database can fit in memory, which means that the only time that RavenDB will need to go to disk is
when it ensures that a write is full persisted. 
In more realistic scenarios, when you have a database that is larger than your memory, RavenDB will try to keep as 
much of the database in memory as you are actively using. 

In addition to memory mapped files, RavenDB also uses memory for internal operations. This can be divided into managed
memory (that goes through the .NET GC) and unmanaged memory (that is managed by RavenDB directly). Typically these will
only be large if RavenDB is busy doing heavy indexing, such as when rebuilding an index from scratch. 

For good performance and stability, it is important to ensure that the working set of RavenDB (the amount of data that 
is routinely accessed and operated on at any given time) is less than the total memory on the machine. Under low memory
conditions, RavenDB will start scaling down operations in an attempt to reduce the memory pressure and use a more 
conservative strategy for many internal operations.

If you are running on a machine with a NUMA node (Non Uniform Memory Access), it can cause issues. RavenDB doesn't use
NUMA aware addressing for requests or operations, which can cause memory to jump between NUMA nodes, cause high
CPU usage  and increase latencies. The recommendation is to configure the machine to behave in non NUMA aware fashion.
Alternatively, run multiple instances of RavenDB on the machine, each bound to a specific NUMA node.

#### CPU

Given unlimited budget, I want the fastest CPU with the most cores. Reads in RavenDB scales linearly with the number
of cores that a machine has, but sequential operations such as JSON parsing are usually bounded by the speed of the 
individual cores.

RavenDB makes larger use of async operations internally, to reduce overall number of context switches, but selecting
whatever to prefer more cores over faster cores is something that you have to determine based on your requirements.
More cores means that RavenDB can have higher concurrent number of requests, but will have higher latency. 
Fewer and faster cores means faster responses, but fewer concurrent requests. 

The question is often academic in nature. We have test RavenDB with a Raspberry Pi 3 which uses a quad core 1.2GHz 
ARM CPU. On that machine, we were able to process about 13,000 document reads per second. Those were simple document
loads, without complex queries or projections, but that should still give you some idea about the expected performance
on your production systems.

#### Network

Network usage in RavenDB is important, as you can imagine. With enough load RavenDB can saturate a 10 Gbit 
connection, sending out gigabytes of data per second. If you get to this point, however, I suggest first taking a look
at your application to see what it is doing. Very often, such network saturation is the result of the application 
asking for much more data than is required. 

A good example is wanting to load the list of orders for a customer, needing to show a grid of the date, total order
value and the status of the order. In some cases, the application will pull the full documents from the server, even
though it uses very little data from them. Changing the application to project just the relevant information is 
usually better overall than just plugging in a 20 Gbit card. 

An important consideration for network usage is that RavenDB will not compress the outgoing data by default when using
HTTPS. If you are talking to RavenDB from a nearby machine (same rack, data center), there is usually enough network
capacity to avoid spending time compressing the responses. There is also the [BREACH](http://breachattack.com/) attack 
for compressed HTTPS to consider, which is why this is off by default.

Compression on the wire is controlled via `Http.UseResponseCompression` and `Http.AllowResponseCompressionOverHttps`
settings. You can also control the compression level using `Http.GzipResponseCompressionLevel` setting, favoring 
speed over compression rate or vice versa. On a local network, it is probably best to not enable that, the CPU time
is better spent handling requests, not compressing responses.

### Recommended production topologies

RavenDB is quite flexible in the ways it allows you to setup itself up for production. In this section, we are going to
look at a few common configuration topologies for RavenDB, giving you the option to pick and chose what is best for 
your environment and needs. These aren't the only options, and you can usually mix and match between them. 

#### A single node 

The single node option is the simplest one. Just have a single node and run everything on top of that. You can see an
example of this topology in Figure 15.3.

![A single node option, hosting multiple databases](./Ch15/img03.png)

In this mode, all the databases you use are all on the same node. Note that this can change over time. Any RavenDB node
is always part of a cluster. It may be a cluster that only contains itself, but it is still a cluster. In operational
terms, this means that you can expand this cluster at any point by adding more nodes to it, and then deciding how to
arrange the databases on the cluster. 

This mode is popular for development, UAT, CI and other such systems. It is possible to use this mode for production, 
but this is _not_ a recommended configuration for production, because it has a single point of failure. If this single 
node is down, there is no one else around that can take its duties, and that makes any issues with the node 
"High Priority" by definition. 

The better alternative by far is a proper cluster.

#### The classic cluster

The classic cluster has either three or five nodes. The databases are spread across the cluster, typically with a 
replication factor of two or three. Figure 15.4 shows a three node cluster with a replication factor of two. 

![A three node cluster, with each database residing on two nodes.](./Ch15/img04.png)

In the mode shown in Figure 15.4, we know that we can lose any node in the cluster, and have no issue continuing 
operations normally. This is because at the cluster level, we have a majority (2 out of 3) and we are guaranteed to 
have all the databases available as well. 

> **You don't need a majority**
>
> The topologies shown in Figure 15.4 and Figure 15.5 showcase a deployment method that ensures that as long as
> a majority of the nodes are up, there is no interruption of service. This is pretty common with distributed 
> systems, but it isn't actually required for RavenDB.
> 
> Cluster wide operations, such as creating and deleting databases, assigning tasks to nodes in the cluster or 
> creating indexes require a majority to be accessible. But these tend to be rare operations. The most common
> operations are the reads and writes to documents, and these can operate quite nicely even with just a single
> suviving node. The mesh replication between the different databases uses a gossip protocol (discussed in more
> depth in Chapter 6) with a multi-master architecture. 
>
> All reads and writes can go to any database in the database group and they will be accepted and processed 
> normally. This gives the operations team a bit more freedom with how they design the system and the ability to
> chose how much of a safety margin is needed, compared to the resources required.

In this way, we reduced the density we have from five databases per server to four databases per server. Not a hude
reduction, but it means that we have more resource available for each database, and we gained the surity of high
availability for ourselves. 

Another classic is the five node cluster, as shown in Figure 15.5. Each node in the cluster contains three databases
and the cluster can survive up to two nodes being down with no interruption in service. At this point, you need 
to consider whatever you actually need this level of redundnacy. 

![A five node cluster, with each database residing on three nodes.](./Ch15/img05.png)

There are many cases where you can live with lower redundancy level than the one shown in Figure 15.5. Having a five
node cluster with each of the databases having a replication factor of two is also an option. Of course, in this mode 
losing two specific nodes in the cluster may mean that you'll lose access to a particular database.
What exactly will happen depends on your specific failure. If you lose two nodes immediately, that will cause the
database to become inaccessible. 

> **High availability cluster**
>
> RavenDB clusters and their highly available properties were discussed at length in Chapter 6 and Chapter 7.
> Here it is important to remember that a failed node will be automatically worked around by the cluster, which
> will redirect clients to another node, re-assign its tasks and start active monitoring for its health. 
>
> If the node is down for long enough, the cluster may decide to add extra copies of the databases that resided
> on the failed node, to ensure the proper amount of replica are kept, according to the configured replication
> factor.

If there is enough time between the two nodes failures of the cluster to react, it will spread the database whose node
went down to other nodes in the cluster to maintain the replication factor, ensuring that a second node failure will
not make a database inaccessible. 

This particular feature is nice to have when you have a five node cluster, but most times you'll use a replication
factor of three and not really have to think about the cluster moving database around. That is far more likely when
the number of nodes that you have in the cluster grow much higher. 

#### The Czar's Cluster

As you add more databases and nodes 

#### Primary / Secondary split

### Secondary topologies

#### External replication

#### Delayed external replication


### How many databases per node?

splitting resources between them

### Managing 