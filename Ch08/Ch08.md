
## Sharing data and making friends with ETL

[Sharing data and making friends with ETL]:(#integrations)

While talking about distributed work with RavenDB so far we have focused primarily on the work that RavenDB is doing to replicate data between different nodes
in the same cluster as part of the same database group or between different database in potentially different clusters. This mode of operation is simple, 
because you don't just setup the replication and RavenDB will take care of everything else. But there are other modes for distributing data in your systems.

Replication assumes that you have another RavenDB instance and that you want to replicate _everything_ to it. When replicating information, that is what you
want, but we also have a need to share a _part_ of the data. ETL^[Extract, Transform, Load] is the process in which we take data that resides in RavenDB
and push it to an exteranal source, potentionally transforming it along the way. That exteranal source can be another RavenDB instance or a relational database. 

Consider a micro service architecture and the Customer Benefits service. This service decides what kind of benefits the customer has. This can be anything
from free shipping to giving a discount on every 3rd jug of milk and the logic can be as simple as "this customer is in our membership club" to complex as 
trying to compute airline miles. Regardless of how the Customers Benefits service works, it need to let other parts of the system know about the 
benefits that this customer has. The Shipping service, the Helpdesk service and many others need to have that information. 

At the same time, we _really_ don't want them to poke their hands into the Customers Benefits database (or worse, have a shared database for everything)^[Doing
so is a great way to ensure that you'll have all the costs of a micro service architecture with none of the benefits.]. We could design an API between the 
systems, but then the Shipping service will be dependent on the Customer Benefits service to always be up. A better solution is to define an ETL process between
the two services and have the Customer Benefits service publish updates for the Shipping service to consume. Those updates are part of the publiccontract of
those services, mind. You shouldn't just copy the data between the databases.

Another example is the reporting database, RavenDB is a wonderful database for OTLP scenarios, but for reporting, your organization likely already have a 
solution, and there is really little need to replace that. But you can't just dump the data from RavenDB directly into a relational database and expect things
to work. We need to transform the data as we send it to match the relational model.

For all of those needs, RavenDB has the notion of ETL processes. RavenDB has builtin ETL to another RavenDB instance and to a relational database 
(such as MS SQL Server, Postgres, Oracle, MySQL, etc). 
Because RavenDB has native relational ETL, brown field systems will typically start using RavenDB by replacing a single component at a time, RavenDB is used
to speed up the behavior of high value targets, but instead of replacing the whole system, we use ETL processes to write the data to the existing relational
database. We'll cover that later in this chapter, discussing the deployment of RavenDB as a write-behind cache.

In most cases, the rest of the system doesn't even need to know that some parts are using RavenDB. This is using RavenDB as the write behind cache.
Some part of your application reads and writes to RavenDB directly, and RavenDB will make sure to update the relational system. 
We'll start talking about ETL processes between RavenDB instances, because we explore the whole ETL process without introducing another database instance.


### ETL processes to another RavenDB instance

Any non trivial system is going to have at least a few ETL processes and RavenDB has a good story on how to handle that. The simplest ETL process between
two RavenDB nodes is telling RavenDB that we want to send just a single collection to the other side. We first need to configure the connection string
we'll use. Because RavenDB ETL work between clusters, we may need to define multiple URLs, and it is easier to put it all in a single location. 

Go to `Settings` and then to `Connection Strings` and create a new RavenDB connection string. The easiest is to use create a new database in your cluster
and setup the connection string to it. As you can see in Listing 8.1, I've defined a connection string to the `helpdesk` database in the live test instance.

![Defining a connection string to another RavenDB instance.](./Ch08/img01.png)

With the connection string defined, we can now go ahead and build the actual ETL process to the remote instance. Go to `Settings` and then to 
`Manage Ongoing Tasks` and click the `Add Task` button and then select `RavenDB ETL`. You can see how this looks like in Figure 8.2.
Give it a name and select the previously defined connection string.

> **What about security?**
>
> We'll cover security in depth in the ["Securing your Ravens"](#security) chapter, but given that we have shown a connection string I wanted
> to address a not so minor issue that you probably noticed. We don't have a place here to define credentials. This is because we are talking
> to another RavenDB server, and for that we use x509 certificates. 
>
> In practice, this means that inside the same cluster ETL processes don't need any special configuration (nodes within the same cluster 
> already trust one another). Outside the cluster, you'll need to register the cluster's certificate with the remote cluster to allow the ETL
> process to work. You can read the full details of that in the security chapter.

![Defining an ETL process to another RavenDB instance](./Ch08/img02.png)

Now we need to let RavenDB know what will be ETL'ed to the other side. We do that by defining scripts that control the ETL proceess. Click on the 
`Add New Script` and give it a name such as "Employees to Helpdesk" and then select the Employees collection below, click on `Add Collection` and
then on `Save`. The result should look like Figure 8.3.

![Defining a simple "copy the whole collection" ETL script](./Ch08/img03.png)

Because we didn't specify anything in the ETL script, RavenDB will just copy the entire collection as is to the other side. This is useful on its own
because it allow us to share a single (or a few) collections with other parties with very little work. 

> **ETL is a database task, with bidirectional failover**
>
> In the previous chapter, we learned about database tasks and how the cluster will distribute such work among the different database instances. If a node
> fails, then the ETL task responsability will be assigned to another node. 
>
> It is important to note that in cases where we replicate to another RavenDB instance, we also have failvoer on the recieving end. Instead of specifying a 
> single URL, we can specify all the nodes in the cluster, and if one of the destination node is down, RavenDB will just run the ETL process against another
> node in the databse group topology.

It is _very_ important to remember that ETL is very different from replication. When a RavenDB node perform ETL to another, it is not replicating the data, it 
is _writing_ it. In other words, there are no conflicts and no attempt to handle such. Instead, we'll always _overwrite_ whatever exists on the other side. 
As far as the destination node is concerned, the ETL process is just another client writing data to the database. This is done because the data is _not_
the same. This is important because of a concept that we didn't touch so far, data ownership. 

> **Data ownership in distributed systems** 
>
> One of the key differences between a centralized system and a distributed system is the fact that in a distributed environment, different parts of the system
> can act on their local information without coordination from other parts of the system. In a centralized system, you can take a look or use transactions to
> ensure consistency. But in a distributed system, that is not possible or prohibitely expensive to do so.
> Because of this limitation, the concept of data ownership is very important.
> 
> Ideally, you want every piece of data to have a single well defined owner and only mutate that data through that owner. That allows you to put all the 
> validation and business roles in a single place and ensure overall consistency. Everything else in the system will get updates to that data through its 
> owner. 
>
> A database group, for example, needs to handle the issue of data ownership. Conceptually, it is the database group as a whole that owns the data stored in 
> the document. However, difference database instances can mutate the data. RavenDB handles that using change vectors and conflict detection and resolution.
> That works for replication inside the database group, because all the nodes in the group share the ownership of the data. But it doesn't work for ETL.
>
> The ETL source is the _owner_ the data and it is distributing the updates made to its data to interested parties. Given that it is the owner, it is expected
> that it can just update it to the latest version it has. That means that if you made any modification to the ETl'ed data, they will be lost. Instead of 
> modifyingg the ETL'ed data directly, you should create a compantion document that _you_ own. In other words, for ETL'ed data, the rule is that you can look,
> but not touch.
>
> An example of such a companion document is when you have ETL for users to the helpdesk system. The `users/123-B` document is owned by the users database and
> the helpdesk system will store all the information it needs about the user in `users/123-B/helpdesk` document, ensuring that there is no contention on the 
> ownership of documents.

So far we have only done ETL at the collection level, but we can also modify the data as it is going out. Let's see how we can do that, and why would we want
to do this. 

#### ETL Scripts

Sometimes you don't want to send a full document in the ETL process. Sometimes you want to filter them or modify their shape. This is quite important  
since ETL processes compose a part of your public interface. Instead of just sending your documents to remote destination willy nilly, you'll typically only
send data that you are interested in sharing, and in a well defined format.

Consider the employees we sent over the wire, we sent them as is, potentially exposing our own internal document structure and making it harder to modify in 
the future. Let us create a new ETL process which will send just the relevant details and add a script for sending redacted employee information over the wire. 
You can see how this looks like in Figure 8.4.

![Using ETL with a script to redact the results](./Ch08/img04.png)

It is important to understand that when we are using an ETL script to modify what is sent, we need to take into account that RavenDB will send just what we told
it to. This seems obvious, but it can catch people unaware. If you don't have a script, the data sent to the other side will include attachments and will go to 
the same collection as the source data.

However, when you provide your own script, you need to take responsability for this yourselves. Listing 8.1 shows an example of a slightly more complex example.

```{caption="Creating a subscription to process customers" .js}
var managerName = null;
if(this.ReportsTo !== null)
{
    var manager = load(this.ReportsTo);
    managerName = manager.FirstName + " " + manager.LastName;
}
    
loadToEmployees({
    Name: this.FirstName + " " + this.LastName,
    Title: this.Title,
    BornOn: new Date(this.Birthday).getFullYear(),
    Manager: managerName
});
```

The script in Listing 8.1 will send the employees, their title, birth year and manager over to the other side. You can see that the script is 
actually full blown javascript and allow you complete freedom with how you extract the data to load into the remote server. A word a caution
is required about using functions such as `load` in this context, though. While this will work just fine, the referencing document will not be 
updated if the referenced document has been updated. In other words, if the manager's name has been udpated, it will not trigger an update 
to the employees that report to this manager.

It is common to limit yourself to just the data from that particular document, since that make it easy to ensure that whenever the document is
changed, the ETL process will reflect these changes completely on the other side. 

> **Reseting the ETL process after update** 
>
> It is common to test out the ETL process as you develop it, but by default, updates to the ETL script will not applied to documents that
> were already sent. You can use the "Apply script to documents from beginning of time" option during the script update, as shown in 
> Figure 8.5 to let RavenDB know that it needs to start the ETL process for this script from scratch, rather than apply the update only 
> to new or updated documents.
> 
> ![Reseting the ETL process after a script update](./Ch08/img05.png)
>
> This is done to avoid expensive reset that would force RavenDB to send all the data all over again for a minor change.

Looking on the other side, you'll be able to see the replicated document, as shown in Listing 8.2.

```{caption="ETL'ed document on the side" .json}
{
	"BornOn": 1966,
	"Manager": "Steven Buchanan",
	"Name": "Anne Dodsworth",
	"Title": "Sales Representative",
	"@metadata": {
		"@collection": "Employees",
		"@change-vector": "A:84-4Xmt8lVCrkiCDii/CfyaWQ",
		"@id": "employees/9-A",
		"@last-modified": "2017-12-04T12:02:53.8561852Z"
	}
}
```

There are a few interesting things in the document in Figure 8.2. First, we can see that it has only a single change vector entry (for the destination
database) and the last modified date is when it was written to the destination, not when it updated on the source. 

#### Multiple documents from a single document

Another interesting ETL example is when we want to push multiple values out of a single document, as showing in Listing 8.3.

```{caption="Sending multiple documents from a single source document in ETL" .js}
loadToEmployees({
    Name: this.FirstName + " " + this.LastName,
    Title: this.Title,
    BornOn: new Date(this.Birthday).getFullYear(),
});

loadToAddresses({
    City: this.Address.City,
    Country: this.Address.Country,
    Address: this.Address.Line1
});
```

The results of the script in Listing 8.3 can be seen in Figure 8.6. You can see that the `Employees` documents were sent, but are also the addresses documents. 
For those, we use the prefix of the source document to be able to identify them after the fact. 

![Multiple outputs from a single source document on the destiantion](./Ch08/img06.png)

An important consideration for sending multiple documents from a single source document is that on every update to the source document _all_ the documents that 
were created from this document are refreshed. You don't have control over the ids being generated and shouldn't assume that they are fixed. 

> **Attachments and Revisions with RavenDB ETL** 
>
> Attachments are sent automatically over the wire when you send a full collection to the destination. However, revisions are not. 
> If you do use a script, there is currently no way to indicate that attachments should also be sent. This feature is planned but wasn't completed in time
> for the 4.0 RTM release.
> In the same vein, another feature that is upcoming is support for ETL processes on top of the revision data, similar to how it is possible to 
> use subscriptions with the `current` and `previous` versions of the document.

### Use cases for ETL between RavenDB instances

When you have a complex system, composed on more than a single application, it is considered to be a _Bad Form_ to just go peek inside
another application's database. Such behavior lead to sharing way too much between the applications and will require constant coordination
between the applications as you develop and deploy them. A boundary between application is required to avoid such issues.

> **Shared Database Integration Anti Pattern**
>
> This kind of behavior is called the Shared Database Integration and is considered to be an anti pattern. For more information on 
> why you should avoid such a system, I refer you to [Martin Fowler's post on the matter](https://martinfowler.com/bliki/IntegrationDatabase.html)
> and in particular to the summary: "most software architects that I respect take the view that *integration databases should be avoided*."

One way to create such a boundary is to mandate that any time that an application needs some data from another application, it will go there.
In concrete terms, whenever the helpdesk system need to lookup a user, it will go to the users application and ask it to get that user's data.
This is often referred to as a service boundary.

The problem with such a system is that many interactions inside a particular service require information that is owned by another service. 
Any support ticket opened by a user will require a call from the helpdesk service to the users management service for details and updates.
As you can imagine, such a system still require a lot of work. In particular, even though we have a clear boundary between the services and
division of responsability between them, there is still a strong temporal coupling between them.

Taking down the users management service for maintenance will require taking down everything else that needs to call to it. A better alternative
in this case is to not rely on making remote calls to a separate service, but to pull the data directly from our own database. This way, if the
users management service is down, it doesn't impact the operations for the helpdesk service. 

Note a key difference here between this type of architecture and the Shared Database model. You don't have a single shared database, instead the
helpdesk database contains a section in it that is updated by the users management service. In this manner, the ownership of the data is retained
by the users management service but the responsability for maintaining it and keep it up is with the helpdesk service.

If the users management service is taken down four maintenance, it has no impact on the helpdesk service, which can resume operations normally.
The design of the ETL processes in RavenDB is meant to allow such a system to be deployed and operated with a minimum of hassle. That is also partly
why the ownership rules and responsability for changes is built the way it is. 

ETL is explciitly about sending data that you own to a 3rd party which is interested in it, but doesn't own it. As such, any change you make will, 
by necessity, overwrite any local changes in the destination. If you are interested in a shared ownership model ETL is not the method you should use
but rather External Replication, discussed in the previous chapter.

#### Modeling concerns for ETL processes

An important aspect to the use of ETL processes in a multi service environment is the fact that the ETL process itself is part of the service contract 
which needs to be deployed, versioned and managed as such. In particular, the format of the documents that are sent via the ETL process compose part 
of the public interface of the service. As such, you should think carefully about the shape of the data you expose and the versioning considerations
around that.

Listing 8.1 is a good example of exposing just enough information to be useful for the other side without leaking implementation details or other aspects
that may change over time. A part of the design for the ETL processes was the notion that you may have different processes and different outputs for 
different destinations. In this way, you may collaborate with another service to update your contract while maintaining the same behavior for all others.

Another option is to only allow additive changes, so adding a property would be fine, but removing one wouldn't be. That _usually_ works, but unforutnately
[Hyrum's Law](http://www.hyrumslaw.com/) applies and even such innocuous changes can break a 3rd party.

#### Other use cases for ETL processes

Beyond using ETL for dessiminating documents between services, there are a few other scenarios in which they can be useful. 


### Failover and recovery in a distributed systems



### Working with attachments

### Working with revisions

// TODO: this isn't there yet

### Required indexes
