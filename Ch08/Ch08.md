
## Sharing data and making friends with ETL

[Sharing data and making friends with ETL]:(#integrations)

While talking about distributed work with RavenDB so far we have focused primarily on the work that RavenDB is doing to replicate data between different nodes
in the same cluster as part of the same database group or between different database in potentially different clusters. This mode of operation is simple, 
because you don't just setup the replication and RavenDB will take care of everything else. But there are other modes for distributing data in your systems.

Replication assumes that you have another RavenDB instance and that you want to replicate _everything_ to it. When replicating information, that is what you
want, but we also have a need to share a _part_ of the data. ETL^[Extract, Transform, Load] is the process in which we take data that resides in RavenDB
and push it to an exteranal source, potentionally transforming it along the way. That exteranal source can be another RavenDB instance or a relational database. 

Consider a micro service architecture and the Customer Benefits service. This service decides what kind of benefits the customer has. This can be anything
from free shipping to giving a discount on every 3rd jug of milk and the logic can be as simple as "this customer is in our membership club" to complex as 
trying to compute airline miles. Regardless of how the Customers Benefits service works, it need to let other parts of the system know about the 
benefits that this customer has. The Shipping service, the Helpdesk service and many others need to have that information. 

At the same time, we _really_ don't want them to poke their hands into the Customers Benefits database (or worse, have a shared database for everything)^[Doing
so is a great way to ensure that you'll have all the costs of a micro service architecture with none of the benefits.]. We could design an API between the 
systems, but then the Shipping service will be dependent on the Customer Benefits service to always be up. A better solution is to define an ETL process between
the two services and have the Customer Benefits service publish updates for the Shipping service to consume. Those updates are part of the publiccontract of
those services, mind. You shouldn't just copy the data between the databases.

Another example is the reporting database, RavenDB is a wonderful database for OTLP scenarios, but for reporting, your organization likely already have a 
solution, and there is really little need to replace that. But you can't just dump the data from RavenDB directly into a relational database and expect things
to work. We need to transform the data as we send it to match the relational model.

For all of those needs, RavenDB has the notion of ETL processes. RavenDB has builtin ETL to another RavenDB instance and to a relational database 
(such as MS SQL Server, Postgres, Oracle, MySQL, etc). 
Because RavenDB has native relational ETL, brown field systems will typically start using RavenDB by replacing a single component at a time, RavenDB is used
to speed up the behavior of high value targets, but instead of replacing the whole system, we use ETL processes to write the data to the existing relational
database. We'll cover that later in this chapter, discussing the deployment of RavenDB as a write-behind cache.

In most cases, the rest of the system doesn't even need to know that some parts are using RavenDB. This is using RavenDB as the write behind cache.
Some part of your application reads and writes to RavenDB directly, and RavenDB will make sure to update the relational system. 
We'll start talking about ETL processes between RavenDB instances, because we explore the whole ETL process without introducing another database instance.


### ETL processes to another RavenDB instance

Any non trivial system is going to have at least a few ETL processes and RavenDB has a good story on how to handle that. The simplest ETL process between
two RavenDB nodes is telling RavenDB that we want to send just a single collection to the other side, without any transformations. Let us see how we can get
that working, shall we?

In the studio, go to `Settings`, `Manage Ongoing Tasks` and click the `Add Task` button and then select `RavenDB ETL`. You can see how this looks like 
in Figure 8.1

![Defining a trivial ETL process between two RavenDB instances](./Ch08/img01.png)

Add the server URL and the database you want to replicate to, the click on `Add` and add the Companies collection and cick `Save`. RavenDB will now take
over and make sure that any update to a `Companies` document is sent to the other side. 

> **ETL is a database task, with bidirectional failover**
>
> In the previous chapter, we learned about database tasks and how the cluster will distribute such work among the different database instances. If a node
> fails, then the ETL task responsability will be assigned to another node. 
>
> It is important to note that in cases where we replicate to another RavenDB instance, we also have failvoer on the recieving end. Instead of specifying a 
> single URL, we can specify all the nodes in the cluster, and if one of the destination node is down, RavenDB will just run the ETL process against another
> node in the databse group topology.

It is _very_ important to remember that ETL is very different from replication. When a RavenDB node perform ETL to another, it is not replicating the data, it 
is _writing_ it. In other words, there are no conflicts and no attempt to handle such. Instead, we'll always overwrite whatever exists on the other side. 
As far as the destination node is concerned, the ETL process is just another client writing data to the database. This is done because the data is _not_
the same. This is important because of a concept that we didn't touch so far, data ownership. 

> **Data ownership in distributed systems** 
>
> One of the key differences between a centralized system and a distributed system is the fact that in a distributed environment, different parts of the system
> can act on their local information without coordination from other parts of the system. In a centralized system, you can take a look or use transactions to
> ensure consistency. But in a distributed system, that is not possible. Because of this limitation, the concept of data ownership is very important.
> 
> Ideally, you want every piece of data to have a single well defined owner and only mutate that data through that owner. That allows you to put all the 
> validation and business roles in a single place and ensure overall consistency. Everything else in the system will get updates to that data through its 
> owner. 
>
> A database group, for example, needs to handle the issue of data ownership. Conceptually, it is the database group as a whole that owns the data stored in 
> the document. However, difference database instances can mutate the data. RavenDB handles that using change vectors and conflict detection and resolution.
> That works for replication inside the database group, because all the nodes in the group share the ownership of the data. But it doesn't work for ETL.
>
> The ETL source is the owner the data and it is distributing the updates made to its data to interested parties. Given that it is the owner, it is expected
> that it can just update it to the latest version it has. That means that if you made any modification to the ETl'ed data, they will be lost. Instead of 
> modifyingg the ETL'ed data directly, you should create a compantion document that _you_ own. In other words, for ETL'ed data, the rule is that you can look,
> but not touch.

So far we have only done ETL at the collection level, but we can also modify the data as it is going out. Let's see how we can do that, and why would we want
to do this. 

#### ETL Scripts

Sometimes you don't want to send a full document in the ETL process. Sometimes you want to filter them or modify their shape. This is quite important for 
since ETL processes compose a part of your public interface. You don't want to expose all your internal state to the world, instead, you want to expose just
what other parties need to be able to integrate with your data.

Let us extend the ETL example to filter some of the data. For example, we might want to push all the customers that agreed to take part in a feedback program



### Document ids in replication

#### Multiple items from a single document

### Working with attachments

### Working with revisions


### Required indexes
