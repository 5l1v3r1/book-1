
# Setting up a RavenDB Cluster

[Clustering Setup]: #clustering-setup

You might be familiar with the term "murder of crows" as a way to refer to a group for crows^[If you are interested in
learning why, I found [this answer](https://www.quora.com/Why-is-a-group-of-crows-called-a-murder) fascinating]. It has
been used in literature and arts many times. Of less reknown is the group term for ravens, which is "unkindness". 
Personally, in the name of all ravens, I'm torned between being insulted and amused. 

Professionally, setting up RavenDB as a cluster on a group of machines is a charming exercise (however, that term is 
actually reserved for finches) that bring a sense of exaltation (taken too, by larks) by how pain free this is. I'll
now end my voyage into the realm of ornithology's etymology and stop speaking in tongues.

On a more serious note, the fact that RavenDB clustering is easy to setup is quite important, because it means that it
is much more approachable. You don't need to have an expert at hand at all times to setup a cluster, and it should mostly
self manage itself. That means that the decision to go from a single server to a cluster is much easier and you can get
the benefit of that sooner.

## An overview of a RavenDB Cluster

A RavenDB cluster is three or more machines^[It doesn't make a lot of sense to have a cluster with just two nodes, since 
we'll require both of the nodes to always be up in most cases. There are certain advanced scenario where such topology 
might make sense, and we'll touch on that briefly in the [Clustering in the Deep End](#clustering-deep-dive) chapter.] 
that have been joined together. 

But what is the point of doing that? Well, you can create databases on a RavenDB cluster, and you can specify that the 
cluster should manage them on its own. A database can live on a single node, some number of the nodes or even all the 
nodes. Each node will then hold a complete copy of the database and will be able to serve all queries, operations and 
writes. 

The cluster will also distribute work among the various nodes automatically, handle failures and recovery and in general
act to make sure that everything is humming along merrily. 

Operations in RavenDB are usually divided to cluster wide operations (including cluster wide operations that impact only
a single database) and internal database operations. For example, creating a new database is a cluster wide operation, 
while writing a document to a database only impact that particular database.

The reason this distinction is important is that RavenDB actually operate two distinct layers in its distributed system. 
The first, at the cluster level, is composed of nodes working together to achieve the same goal. This is done by using
the [Raft consensus protocol](https://raft.github.io/) and having the cluster members vote to select a strong leader 
among themselves. 

This leader, in turn, is responsbile for such things as monitoring the cluster health, selecting the preferred node that
clients will use for each database, configuring the databases and making sure that there is a consistent way to make 
decisions at the cluster level. At the database level, however, instead of selecting a leader, the nodes are all working together, cooperatively and as equals. 

Why do we want that? Wouldn't it be better to have just a single mode of operation? The answer is that it probably would 
be simpler, but not necessarily better. Let us examine the pros and cons for each approach, and how they are used by 
RavenDB.

Cluster consensus with strong Leader (Raft algorithm) provide for strong consistency and ensure that as long as a 
majority of the nodes are functioning and can talk to one another we'll remain in operations. The strong consistency
mode is quite nice, since it means the cluster can make a decision (such as add a database to a node) and we can be 
sure that this decision will either be accepted by the entire cluster (eventually) or we fail to register that decision.
That means that each node can operate on its on internal state for most operations, resulting in a more robust system. 

> **The CAP theorem and database design**
> 
> The CAP theorem, also named Brewer's theorem states that given Consistency, Availability and Partition tolerance, a 
> system must choose two out of the three. It is not possible to provide all three options.
> 
> In practice, since all production systems are vulnerable to partitions, it means that you can select to be either CP
> (Consistent and Partition tolerant) or AP (Available and Partition tolerant). Different database systems have decided
> to take different design decisions to handle this issue. 
>
> RavenDB has opted to be CP and AP. That isn't quite as impossible as it sounds. It is just that it isn't trying to be
> CA and AP on the same layer. With RavenDB, the cluster layer is CP (it is always consistent, but may not be available
> in the precense of a partition) but the database layer is AP (it is always available, even if there is a partition, but
> it is eventually consistent).

However, if a majority of the nodes aren't available, we cannot proceed. This is pretty much par the course for consensus 
algorithms. Another issue with consensus algorithms is that they incur additional network roundtrips for each operation. 
For cluster maintenance and the configuration of databases, RavenDB uses the Raft consensus protocol. The RavenDB 
implementation or Raft is codenamed Rachis^[Rachis is the central shaft of pennaceous feathers.].

Databases, on the other hand, are treated quite differently. Each node in the cluster have a full copy of the topology, 
which specify which nodes host which databases. That information is managed by the Rachis, but each node is able to act
upon it indepdnently. 

The connection between the databases in different nodes do not go through Rachis or any other consensus protocol. Instead,
they are direct connections between the various nodes that hold a particular database, and they from a multi master mesh. 
A write to any of those nodes will be automatically replicated to all the other nodes.

This design result in a very robust system. First, you define your cluster, distribute your databases between the nodes,
assign work, etc. Once the cluster is setup, each database instance on the various nodes is independent. That means that
it can accept writes without consulting the other instances of the same database.

The design of RavenDB was heavily influenced by the [Dynamo paper](http://dl.acm.org/citation.cfm?id=1294281) and one of 
the key features was the notion that writes are _important_. If a user is writing to the database, you really
want to hold on to that data. In order to achieve that, RavenDB uses multi master replication inside a database, and is 
always able to accept writes.

In other words, even if the majority of the cluster is down, as long as a single node is available, we can still process 
read and writes.

This is a lot to digest, and at this point, it is probably all a bit theoretical for you. We'll get back to the different
layers that compose the RavenDB distributed architecture later on. For now, let us get ready to setup our first cluster.


## Your first RavenDB Cluster

We have worked with RavenDB before, but we have done that only with a single node. What you might not be aware of is that
we actually worked with a cluster. Admittedly, it is a cluster of a single node, and it isn't that interesting from the
point of view of distributed systems. However, even a single node instance is always running as a cluster. That has the 
advantage that the same codepaths are always being used and excersized. 

Another major benfit is that you don't need any special steps to get to a cluster, all we need to do is to add a few 
nodes. Let us see how it works in practice. Close any instances of the RavenDB server that you currently have running 
and open the command line termnial at the RavenDB folder.

> **Downloading RavenDB**
>
> In [Zero To Hero](#zero-to-ravendb) we have gone over how to download and seutp RavenDB for various environments. 
> If you skipped on that you can go to the RavenDB Download Page at 
> [https://ravendb.net/download](https://ravendb.net/download). And download the zip package for your platform. 
> The rest of this chapter assumes that you run all commands inside the unzipped RavenDB directory.

The first thing we want to do is to run an instance of RavenDB. Assuming you are running in `PowerShell` in the RavenDB
folder, please run the following command:

	$cmdLine =  "--Raven/ServerUrl=http://127.0.0.{0}:8080 " `
		+ 	" --Raven/LogsDirectory=Logs/{0} " `
		+ 	" --Raven/DataDir=Data/{0}"
	start ./Server/Raven.Server.exe ( $cmdLine -f 1 )
		
The `$cmdLine` is the commnad line arguments that we'll pass to RavenDB, and we use this construct because it allows 
us to customize the arguments easily. In the string, we use `{0}` which are later replaced by the `1` later on using the
`$cmdLine -f 1` syntax. This is a somewhat convoluted way to ask RavenDB to listen to `127.0.0.1:8080`, write the logs to
`Logs/1` and write the data file to `Data/1`. We'll see exactly why we do it this wan in a bit. 

You can point your browser to `http://127.0.0.1:8080` and see the familiar studio. Let us create a new database and call
it "Spade". The result should look like Figure 6.1. 

![A simple database on a single node](./Ch06/img01.png)

Let us go into the database and create a couple of documents ("users/1" and "users/2" with your first and last name as 
the name). We are merely going about creating this database and those documents so we can see how we can grow a RavenDB
cluster. 

Now, we need to bring up another RavenDB node. Typically, you would do that on a separate machine, but to make the demo
easier, we'll just run it on the same node. Still in the same `PowerShell` session, now run the following command.

	start ./Server/Raven.Server.exe ( $cmdLine -f 2 )

This used the previously defined `$cmdLine` with a new format string and result in another RavenDB instance, this time 
bound to `127.0.0.2:8080` with logs at `Logs/2` and the data in `Data/2`. You can go to `http://127.0.0.2:8080` and 
see that you can also see the studio. However, unlike the instance running in `127.0.0.1:8080`, there are no databases
in the `127.0.0.2:8080`'s studio. 

Another difference between the two nodes can be seen at the bottom center, as you can see in Figure 6.2. 

![The node tag in two nodes](./Ch06/img02.png)

You can see in `127.0.0.2:8080` that the node is marked with a question mark. This is because this is a new node that 
didn't have any operations made on it. On the other hand, the `127.0.0.1:8080` node had a database created on it and 
as such is a single node cluster marked with the node tag `A`. 

Now, let us add a node to our cluster. Go to `127.0.0.1:8080` and then to `Manage server` and then to `Cluster`. The
cluster management screen is shown in Figure 6.3. 

![A single node cluster in the management screen](./Ch06/img03.png)

Click on `Add node` and enter `http://127.0.0.2:8080`. The new node will show up as a cluster member , as shown in Figure 6.4.

![Our cluster after adding the 2nd node](./Ch06/img04.png)

If you'll go the `127.0.0.2:8080` in the browser you'll several interesting things. First, at the bottom, you can see 
that we are no longer unknown node, instead, the `127.0.0.2:8080` server has been designated as node B.

> **Node tags and readability**
>
> To simplify matters, RavenDB assing each node in the cluster a tag. Those tags are used to identify the nodes. They
> do not replace the node URLs, but supplement them. Being able to say "Node B" instead of `127.0.0.2:8080` or even 
> WIN-U0RFOJCTPUU` is quite helpful, we found.
> It also help in more complex network environments where a node may be access via several hosts, IP and names. From
> now on I'm going to refer to node by their tags, instead of keep wrting 

You can also see that we have the "Spade" database, but it is marked as offline on Node B. This is because this database
is only setup to be held on Node A. Click on the "Manage group" button on the "Spare" database, which will take you to
a page allowing you to add a node to the database. Click on Node B and then click on "Add node to group". 

This is it, you now have a database that is spanning two nodes. You can go to Node B and see that the "Spade" database
status moved from `Offline` to `Online` and you can enter into the database and see the `users/1` and `users/2` documents
that we previously created on Node B. RavenDB has automatically moved them to the secondary node.

Play around for a bit with this setup. Creating and update documents on both nodes, see how they flow between the two 
database instances. Next, we are going to take our cluster and beat on it until it squeal, just to see how it handles 
failure conditions.

### Kicking the tires of your RavenDB Cluster

Running on two nodes is an interesting exprience. On the one hand, we now have our data on two separate nodes, and assuming
that we have actually run them on two separate machines, we are safe from a single machine going in flame. However, two nodes
aren't actually enough to handle most failure cases.

Let us consider a situation when a node goes down. We can simulate that by killing the RavenDB process running Node A (the one
running the server on `127.0.0.1:8080`). Now, go to Node B (`127.0.0.2:8080`) and go to Manage server and then to Cluster. You'll
see both nodes there, but that none of them is marked as leader. This is because Node B is unable to communicate with Node A 
(this makes sense, we took Node A down). But there isn't anyone else in the cluster that it can talk to. 

The cluster at this point is unable to proceed. There is no leader to be found and no way to select one. The _database_, on the
other hand, can function just fine. Go to the "Spade" database on Node B and add a new document (`users/3`). 
You'll observe that there is no issue with this at all. The new document was created normally and without issue. This is because
each database is independent. Each database instance will cooperate with the other instances, but it isn't dependant on them.

> **How clients respond to node failures?**
>
> When a node fails, the clients will transparently will move to another node that hold the database that they are connected to.
> One of the tasks of the cluster's leader is to maintain the database topology, which clients will fetch as part of their 
> initialization (and then keep current). We'll talk about failover from the client side more later in this chapter.

Now, start the first node. You can do that using the following command:

	start ./Server/Raven.Server.exe ( $cmdLine -f 1 )

The node will start, reconnect into the cluster and if you'll check the cluster state, you'll see something similar to Figure 6.5.

![New leader selected after recovery of failed node](./Ch06/img05.png)

It is possible that in your case, the leader will be Node A. There are no guarantees about who the leader would be. Now you can 
check the "Spade" database inside Node A, and you should see the `users/3` document inside it that was replicated from Node B. 

So our database remained operational even while we had just a single node available. Our _cluster_ was down, however, since it 
couldn't elect a leader. But what does this mean? During the time that we only had Node B, we wouldn't be able to do any cluster
wide operations.

Those include creating a new database and monitoring the health of other nodes. But a lot of database specific configuration 
(anything that impact all instances of a database) goes through the cluster as well. For example, we wouldn't be able to schedule 
a backup with the cluster down^[Of course, we will be able to take a backup manually. See the discussion about cluster wide 
tasks later in this chapter] and you'll not be able to deploy new indexes or modify database configuration. 

> **Operations in failed cluster**
>
> The list of operations that you can't perform if the cluster as a whole is down (but isolated nodes are up) seems frightening.
> However, a closer look at those operations will show you that they are typically not run of the mill operations. Those are one
> time operations (setting indexes, setting a backup schedule, creating databases, adding nodes to cluster, etc).
> 
> Normal read/write/query operations from each database instance will proceed normally and your applications shouldn't fail
> over automatically and immediately. On the other hand, background operations, such as subscriptions or ETL will not be able
> to proceed until the cluster is back up (which require a majority of the nodes to be able to talk to one another).

Remember the previous mention that running with two nodes in a cluster is strange? This is because any single node going down will
push us down to this half & half state. Normally, you'll run three or more nodes, so let us exapnd our cluster even further.

### Expanding your cluster

You can probably guess what we are going to do now. In exactly the same way that we previously added Node B, we are going to add
the three new nodes. Please execute the following commands:

	start ./Server/Raven.Server.exe ( $cmdLine -f 3 )
	start ./Server/Raven.Server.exe ( $cmdLine -f 4 )
	start ./Server/Raven.Server.exe ( $cmdLine -f 5 )

Then in the studio, go into "Manage server" and then into "Cluster" and add the new nodes (`http://127.0.0.3:8080`, 
`http://127.0.0.4:8080`, `http://127.0.0.5:8080`). The end result of this is shown in Listing 6.6.

![A five nodes cluster](./Ch06/img06.png)

You can click on the nodes in the cluster to open the studio on each node. If you'll do that, look at the tab headers, it will
tell you which node you are on, as you can see in Figure 6.7.

![An easy way to know which node you are on is to look at the tab icon](./Ch06/img07.png)

Now that we have our cluster, we still need to understand the layout of the databases in it. Here is what it looks like when we 
open the studio on Node D, as shown in Figure 6.8. 

![The Spade database as seen from Node D](./Ch06/img08.png)

The "Spade" database is marked as offline because it doesn't reside on this node. This lead to a few important discoveries, 
databases that we manually configured are going to remain on the nodes that they have been configured to run on. It also appears
that we can see the entire cluster topology from every node. 

Now, let us actually use our cluster and create a new database, called "Pitchfork"^[I'm on a garden tools naming streak, it 
appears]. Everyone knows that a proper pitchfork has three tines, the four tines pitchfork is used for shoveling manure while 
a novel tree tine pitchfork is the favorite tool of Poseidon. As such, it is only natural that our "Pitchfork" database will 
have a replication factor of 3. Once that is done, just create the database and observe the results.

Since we didn't explictly specified where the new database is going to reside, the cluster will distribute it to three nodes in 
the cluster. This means that we now have a 5 nodes cluster with two databases, as you can see in Figure 6.9.

![A spade and a pitchfork in our RavenDB cluster](./Ch06/img09.png)

Figure 6.9 shows the studio from the studio in Node D. So we have Pitchfork on three nodes and Spade on two. You can go ahead and
create a few documents on the Pitchfork database and observe how they are spread to the rest of the database instances in the 
cluster.

### Appropriate utilization of your cluster

Setting up a five nodes cluster just to run a couple of database seems pretty wasteful, I mean, Node E doesn't even have a 
single database to take care of. Why would we do something like this? Typically, production clusters are setup with either three 
or five nodes or a much higher number. If you have one or two databases, you'll typically deploy a three node cluster and make 
sure that the database(s) on it are spread across all the nodes in the cluster.

Sometimes you'll have a five node cluster with the data replicated five times between the nodes. For maximum survivability. 
But in most cases, when you go to five nodes or higher, you are running a number of databases on the cluster. For instance, 
consider the situation in Figure 6.10.

![A cluster hosting a whole garden shed of databases](./Ch06/img10.png)

In Figure 6.10 you can see that we created eight databases and that they are all spread out throughout the cluster. This means
that there is no single point of failure for the cluster. In fact, we can lose any node in the cluster and still remain in full
operations.

I intentionally defined some of the database so they'll only run on two nodes, instead of three. In this configuration, it is 
possible to lose access to a few databases if we kill Node A and Node B. 

> **Typical deployment in a cluster**
> 
> The normal approach is to decide how important your data is. It it common to require the data to reside on three separate 
> nodes, regardless of the cluster size. In more critical systems, you'll either have this spread out across multiple data
> centers or have a copy (or copies) of the database being maintained by the cluster in a second datacenter by RavenDB, we'll
> discuss this feature in the [Sharing data and making friends with ETL](#integrations) chapter when we'll discuss External
> Replication.




## Distribution of work inside a database group

## Utilizing your cluster

many db, tenants, micro services