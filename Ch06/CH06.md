
# Setting up a RavenDB Cluster

[Clustering Setup]: #clustering-setup

You might be familiar with the term "murder of crows" as a way to refer to a group for crows^[If you are interested in
learning why, I found [this answer](https://www.quora.com/Why-is-a-group-of-crows-called-a-murder) facinating]. It has
been used in literature and arts many times. Of less reknown is the group term for ravens, which is "unkindness". 
Personally, in the name of all ravens, I'm torned between being insulted and amused. 

Professionally, setting up RavenDB as a cluster on a group of machines is a charming exercise (however, that term is 
actually reserved for finches) that bring a sense of exaltation (taken too, by larks) by how pain free this is. I'll
now end my voyage into the realm of ornithology's etymology and stop speaking in tongues.

On a more serious note, the fact that RavenDB clustering is easy to setup is quite important, because it means that it
is much more approachable. You don't need to have an expert at hand at all times to setup a cluster, and it should mostly
self manage itself. That means that the decision to go from a single server to a cluster is much easier and you can get
the benefit of that sooner.

## An overview of a RavenDB Cluster

A RavenDB cluster is three or more machines^[It doesn't make a lot of sense to have a cluster with just two nodes, since 
we'll require both of the nodes to always be up in most cases. There are certain advanced scenario where such topology 
might make sense, and we'll touch on that briefly in the [Clustering in the Deep End](#clustering-deep-dive) chapter.] 
that have been joined together. 

But what is the point of doing that? Well, you can create databases on a RavenDB cluster, and you can specify that the 
cluster should manage them on its own. A database can live on a single node, some number of the nodes or even all the 
nodes. Each node will then hold a complete copy of the database and will be able to serve all queries, operations and 
writes. 

The cluster will also distribute work among the various nodes automatically, handle failures and recovery and in general
act to make sure that everything is humming along merrily. 

Operations in RavenDB are usually divided to cluster wide operations (including cluster wide operations that impact only
a single database) and internal database operations. For example, creating a new database is a cluster wide operation, 
while writing a document to a database only impact that particular database.

The reason this distinction is important is that RavenDB actually operate two distinct layers in its distributed system. 
The first, at the cluster level, is composed of nodes working together to achieve the same goal. This is done by using
the [Raft consensus protocol](https://raft.github.io/) and having the cluster members vote to select a strong leader 
among themselves. 

This leader, in turn, is responsbile for such things as monitoring the cluster health, selecting the preferred node that
clients will use for each database, configuring the databases and making sure that there is a consistent way to make 
decisions at the cluster level. At the database level, however, instead of selecting a leader, the nodes are all working together, cooperatively and as equals. 

Why do we want that? Wouldn't it be better to have just a single mode of operation? The answer is that it probably would 
be simpler, but not necessarily better. Let us examine the pros and cons for each approach, and how they are used by 
RavenDB.

Cluster consensus with strong Leader (Raft algorithm) provide for strong consistency and ensure that as long as a 
majority of the nodes are functioning and can talk to one another we'll remain in operations. The strong consistency
mode is quite nice, since it means the cluster can make a decision (such as add a database to a node) and we can be 
sure that this decision will either be accepted by the entire cluster (eventually) or we fail to register that decision.
That means that each node can operate on its on internal state for most operations, resulting in a more robust system. 

> **The CAP theorem and database design**
> 
> The CAP theorem, also named Brewer's theorem states that given Consistency, Availability and Partition tolerance, a 
> system must choose two out of the three. It is not possible to provide all three options.
> 
> In practice, since all production systems are vulnerable to partitions, it means that you can select to be either CP
> (Consistent and Partition tolerant) or AP (Available and Partition tolerant). Different database systems have decided
> to take different design decisions to handle this issue. 
>
> RavenDB has opted to be CP and AP. That isn't quite as impossible as it sounds. It is just that it isn't trying to be
> CA and AP on the same layer. With RavenDB, the cluster layer is CP (it is always consistent, but may not be available
> in the precense of a partition) but the database layer is AP (it is always available, even if there is a partition, but
> it is eventually consistent).

However, if a majority of the nodes aren't available, we cannot proceed. This is pretty much par the course for consensus 
algorithms. Another issue with consensus algorithms is that they incur additional network roundtrips for each operation. 
For cluster maintenance and the configuration of databases, RavenDB uses the Raft consensus protocol. The RavenDB 
implementation or Raft is codenamed Rachis^[Rachis is the central shaft of pennaceous feathers.].

Databases, on the other hand, are treated quite differently. Each node in the cluster have a full copy of the topology, 
which specify which nodes host which databases. That information is managed by the Rachis, but each node is able to act
upon it indepdnently. 

The connection between the databases in different nodes do not go through Rachis or any other consensus protocol. Instead,
they are direct connections between the various nodes that hold a particular database, and they from a multi master mesh. 
A write to any of those nodes will be automatically replicated to all the other nodes.

This design result in a very robust system. First, you define your cluster, distribute your databases between the nodes,
assign work, etc. Once the cluster is setup, each database instance on the various nodes is independent. That means that
it can accept writes without consulting the other instances of the same database.

The design of RavenDB was heavily influenced by the [Dynamo paper](http://dl.acm.org/citation.cfm?id=1294281) and one of 
the key features was the notion that writes are _important_. If a user is writing to the database, you really
want to hold on to that data. In order to achieve that, RavenDB uses multi master replication inside a database, and is 
always able to accept writes.

In other words, even if the majority of the cluster is down, as long as a single node is available, we can still process 
read and writes.

This is a lot to digest, and at this point, it is probably all a bit theoretical for you. We'll get back to the different
layers that compose the RavenDB distributed architecture later on. For now, let us get ready to setup our first cluster.


## Your first RavenDB Cluster

We have worked with RavenDB before, but we have done that only with a single node. What you might not be aware of is that
we actually worked with a cluster. Admittedly, it is a cluster of a single node, and it isn't that interesting from the
point of view of distributed systems. However, even a single node instance is always running as a cluster. That has the 
advantage that the same codepaths are always being used and excersized. 

Another major benfit is that you don't need any special steps to get to a cluster, all we need to do is to add a few 
nodes. Let us see how it works in practice. Close any instances of the RavenDB server that you currently have running 
and open the command line termnial at the RavenDB folder.

> **Downloading RavenDB**
>
> In [Zero To Hero](#zero-to-ravendb) we have gone over how to download and seutp RavenDB for various environments. 
> If you skipped on that you can go to the RavenDB Download Page at 
> [https://ravendb.net/download](https://ravendb.net/download). And download the zip package for your platform. 
> The rest of this chapter assumes that you run all commands inside the unzipped RavenDB directory.

The first thing we want to do is to run an instance of RavenDB. Assuming you are running in `PowerShell` in the RavenDB
folder, please run the following command:

	$cmdLine =  "--Raven/ServerUrl=http://127.0.0.{0}:8080 " `
		+ 	" --Raven/LogsDirectory=Logs/{0} " `
		+ 	" --Raven/DataDir=Data/{0}"
	start ./Server/Raven.Server.exe ( $cmdLine -f 1 )
		
The `$cmdLine` is the commnad line arguments that we'll pass to RavenDB, and we use this construct because it allows 
us to customize the arguments easily. In the string, we use `{0}` which are later replaced by the `1` later on using the
`$cmdLine -f 1` syntax. This is a somewhat convoluted way to ask RavenDB to listen to `127.0.0.1:8080`, write the logs to
`Logs/1` and write the data file to `Data/1`. We'll see exactly why we do it this wan in a bit. 

You can point your browser to `http://127.0.0.1:8080` and see the familiar studio. Let us create a new database and call
it "Spade". The result should look like Figure 6.1. 

![A simple database on a single node](./Ch06/img01.png)

Let us go into the database and create a couple of documents ("users/1" and "users/2" with your first and last name as 
the name). We are merely going about creating this database and those documents so we can see how we can grow a RavenDB
cluster. 

Now, we need to bring up another RavenDB node. Typically, you would do that on a separate machine, but to make the demo
easier, we'll just run it on the same node. Still in the same `PowerShell` session, now run the following command.

	start ./Server/Raven.Server.exe ( $cmdLine -f 2 )

This used the previously defined `$cmdLine` with a new format string and result in another RavenDB instance, this time 
bound to `127.0.0.2:8080` with logs at `Logs/2` and the data in `Data/2`. You can go to `http://127.0.0.2:8080` and 
see that you can also see the studio. However, unlike the instance running in `127.0.0.1:8080`, there are no databases
in the `127.0.0.2:8080`'s studio. 

Another difference between the two nodes can be seen at the bottom center, as you can see in Figure 6.2. 

![The node tag in two nodes](./Ch06/img02.png)

You can see in `127.0.0.2:8080` that the node is marked with a question mark. This is because this is a new node that 
didn't have any operations made on it. On the other hand, the `127.0.0.1:8080` node had a database created on it and 
as such is a single node cluster marked with the node tag `A`. 

Now, let us add a node to our cluster. Go to `127.0.0.1:8080` and then to `Manage server` and then to `Cluster`. The
cluster management screen is shown in Figure 6.3. 

![A single node cluster in the management screen](./Ch06/img03.png)

Click on `Add node` and enter `http://127.0.0.2:8080`. The new node will show up as a cluster member , as shown in Figure 6.4.

![Our cluster after adding the 2nd node](./Ch06/img04.png)

If you'll go the `127.0.0.2:8080` in the browser you'll several interesting things. First, at the bottom, you can see 
that we are no longer unknown node, instead, the `127.0.0.2:8080` server has been designated as node B.

> **Node tags and readability**
>
> To simplify matters, RavenDB assing each node in the cluster a tag. Those tags are used to identify the nodes. They
> do not replace the node URLs, but supplement them. Being able to say "Node B" instead of `127.0.0.2:8080` or even 
> WIN-U0RFOJCTPUU` is quite helpful, we found.
> It also help in more complex network environments where a node may be access via several hosts, IP and names. From
> now on I'm going to refer to node by their tags, instead of keep wrting 

You can also see that we have the "Spade" database, but it is marked as offline on Node B. This is because this database
is only setup to be held on Node A. Click on the "Manage group" button on the "Spare" database, which will take you to
a page allowing you to add a node to the database. Click on Node B and then click on "Add node to group". 

This is it, you now have a database that is spanning two nodes. You can go to Node B and see that the "Spade" database
status moved from `Offline` to `Online` and you can enter into the database and see the `users/1` and `users/2` documents
that we previously created on Node B. RavenDB has automatically moved them to the secondary node.

Play around for a bit with this setup. Creating and update documents on both nodes, see how they flow between the two 
database instances. Next, we are going to take our cluster to the next level. 

### Growing our cluster
