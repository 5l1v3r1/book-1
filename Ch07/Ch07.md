
## Scaling distributed work RavenDB

[Distributed Deep Dive]:(#clustering-deep-dive)

In the previous chapter, we went over how RavenDB clusters and database groups work, we looked at the nitty gritty details, such as conflicts
and change vectors and how a cluster can handle failover and recovery. But we haven't actually talked about how to actually make use of a cluster.
This is primarily what we'll cover in this chapter. How to properly utilize your RavenDB cluster to best effect.

We'll cover how to grow your cluster to include a large number of nodes and how to host a _lot_ of databases in the clsuter, how we can automatically
have the cluster adjust the nodes a database reside on to ensure a minimum number of replicas and how we can deploy RavenDB in a geo distributed 
environment.

But first, we need to go back a bit and discuss the distributed mechanims in RavenDB, the cluster and the database group. The separation RavenDB makes between 
the cluster and the database group can be artifical. If you are running a single database on all your nodes, you usually
will not make any distinction between the cluster as a whole and the database group. This distinction starts to become a lot more important if you are working
in a system that utilizes many databases.

The simplest example for such a system is a micro service architecture. Each micro service in your system have its own database group that is running on the 
cluster, and you can define the number of nodes each database group will run on. This tend to be easier to manage, deploy and work with then having a separate
cluster per micro service. 

Other examples where you'll have multiple databases is for multi tenancy, where each tenant gets their own separate database. This make it very easy to deal
with tenant separation and you can adjust the number of nodes per tenant easily. This approach will also allow you to scale your system easily. As you have 
more tenants, you can just add more machines to the cluster and spread the load among them. 

That said, note that there is a certain cost for running each database instance, and that it is usually easier for RavenDB to have a single large database then
_many_ small ones. The general rule of thumb is that you shouldn't host more than a hundred or so active databases per machine.

#### Growing your cluster

RavenDB is using Raft as the underlying consensus protocol for managing the cluster. The [Raft Paper](https://raft.github.io/raft.pdf) is a truly impressive 
reading, mostly because the paper manage to make one of the hardest tasks in distributed programming _understandable_. I highly recommend reading it even if
you never intend to dig into the details of how RavenDB or other distributed system does their magic. 

The simplest way to explain how it works is that the cluster make decisions based on majority confirmation. That does great injustice to both the algorithm and
the paper, but it simplify things and allow us to reason about them without deviating _too_ much from what is really going on. Majority confirmation is defined
as having a particular value on `N/2+1` of the nodes, using integer math and assuming that `N` is the number of nodes. In other words, if your cluster size is
3, then a majority would be 2 and any value that was confirmed by any two nodes is considerred committed.

Table 7.1 shows the majority for several common cluster size. Note that even numbers have the same majority as the odd number preceding them. Because of that,
you'll typically have an odd number of nodes in your cluster. 

| Cluster size | Majority |
|--------------|----------|
|            2 |        2 |
|            3 |        2 |
|            4 |        3 |
|            5 |        3 |
|            7 |        4 |
|            9 |        5 |
|           15 |        8 |
|           21 |       11 |
|           51 |       26 |

Table: Majories for different sized cluster

This majority behavior has a few very interesting implications that you need to consider. First, and quite obvious, if we have a failure condition that 
took out more than half of our cluster, the cluster as a whole will not be able to make any decisions (even while individial database instances will operate
normally). In a cluster of 5 nodes, if there aren't any 3 nodes that can communicate with each other, there is no way to reach any decision. 

On the other hand, the more nodes there are in your cluster, the more network traffic you need to make to reach any decision. In patalogical cases, such as a 
cluster size of 51, you'll need to contact at least 26 servers to reach any decision. That is going to impose a high latency requirement on anything that 
the cluster is doing. 

In practice, you very rarely grow the cluster beyond seven members or so. The cost of doing that is usually too high. At that point, you will either setup 
multiple independent clusters or use a different method. The cluster size we mentioned so far is for voting members in the cluster, but the cluster doesn't
have to contain only voting members. We can just add nodes to the cluster as watchers.

These nodes do not take part in the majority calculations and are only there to watch what is going on in the cluster. As far as RavenDB is concerned, they are
full blown members in the cluster, they can be assigned databases and work to be done, but we aren't going to include them in the hot path of making decisions 
in the cluster.

Using this approach, you can decide that five nodes in your cluster are the voting members and all the rest are just watchers. This gives us the ability to 
make decisions with a majority of only three nodes while the actual size of the cluster can be much higher. Of course, if three of the the voting members of
the cluster are unable to talk to one another (because they are down, the network failed, etc) then the cluster as a whole will not be available. 

//TODO: When watchers / graphs are available, put some screen shots and guidelines here

Given that a cluster being unavailable doesn't usually impact ongoing database operations, the actual topology at scale is something that the operations team
need to consider. A single large cluster is usually easier to manage and the cluster can add as many watchers as you need to handle the load you are going to 
put on it. The databases themselves do not care what node they run on, whetever it is a voting member or just a watcher. And the cluster can manage database 
instance assignment across all machines with ease. 

#### Sharing data between clusters

A RavenDB cluster is a standalone unit, it manages itself and doesn't concern itself much with the outside world. There are situation, however, where you want to
have data shared between multiple clusters. This can happen if you want an offsite replica of all your data or if you have decided to have different clusters in 
each data center rather than a single cross data center cluster.

The offsite replica scenario is probably the easiest to grasp, so we'll start with it. In addition to the nodes you have in your cluster, you also want to have a 
full replica of a database that is outside the cluster and typically in a secondary data center. This can be done because you want the ability to spin up the 
system in another data center or because you want to setup a replica to do certain tasks (analytics, big queries, etc).

> **Replica isn't a backup**
>
> It is tempting to think that an offsite replica is also going to serve as the backup and not pay careful attention to the backup / restore portion of your
> database strategy. That would be a mistake. RavenDB's External Replication suppoert means that you get an offsite replica, but it doesn't provide good
> answers to a lot of backup scenarios. For example, protecting you from a "delete this collection" or "what was the state of the system in 9:03 AM last Friday?"
>
> An offsite replica gives you an offsite live copy of the data, which is very useful if you need to shift operations to a secondary data center, but it isn't
> going to allow you to skimp on backups. We cover backups (and restoring databases) in [Disaster Recovery and Plan B (for Backups)](#disaster-recovery).

This can be done quite easily, because RavenDB make a strong distinction between cluster and database group interactions. In this case, this allows us to define
a replication target that is not part of the cluster itself. We can do that in the database by going to `Tasks` and then `Manage Ongoing Tasks` and adding an 
`External Replication` task. You'll need to provide the url and database name for the destination, then save. Assuming you don't have an additional cluster to
test this on, you can specify one of your own nodes and create a _separate_ database to replicate to.

Ongoing tasks in general is quite interesting, and we'll discuss that at length in the next section. For now, we'll focus on what the External Replication feature
and what it means for us. Once we have finished configuring it, the cluster will assign on of the database group nodes to keep that replica up to date at all 
times.

It is important to remember that at the database level we treat it as just another destination, the cluster is _not_ managing it. That means that cluster 
level behaviors, such as defining conflict resolvers, failover for the client or index replication are _not_ send over. An External Replica is just that, 
external. You can configure both the source cluster and the destiation replica the same, of course, but there is nothing that forces you to do so. In fact, 
the other common reason why you will want to setup an External Replica is to have _different_ configuration.

A good example of this is when you want to have expensive indexes and only run them on a particular machine. Maybe you need to run expensive analytics or to do
certain work on a specific location. Using External Replication gives you the ability to control the flow of the data without also dictating how it is going to
be processed. 

I mentioned earlier that once the External Replication is setup, the cluster will assign it to one of the nodes. We haven't discussed it yet, but that is one of
the more important roles of the cluster, deciding what work goes where. 

### Ongoing tasks and work distribution

The cluster as we know it so far isn't really smart. It give us the ability to distribute configuration, such as what databases goes where, what indexes are 
defined, etc. But it doesn't _do_ much. This is actually quite far from the way things are. This is because we have focused specifically on the flow of data 
inside a database group, rather then the flow of _work_. 

What does it mean, to have work assigned to a database group? The simplest example is the one we just talked about, external replication. If we have three nodes 
in this database group, which node is going to update the External Replica? We don't want to have all three nodes do that, there is no real point in doing so, 
and it can cause unnecessary network traffic. Instead, the cluster will assign this work to one of the database instances, which will be responsible for keeping
the External Replica up to date. 

Another example is hourly incremental backup of our data as well as a full backup on a weekly basis. We don't want to have this backup run on all three nodes at 
the same time, leaving aside the fact that this will increase the load on the system across the board, we don't really have anything to do with triplicate 
backups of the same data. This is where the work assignment portion of the cluster come into play. 

Whenever there is a task for a database group to do, the cluster will decide which node will actually be responsible for it. That seems pretty easy, but there is
actually a bit more to the story. Just assigning work is easy, but the cluster is also watching the nodes and checking how healthy they are. If a node is down, 
then the cluster will reassign the work to another node for the duration. 

In the case of a backup, if the node that is responsible for the backup is down when the backup is scheduled, another will shoulder the load and make sure that 
you don't have any gaps in your backups. In the case of External Replication another node will transparently take over keeping the External Replica up to date 
with all the changes that happened in the database group. 

Another type of work in the cluster that we already talked about in this book is subscriptions. The cluster will divide all the subscriptions between the various
nodes in the database group and reassign them on failure without your code needing to change anything (or even typically even be aware of that). Other types of
work that we can define for the database group include ETL processes, which are covered in the next chapter. 

### The supervisor's promotion

The cluster is continously measuring the health of each node in the cluster, using a component known as the supervisor^[I always imagine it in bright neon green
tights and a cape, flying around spouting cliches as it keeps everything in order.]. The supervisor will detect any errors in the cluster, react to a failed node
by reasssigning work and in general keep everything running.

This is important enough to mention here and not in the Operations portion of the book because the Supervisor also gives you a wealth of information about the 
state of the different nodes and database instances that are running on them. In particular, one of its most important roles is the promotion and demotion of 
database instances.

Given that RavenDB is a distributed database and not a military organization, what does it means, promotions and demontions? Consider the case of a database 
group that has two database instances. We want to increase the number of replicas so we add a new node to the database group. The process is pretty easy, all we 
need to do is to click on `Manage group` in the Studio and add the new node. 

On large databases, the process of adding a new replica can take a while. During that process, the newly added node isn't really part of the group. It cannot 
take over work in the group (we certainly don't want its backup until it is completely caught up, for example) and we don't want to failover clients to it 
(since it doesn't have the full data yet). Because of this, when you add a new node to a database group, it isn't added as a full fledged member, instead, it is
added as a promotable instance. 

What does it means, promotable? It means that it cannot be assigned work for the database group, that it cannot be failed over to. In fact, it _is_ work for the
database group, since one of the full members is going to have to start pushing data into this new node until it is fully up to date. This goes on until the 
supervisor can confirm that the new node is caught up with the state of the database group and that it finished indexing all the data that we sent to it. At 
this point, the supervisor will promote the node into a full member in the database group, work will be assigned to it and clients will consider as a 
failover target.

Conversely, when the supervisor detects that a node is down, it will demote it from a full member status into a promotable. This is done so when the node is 
coming back up, it will not start sending out outdated information. Instead, one of the other members in the database group will connect to it and update the 
recovered node until it is fully up to date, at which point the supervisor will promote it to full member again.

Using demotions and promotions as needed, the supervisor is able to communicate to clients (and the rest of the cluster) which are the authoritative nodes for a 
particular database group. Note that a failure doesn't have to be just a down node, something as simple as running out of disk space can also cause a database
on a node to fail and demote a node from being a full member. 

You won't generally need to handle such demotions and promotions yourself, or even really be aware of them. RavenDB is taking great pains to make the experience
of using the cluster as seamsless as possible, but you do need to understand _how_ this is happening so you can take advantage of RavenDB's behavior.


### Summary


We looked at some architectual considerations for building a RavenDB cluster and how to scale it to a large number of nodes and how we can share data 
between different clusters. Finally, we peeked into the management of the cluster itself, with the supervisor assigning work to the various nodes and constantly
monitoring their health, promoting and demoting nodes as needed.

This was anything but a trivial chapter, it is packed with new ideas and it tries to cover the whole of running a distributed cluster in a single chapter. I 
tried to make sure that the topic in this chapter follow logically one another as we tour through the distributed portion of RavenDB, but there is quite a lot
to go through. In particular, I haven't talked about operations, monitoring, statistics etc at all. I'm leaving all of that a chapter dedicated just for that,
[Statistics, Monitoring and Insight](#monitoring). We'll also talk a abit more about the implementation details that I intentionally skipped here, because the
operations team need to understand what is going on exactly, while this chapter was about the overall concept.




* External replication
* ETL
* Watchers
* Cross data center clusters
* Geo distribution with ravendb
* Clustering topologies ?? 
