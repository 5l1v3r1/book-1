
## Map-Reduce in RavenDB

Map-Reduce is an old term. It came from Lisp and was used as early as the 1960s. For a long time, it was primarily known only for
functional language afficandos and very rarely if ever seen outside their circles. In 2004, the Google paper
[MapReduce: Simplified Data Processing on Large Clusters](https://research.google.com/archive/mapreduce-osdi04.pdf) was released 
and Map-Reduce was instantly a big hit in the distributed programming circles. Everyone had to have a distributed map-reduce
implemention.

RavenDB is a distributed database, and it uses Map-Reduce. However, it does not do so in the context of distributed computing. 
Instead, RavenDB is using Map-Reduce for aggregation of data on each node independently. If you are used to Map-Reduce jobs that
are running on large clusters and processing terrabytes of data, that might look very strange. 
But RavenDB isn't using Map-Reduce to break apart large computation across different machines, instead, it uses Map-Reduce to 
break apart computation across _time_.

It is usually easier to explain with an example, so let's jump into that.

### Executing simple aggregations

Listing 11.1 shows a simple aggregation query, giving us the total number of orders and items purchased by a particular company.

```{caption="A simple aggregation query in RavenDB" .sql}
from Orders as o
group by o.Company
where o.Company  = 'companies/1-A'
select count() as NumberOfOrders, 
	   sum(o.Lines[].Quantity) as ItemsPurchased, 
	   o.Company
```

The query in Listing 11.1 should be very familiar to anyone who has used SQL before. Now, let us analyze what RavenDB must do in
order to answer this kind of query:

1. Find all the `Orders` documents where the `Company` field is set to `'companies/1-A'`.
2. Iterate over all of those, count their number and sum the number of line items in each.
3. Return the results to the client.

This seems quite straightforward, but it has a couple of issues. In particular, the first two steps. The sample data we are using
has just over a thousand documents in the database. There isn't much that we can do to make any query expensive over that dataset.
However, with real world datasets, we will typically deal with collections that contain hundreds of thousands to many milliions 
of documents. 

Consider the first step, finding all the `Orders` documents with the `Company` field set to a value. If I have a few millions 
documents to scan, that alone can be quite expensive, both in I/O (to read the data from disk) and computation (to check equality
so often). This protion is quite obviously something that we can optimize with an index. It is the next portion that is much 
harder to work with.

If a company has a _lot_ of orders, then the process of iterating over each of these orders can be extraordinarily expensive. 
Consider the case where you want to show the number of orders and the number of items purchased on the company page. This is a 
small amount of information which can tell how important this particular customer is. 

However, if we would need to repeat the second step each time that we run the query, that can get expensive very quickly. What is 
worse is that the more important the company, the more orders and items this particular customer purchased from us, the slower 
things will become. This method won't work, it is punishing success, and that isn't something we want.

Typically you will not run such queries directly, because of their cost and time to run.
Instead, you'll write a job that will run these queries at idle times and cache the results. Then you only have to deal with cache invalidation, making sure that this job is never run under load, explaining to users that the results are delayed and...

It's complex. All we wanted was to put a couple of numbers on a page, and suddenly you need to deploy a background job, monitor
its cost and execution and wait until the daily or weekly run to get updated numbers. Such high cost of aggregation will usually
cause such features to be dropped.

And even if you don't have enough data to require such measures, aggregation queries are still typically very costly for the 
database. Enough so that they are used sparingly in most systems.

RavenDB's aggregation doesn't work like this. 

#### The gory details of aggregation in RavenDB

Instead of gather all the data and then aggregating it all in place, RavenDB uses Map-Reduce to break apart the aggregation 
computation into discrete steps, the Map and the Reduce. Let's look at how RavenDB is _actually_ processing the query in 
Listing 11.1. Run the query in Listing 11.1 and then click on the index button in the results, as shown in Figure 11.1.

We are going to get deep into how RavenDB is handling aggregations, you'll not typically need to know that level of details, 
and can feel free to just skip this section. I'm including this here because it is important to understand the implication of
how things work, that aggregation queries in RavenDB are _very_ cheap.

![Getting to the index details on the query page](./Ch11/img01.png)

You can already see that an aggregation query in RavenDB is also using an index. In addition to the usual options, such as the 
index terms and performance statistics, there is also the Map-Reduce Visualizer, which we'll look in more detail later. For now
click on the `View Index` option, which should open the index details page, shown in Figure 11.2.

![A Map-Reduce index aggregation `Orders` by `Company`](./Ch11/img02.png)

Figure 11.2 shows the structure of the index. Operating over `Orders`, it is grouping by the `Company` and getting then 
aggregating over them. One thing to note here is that there is no mention anywhere of `companies/1-A`. Even though the query in
Listing 11.1 mentioned it, the index is not operating on that particular value, but on the generic concept of aggregating by
the `Company` field.

In other words, as usual, RavenDB has looked at the query and generalized the operation to answer any such question using any 
`Company`. But what about the aggregation?

Aggregation is actually handled via two separate actions, `map` and `reduce`. The first stage is running a `map` operation on
each of the documents, grabbing just `Company`, `sum(Lines[].Quantity)` and `Count = 1` from each of the `Orders` documents, the 
second stage is to group them all by the `Company` and run the `reduce` operation to get the final result by each of the 
`Company` field values.

If this doesn't make sense to you, don't worry, RavenDB contains a few tools specifically to help you understand how the 
`map-reduce` process work.  

In the `Indexes` tab, go to `Map-Reduce Visualizer` and then select the `Auto/Orders/ByCountAndLines[].QuantityReducedByCompany`
index from the drop down. Then search for the following document ids: `orders/396-A` and `orders/445-A`. The result should look
similar to Figure 11.3.

![The Map-Reduce Visualizer allows us to inspect the internal structure of the index](./Ch11/img03.png)

Clicking on the `{"Company":"companies/1-A"}` rectangle in Figure 11.3 will give us more details about that particular value, as
you can see in Figure 11.4.

![A single reduce result and the total reduced value for a map-reduce index](./Ch11/img04.png)

With the details in Figures 11.3 and 11.4, we can now see exactly what we mean when we talk about `map` output and the resulting
aggregation. The documents we selected (`orders/396-A` and `orders/445-A`) both belong to `companies/1-A`, and we can see that 
for `orders/396-A` the `map` output was `{"Company": "companies/1-A","Count": 1,"Lines[].Quantity": 38}`. Indeed, if we'll go 
and inspect the document, we'll see three line itmes, with quantities of 15, 21 and 2, totalling 38. For `orders/445-A` we can
see that the total quantity is 20, with a single line item. 

This is interesting, but what is even more interesting is the aggregated value. For `companies/1-A`, you can see the aggrated 
values of a total of 6 orders for this company with a final quantity of 174 items ordered. Clicking on the aggregation summary
will take us even further down, into the inidivual page entries, as you can see in Figure 11.5.

![Individual mapped entries for `companies/1-A`](./Ch11/img05.png)

In Figure 11.5 you can see all the details for each of the entries for `companies/1-A`. This is the lowest level we need to 
inspect as well as the reason why RavenDB implements aggregation in such a manner. I already spent several pages just explaining
what is going on, why is the aggregation implementation for RavenDB so _complex_?

The reason is quite simple, actually. The reason for the complexity is that we don't run aggregation queries once, instead, we
compute the aggregation result once, and then we store it. When we issued the query in Listing 11.1 we queried only for results
for `companies/1-A`, but the index that the query optimizer generated for us applies to all companies.

In fact, if we would now run the same query, but for `companies/2-A`, we'll be able to reuse the same index, and in fact, we'll 
have to do very little work. The query will use the index and fetch the already pre-computed results for `companies/2-A` and
won't have to actually perform any aggregation whatsoever. All the work has already been done. 

As great as that is, you might be asking yourself why this level of complexity. After all, surely we could have done the same
without so many moving parts, right? This is correct, but there is one additional item that we need to consider. How are we 
going to handle updates?

The map-reduce indexes in RavenDB aren't simply a cache of the already computed results, instead, we store the data in such a 
way that make it cheap to also update the results. Consider what will happen inside RavenDB when a new order comes in. We'll run
the `map` portion of the index, getting the `Company`, `sum(Lines[].Quantity)` and `Count = 1` from the newly created document.

The easiest way to visualize that is to just add another row to Figure 11.5. At this point, RavenDB can then just aggregate the
new results alongside the already existing result and get to the final tally. In other words, the complexity here exists in order
to allow RavenDB to efficently update map-reduce results when documents are created or updated.

This works great when we have a small number of items to aggregate, such in the case with `companies/1-A`, but what happens when
the number of items grow? Let's increase the number of documents that we are aggregating by a hundred fold and see where that 
takes us. Go do `Documents` and then `Patch` and run the update script in Listing 11.2.

```{caption="Increase the number of Orders documents by a hundred" .sql}
from Orders 
update {
    for(var i = 0; i < 100; i++ ){
        put("orders/", this);
    }
}
```

After running this script, we should have about 83,000 Orders documents in the database. Where previously we had 6 entries for
`companies/1-A`, we now have 600. Let's look at how that work. Go back to the Map-Reduce Visualizer and select the
`Auto/Orders/ByCountAndLines[].QuantityReducedByCompany` index and then add the `orders/396-A` and `orders/445-A` documents.
The result is shown in Figure 11.6

![Showing 600 mapped entries for `companies/1-A`](./Ch11/img06.png)

This is very similar to how it looked before, and indeed, the structure of the data is exactly the same. This is because the
amount of entires for `companies/1-A` is still quite small. Let's select another pair of documents, this time belonging to
`companies/77-A` and see what kind of structure we have there. Add `orders/77-A` and `orders/146-A` to the visualizer and 
see what the result looks like. You can see the results in Figure 11.7.

![Map-reduce works as a tree once we have enough entries for a particular value](./Ch11/img07.png)

On the left in Figure 11.7 you can see that unlike for `companies/1-A` in Figure 11.6, were we had a flat list, Figure 11.7
shows a tree structure. Indeed, once we are past a certain size, RavenDB will start processing map-reduce entries in a tree
like fashion.

Consider an update to `orders/77-A` and how RavenDB will apply it. First, we'll run the `map` on the updated document, giving
us the map entry to write to `#1028`. Then, we'll run the reduce on that page, giving us the final tally for this page. We'll
then recurse upward, toward `#1437`, where we'll also run the reduce.

> **Using aggregation in RavenDB**
>
> Aggregation operations in RavenDB are cheap, both to compute and to query. This is in stark contrast to the usual behavior of 
> aggregation queries in other databases. The more data you have to go through, the more efficently will RavenDB be able to 
> actually process and aggregate it. 
>
> While typically you'll run aggregation as a daily or weekly report (usually during off hours) and store these results for use 
> later, RavenDB allow you to just query the aggregated data and RavenDB will provide the answers you need as well as keep 
> everything up to date. 

The end result is that an update operation will take us a single `map` invocation and two `reduce` calls to update the final
result for `companies/77-A`. In other words, if we have a lot of data we have for a particular key, RavenDB will start segmenting
this data and apply aggregation operations in such a way to reduce the number of operations required to a minimum. This ensures
that not only are we able to answer queired efficently, but we are also able to update the map-reduce results with very little 
costs.

### Defining your own map/reduce indexes

In Listing 11.1 we have seen how we can query using `group by` and RavenDB will generate a map-reduce index for us behind the 
scenes. This works quite nicely, but there is only so much that you can do with a dynamic query before the query optimizer will
give up.

The query optimizer can do quite a lot, but in order to make sure that it is predictable, it is currently limited to recognizing 
and being able to generate map-reduce indexes from a fairly small list of predictable patterns. For more complex things, you'll 
need to create your own map-reduce index. Luckily, this is quite easy to do.

Let's say that we want to get the total number of sales and the total amount we made on each product. We cannot express this as 
a simple query, so we'll need to create an map-reduce index of our own for that. Go to the `Indexes` page and click on 
`New Index`. Name the index `Products/Sales` then click on `Add Reduction` button. Listing 11.3 has the contents of the `Map` 
and `Reduce` fields. After you have done filling these in, click on `Save` to create the new index.

```{caption="Compute total number of sales and revenue per product" .cs}
// map 
from o in docs.Orders
from l in o.Lines
select new
{
    l.Product,
    l.Quantity,
    Total = (l.Quantity * l.PricePerUnit) * (1 - l.Discount)
}


// reduce
from r in results
group r by r.Product into g
select new
{
    Product = g.Key,
    Quantity = g.Sum(x=>x.Quantity),
    Total = g.Sum(x=>x.Total)
}
```

Before we get to investigating how this index work, let's talk about what it does. The map portion runs over all the orders and 
all the line items for each order. Then, for each of the line items, we output an entry with the quantity sold and the amount
of money we made on this product. The reduce will group all these results based on the `Product` field and then sum up all the 
final numbers for the `Total` and `Quantity` sold. 

Now we can really see the separate operations of `map` and `reduce`, it might make more sense how they are truly two separate
operations. It is also important to understand that we aren't actually running the `map` or the `reduce` on the full results 
all the time. Instead, we break it apart internally and apply them to portions of the data each time. This leads to several 
important restrictions on map-reduce indexes:

* Both the `map` and `reduce` functions must be _pure_ functions. In other words, they should have no external input, and 
  calling them with the same input must always return the same output. In particular, usage of `Random` or `DateTime.Now` and
  similar calls is not allowed. 
* The output of the `map` is fed into the `reduce`, this is quite obvious, but what may not be obvious is that the output of 
  the `reduce` is _also_ fed into the `reduce`, recursively. In particular, you should make no assumptions in the `reduce` about
  the number of calls or the amount of data that you have to process in each invocation to `reduce`. 
* The output of the `reduce` must match the output of the `map` (because both are being fed back into the `reduce`, they must
  have the same structure). RavenDB will error if you have a different shape for each of the functions.

Because of these restrictions, RavenDB can apply the `map` and `reduce` functions in an incremental fashion and generate the 
results that we have already seen. This is the key for RavenDB's ability to compute aggregation cheaply over time.

We can now run the query in Listing 11.4 to find out the top grossing products:

```{caption="Finding the top grossing products across all orders" .sql}
from index 'Products/Sales'
order by Total as double desc
```

You'll note that the results of this query are effectively instantaneous, even though we have tens of thousands of records, 
RavenDB only need to look through 77 precomputed results. We can also sort by `Quantity` to find the most popular products. 

Go in to the `Map-Reduce Visualizer` and select the `Products/Sales` index and then enter `orders/6-A` and `orders/10-A` to
see the internal structure of the map-reduce index, as shown in Figure 11.8.

![Internal sturcutre of the `Products/Sales` index in the map-reduce visualizer](./Ch11/img08.png)

As you can see in Figure 11.8, all of the entries we have here are big enough to require a tree structure. If we'll look at
`products/39-A` you can see that this is holding just over 80,000 entries, with Figure 11.9 zooming into a single page inside
that tree.

![A single page holding over 25,000 map entries, due to high compression rate](./Ch11/img09.png)

Page `#578` holds over 25,000 entries for this product. How can a single page hold so many? The answer is that RavenDB will 
apply compression to the mapped entries. Since they are mostly very similar, they have a very high compression rate, allowing
us to pack a _lot_ of entries in a very small amount of space. This also adds to performance of the updating entries in the tree,
since we don't have to do as much work and the depth of the tree is much smaller.

This is about as low level as we'll get when discussing the map-reduce implementation. You know have sufficent information to 
have a good feel about the relative costs of using aggregation in general and map-reduce in particular in RavenDB. With that 
knowledge under your belt, let's explore some of the more interesting things that we can do with map-reduce indexes in RavenDB.


#### Multi map reduce

#### Recursive map-reduce

### Disadvantages of Map-Reduce indexes in RavenDB