
# Map/Reduce Indexes

Map/Reduce is a programming concept that was made popular after Google started talking about doing their Page Rank algorithm using it. The idea isn't new, but the scale in which Google applied it was.

You've probably heard about Map/Reduce in the past, at least in passing. But my experience have shown that this is a subject most people have marked with "There Be Dragons" in their head and consider this something that is only relevant for Big Data or special use cases.

The terminology used is a strong part of that. The terms Map/Reduce actually refer to function name in Lisp and to a common usage pattern in functional languages. All of which result in even technical people assuming that this is nerd stuff that they don't need to care about.

Another way to talk about Map/Reduce is to talk about aggregation. In particular, aggregation of a data set to a smaller summary. I assume that you are familiar with SQL, so you shouldn't have a problem understanding the SQL statement in Listing 10.1.

```{caption="{Aggregation using GROUP BY in SQL}" .sql}
SELECT o.CategoryId, COUNT(*) TotalPurchases 
FROM Products p
GROUP BY o.CategoryId
```
If you aren't familiar with SQL, don't worry. What this statement is doing is to get the total amount of all purchases for each customer. Nothing surprising or scary about this, right?

The statement in Listing 10.1 is something that most developers encounter on a regular basis. We are just doing a bit of aggregation, nothing as fancy or hard as Map/Reduce. But the interesting thing about Map/Reduce is that it is best when it is doing exactly what Listing 10.1 is doing. Aggregation over a data source to get the summary. In Listing 10.2, the SQL statement was translated into a Linq expression.

```{caption="{Aggregation using GroupBy in Linq}" .cs}
from o in products
group o by o.Category into g
select new 
{
	Category = g.Key,
	Count = g.Count()
}
```

So far, it is perfectly obvious what is going on here. Let us take Listing 10.2 and turn that into a Map/Reduce definition. You can see that in Listing 10.3.

```{caption="{Aggregation using Map/Reduce}" .cs}
var mapResults = // map operation
	from o in products
	select new 
	{
		o.Category,
		Count = 1
	};

var reduceResults = // reduce operation
	from result in mapResults
	group result by result.Category into g
	select new
	{
		Category = g.Key,
		Count = g.Sum(x=>x.Count)
	};
```

Listing 10.3 isn't actually all that different from Listing 10.2. Instead of doing everything in a single expression, we have simply split the operation into two expressions. One that just gather the details, and another that will perform the actual aggregation. But even though Listing 10.2 and Listing 10.3 are very similar, Listing 10.3 is the more complex one. So why go with that approach?

The reason for splitting an aggregation operation like that is very simple, it allow us to break apart the actual execution into distinct parts, that can run independently from one another, and _that_ in turn, allow us to do some really interesting things with aggregation computation.

## The magic of Map/Reduce

So Map/Reduce is basically a complex way to do a Group By, nothing more. Why add this complexity? Because having the steps of the aggregation process split into an explicit map and reduce steps allow us to treat each of them in isolation. And that lead to an interesting observation. If we can treat each step in isolation, that means that we aren't required to run them sequentially and at the same time.

The common case for running Map/Reduce is that it allows us to easily parallelize the operation. And we can do that on multiple machines to quickly aggregate very large amount of information. That was the original usage of Google's Map/Reduce framework.

Let us see how we can process a Map/Reduce operation on the data showing in Table 10.1.

Id 		  	Name 		 					Category
--------- 	------     						---------
products/37 Gravad lax  					categories/8
products/38 CÃ´te de Blaye   				categories/1
products/39 Chartreuse verte   				categories/1
products/40 Boston Crab Meat   				categories/8
products/41 Jack's New England Clam Chowder categories/8

: Sample data for Map/Reduce test run 

First, we need to run the map. We want to see how Map/Reduce allow us to work with large amount of data, so we'll limit ourselves to 3 rows per run. So we run the map twice, on the first three rows and then on the last two rows. The results are shown in Table 10.2 and table 10.3.

Category 		Count
------------ 	------
categories/8	1
categories/1 	1
categories/1 	1
: Running the Map operation on the first three rows of Table 10.1

Category 		Count
------------ 	------
categories/8 	1
categories/8 	1
: Running the Map operation on the last two rows of Table 10.1

We have run the Map operation, now it is time to run the Reduce operation. The results are shown in Table 10.4 and Table 10.5.

Category 		Count
------------ 	------
categories/8	1
categories/1 	2
: Running the Reduce operation on Table 10.2

Category 		Count
------------ 	------
categories/8 	2
: Running the Reduce operation on Table 10.3

We are almost done, but we still have two separate tables, and no final results. In order to get those, we now need to run the Reduce again. But this time (and this is crucial for understanding), we are going to run the Reduce on its own output. 

Listing 10.4 shows how this works, in terms of executing the Map and Reduce functions.

```{caption="{Showing the Map/Reduce calls and iterative reduction}" .cs}
var rawData = GetDataFromTable10_1();

var map1 = Map(rawData.Take(3));
var map2 = Map(rawData.Skip(3));

var reduce1 = Reduce(map1);
var reduce2 = Reduce(map2);

var final = Reduce(reduce1.Concat(recuce2));
```

Again, this all seems to just create more complexity. 