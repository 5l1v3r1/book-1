
# Batch Processing with Subscriptions

RavenDB needs to handle some very different usecases in its day to day operations. On the one hand, we have transaction oriented processing, which typically
touch a very small number of documents as part of processing a single request. And on the other hand, we have batch processes, which usually operate on 
very large amount of data. Impleenting those kind of processes with the OLTP mode is possible, but it isn't really easy to do.

RavenDB supports a dedicated batch processing mode, using the notion of subscriptions. A subscription is simple a way to register a certain criteria on the
database and have the database send us all the documents that match this criteria. For example, "gimme all the support calls". So far, this sounds very much
like the streaming feature, which we covered in the previous chapter. However, subscriptions are _meant_ for batch processing. Instead of just getting all 
the data that match the criteria, the idea is that we'll keep the subscription open forever.

The first stage in the subscription is to send you all the existing data in the data that matches your query. Depending on your data size, this can take a 
while. This is done in batches, and RavenDB will only proceed to send the next batch when the current one has already been acknowledge as successuflly
processed. 

The second stage in the lifetime of a subscription is when it is done going through all the data that is already in the database. This is where things 
start to get really interesting. At this point, the subscription _isn't_ done. Instead, it is kept alive and wait around for a doucment that matches
it criteria to arrive. When that happens, the server will immediately send that document to the subscription. In other words, not only did we get batch
processing, but in many cases, we have _live_ batch processes. 

Instead of waiting for a nightly process to run, you can keep the subscription open and handle changes as they come in. This keep you from having to do
polling, remember the last items that you read, etc. RavenDB's Subscription also include error handling, recovery and retries, on the fly updates, etc. 

Working with subscriptions is divided into two distinct operations. First we need to create a thesubscription and then open it. Subscriptions aren't like
queries, they aren't ephemeral. A subscription is a persistent state of a particular business process. It indicates what documents that particular subscription
have processed, the criteria it is using, etc. 

In Listing 4.32 you can see how we create a subscription to process all the `Customer` documents in the database. 

```{caption="Creating a subscription to process customers" .cs}
var options = new SubscriptionCreationOptions<Customer>();
string subsId = store.Subscriptions.Create(options);
```

The result of the `Create` call in Listing 5.1 is a subscription id, which we can pass to the `Open` call. Listing 5.2 shows how we open the subscription and register our batch handling routine. 

```{caption="Opening and using a subscription to process customers" .cs}
var options = new SubscriptionConnectionOptions(subsId);
using(var subscription = store.Subscriptions.Open<Customer>(options)))
{
	// wait until the subscription is done
	// typically, a subscription lasts for very long time
	await subscription.Run(batch => /* redacted */ );
}
```

There isn't much in Listing 5.2. We open a subscription using the previous gotten `subsId`, then we `Run` the subscription, I redacted the actual batch processing
for now, we'll see what it is in the nexte section. After calling `Run` we wait on the returned task, why do we do that?
In general, a subscription will live for a very long time, typically, the lifetime of the process that is running it. In fact, typically you'll have a process
dedicated just to running subscriptions (or even a process per subscription). What we are doing in Listing 5.2 is basically wait until the subscription will exit.

However, there are very few reasons that a subscription _will_ exit. Problems with the network will simply cause it to retry or failover to another node, for
example. And if it processed all the data that the ddatabase had to offer, it will simply sit there and wait until it has something more to give it. However,
there are a few cases where the subscirption will exit. First, of course, if you want to close it by disposing the subscription. It is safe to dispose the 
subscription while it is running, and that is how you typically do orderly shutdown of a subscription. 

Next, we might have been kick off the subscription for
some reason. The admin may have deleted the subscription or database, the credentials we use are invalid, or the subscription connection has been taken over
by another client. As you can see, typically the `Run` method will not just return (only if you manually disposed the subscription), it will typically throw 
when there is no way for it to recover. We'll cover more on this in the subpscription deployment section later on in this chapter. 

The actual batch processing of the documents is done by the lambda that is passed to the `Run` call  on a background thread. In Listing 4.33 we used the `subsId` 
from Listing 4.32. This is typical to how we are using subscriptions. The subscription id should be persistent and survive restat of the process or machine, 
becuase it is used to represent the state of the subscription and what doucments have already been processed by that particular subscription.

You can set your own subscription id during the `Create` call, which give you well known subscription name to use, or you can ask RavenDB to choose one for you, 
as we have done in Listing 4.32. Note that even if you use a hard coded subscription id, it still needs to be created before you can call `Open` on it. 
Why do we have all those moving parts? We have the creation of the subscription, open it, run it and wait on the resulting task and we haven't even gotten to
the part where we are actually do something using it. 

The reason for this is that subscriptions are very long lived processes, which are resilient to failure. Once a subscription is created, a client will open it 
and then keep a connection open to the server, getting fed all the documents that match the subscription criteria. This is true for all existing data in the
database. 

Once we have gone over all the documents currently in the database, the subscription will go to sleep, but will remain connected to the server. Whenever a new
or updated document will match the subscription criteria, it will be sent again. Errors during this process, either in the network, the server or the client are
tolerated and recoverable. Subscriptions will ensure that a client will recieve each matching document at least once^[Although errors may cause you to recieve 
the same document multiple times, you are guranteed to never miss a document]. 

> **Subscription in a cluster**
>
> The subscription will connect to _a_ server in the cluster, which may redirect the subscription to a more suitable server for that particular subscription. 
> Once the subscription found the appropriate server, it will open the subscription on that server and start getting documents from it. A failure of the client 
> will result in a retry (either from the same client or possibly another one that was waiting to take over). A failure of the server will cause the client to 
> transparently switch over to another server. 
>
> The entire process is highly available on both client and server. The idea is that once you setup your subscriptions, you just need to make sure that the
> processes that open and process the subscription is running, and the entire system will hum along, automatically recovering from any failures along the way.

Typically, on a subscription that already processed all existing documents in the database, the lag time between a new document coming in and the subscription 
recieving it is a few milliseconds. Under load, when there are many such documents, we'll batch documents and send them to the client for processing
as fast as it can process them. The entire model of subscription is based on the notion of batch processing. While it is true that subscriptions can remain
up constantly and get fed all the changes in the database as they come, that doesn't have to be the case. If a subscription is not opened, it isn't going to 
miss anything. Once it has been opened, it will get all the documents that have changed when it was gone.

This allow you to build business processes that can either run continiously or to run them at off peak times in your system. Your code doesn't change, nor does
it matter to RavenDB (a subscription that isn't opened consume no resources). In fact, a good administrator will know that it can reduce the system load by 
shutting down subscriptions that are handling non time critical information with the knowledge that once the load has passed, starting them up again will allow
them to catch from there last processed batch.

This is done by having the client acknowledge to the server that it successfully completed a batch once it has done so, at which point the next batch
will be sent. It is the process of acknowledging the processing of the batch that make the process reliable. Until the client confirmed reciept of the batch,
we'll not move forward and send the next one. 

Okay, that is about as much as we can talk about subscriptions without actually showing what they are _doing_. Let us go to handle the actual document processing.

## Processing a batch of documents in a subscriptions.

We previously seen the `Run` method, in Listing 5.2. But we haven't seen yet what is actually going on there. The `Run` method is simply taking a lambda that will
go over the batch of documents. Listing 5.3 shows the code to handle the subscription that we redacted from Listing 5.2. 

```{caption="Processing customers via subscription" .cs}
await subscription.Run( batch =>
{
	foreach(var item in batch.Items)
	{
		Customer customer = item.Result;
		// do something with this customer
	}
});
```

After all this build up, the actual code in Listing 5.3 is pretty boring. The lambda we sent get a batch instance, which has a list of `Items` that are contained
in this batch. And on each of those items, we have a `Result` property that contains the actual document instance that we were sent from the server. 
This code will first get batches of all the customers in the database. Once we have gone through all the customer documents, this subscription will wait, and 
whenever a new customer comes in, or an existing customer is modifed, we'll have a new batch with that document. If there are a lot of writes, we might get 
batches that will contain several documents that were changed in the time it took us to complete the last batch.

What can we do with this? Well, quite a lot, as it turns out. We can use this to run all sort of business processes. For example, we may want to check if this
customer have a valid address, and if so, record the GPS coordinates so we can run spatial queries on it. Because of the way subscriptions work, we get a full
batch of documents from the server, and we can run heavy processing on them, we aren't limited by the data streaming over the network, unlike streaming, we won't 
run out of time here. As long as the client remains connected, the server will be happy to keep waiting for batch to complete. Note that the server will ping
the client every now and then to see what its state is, to detect client disconnection at the network level. If that is detected, the connection on the server
will be aborted and all resources will be released.

> **Subscriptions are background tasks**
>
> It may be obvious, but I wanted to state this explicitly, subscriptions are background tasks for the server. There is no requirement that a subscription 
> will be opened at any given point in time, and a subscription that wasn't opened will simply get all the documents that it needs to since the last 
> acknowledged batch.
> 
> That means that if a document was modified multiple times, it is possible that the subscription will only be called upon it once. See the section about
> Versioned Subscriptions if you care about this scenario.

One of the things we can do here is to open a new session, modify the document we got from the subscription, `Store` the document and call `SaveChanges` on it, 
right from the subscription lambda itself. But note that doing so will also typically put that document right back on the path to be called again with this 
subscription, so you need to be aware of that and protect against infinite loops like that. There are also a few other subtle issues that we need to handle 
with regards to running in a cluster and failover, we'll discuss those issues later in this chapter. 

## Conditional Subscriptions

Subscriptions so far are useful, but not really something to get excited about. But the fun part starts now. Subscirptions aren't limited to just fetching all
the documents in a particular collection. We can do much better than this. Let us say that we want to send a survey for all customers where we had a complex
support call. The first step for that is to created a subscription using the code in Listing 5.4.

```{caption="Creating a subscription for complex calls" .cs}
var options = new SubscriptionCreationOptions<SupportCall>(
		call => 
			call.Comments.Count > 25 && 
			call.Votes > 10 		 && 
			call.Survey == false
	);
string subsId = store.Subscriptions.Create(options);
```

We are registering for subscriptions on support calls that have more than 10 votes and over 25 comments, and we add a flag to denote that we already sent the
survey. It is important to note that this filtering is happening on the _server side_, not on the client. Internally we'll transform the conditional into a
JavaScript expression and send it to the server, to be evaluated on each doucment in turn. Any matching document will be sent for the client for processing.
Of course, this is just part of the work, we still need to handle the subscription itself. This is done in Listing 5.5.

```{caption="Taking surveys of complex calls" .cs}
await subscription.Run(batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCall call = item.Document;
		
		var age = DateTime.Today - call.Started;
		if( age > DateTime.FromDays(14))
			return; // no need to send survey for old stuff

		// DON'T open a session from the store directly
		// using(var session = store.OpenSession())

		// INSTEAD, open a session from the batch
		using(var session = batch.OpenSession())
		{
			var customer = session.Load<Customer>(
				call.CustomerId);

			call.Survey = true;		

			session.Store(call, item.Etag, item.Id);

			try
			{
				session.SaveChanges();
			}
			catch(ConcurrenyException)
			{
				// will be retried by the subscription
				return;
			}	

			SendSurveyEmailTo(customer, call);
		}
	}

});
```

A lot of stuff is going on in Listing 5.5, including stuff that we need to explain _not_ to do. Even though we removed the code for actually opening the 
subscription (this is identical to Listing 5.4) there is still a lot going on. For each item in the batch, we'll create a new session load the customer for
this support call, then we mark the call as having sent the survey. Then we call `Store` and pass it not just the instance that we got from the subscription, 
but also the etag and id for this document. This ensures that when we call `SaveChanges`, if the document has changed in the meantime on the server side, we'll 
get an error.

> **Don't use the `store.OpenSession` in batch processing**
>
> The code in Listing 5.5 calls out the use of `store.OpenSession` as a bad idea. Why is that? This is where I need to skip ahead a bit and explain some 
> concepts that we haven't seen yet. When running in a cluster, RavenDB will divide the work between the various nodes in a database. That means that a
> subscription may run on node B while the cluster as a whole will consider node A as the preferred node to write data to. However, since the subscription
> is being served from node B, it means that the etag that we get from the subscription is also local to node B.
> 
> Remember our discussion on etags in [Chapter 3](#modeling), an etag is local to the database where it was created. When using the `store.OpenSession`
> the returned session will use the preferred node on the cluster to write to. That means that when you call `SaveChanges`, even though you read the
> value from node B, the `SaveChanges` call will go to node A. Since you are using optimistic concurrency and using the etag from node B, you are pretty
> much guranteed to always hit a concurrency exception. 
>
> In order to avoid that, you should use `batch.OpenSession` (or `batch.OpenAsyncSession`) to create the session. This will ensure that the session you have
> created will operate against the same node that you are reading from and thus allow us to use optimistic concurrency properly. 

In Listing 5.5, the concurrency exception is expected error, we can just ignore it and skip processing this document. There is a bit of a 
trickery involved here. Because the document have changed, the subscription will get it again anyway, so we'll skip sending an email about this call now, but 
we'll be sending the emaial later, when we run into the support call again.

Finally, we send the actual email. Note that in real production code, there is also the need to decide what to do if sending the email failed. In this case, 
the code is assuming that it cannot fail, and favor skipping sending the email rather then sending it twice. Typical mail systems have options to ignore 
duplicate emails in a certain time period, which is probably how you would solve this in production.

Instead of using explicit concurrency handling, you can also write the code in Listing 5.5 using a `Patch` command, as you can see in Listing 5.6.

```{caption="Taking surveys of complex calls, using patches" .cs}
await subscription.Run( batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCall call = item.Document;
		var age = DateTime.Today - call.Started;
		if( age > DateTime.FromDays(14))
			return; // no need to send survey for old stuff

		using(var session = batch.OpenSession())
		{
			var customer = session.Load<Customer>(
				call.CustomerId);

			session.Advanced.Patch<SupportCall, bool>(
				result.Id,
				c => c.Survey,
				true);

			SendSurveyEmailTo(customer, call);

			session.SaveChanges();
		}
	}
});
```

In Listing 5.6 we are doing pretty much the same thing we did in Listing 5.5. The difference is that we are using a `Patch` command to do so, 
which saves us from having to check for concurrency violations. Even if the document have changed between the time the server has sent it to us
we will only set the `Survey` field on it. In Listing 5.6 we are also sending the survey email _before_ we set the `Survey` flag, so a failure
to send the email will be throw all the way to the calling code, which will typically retry the subscription. This is different from the code in
Listing 5.5, where we first set the flag and then send the email.

The main difference here is in what happens in the case of an error being raised when sending the survey email. In Listing 5.5, we have already
set the flag and sent it to the server, so the error will mean that we didn't send the email. The subscription will retry, of course, but the
document was already changed and will be filtered from us. In Listing 5.5, if there was an error in sending email, the email will not be sent.

> **Subscribing to documents on the database we are writing to**
> 
> There are a few things to rembmer if you are using the subscription to write back to the same database you are subscribing to:
>
> * Avoiding subscription/modify loop. When you modify a document that you are subscribing to, the server with send it to 
>   the subscription again. If you'll modify it every time that it is processed, you'll effectively create an infinite loop, with all the costs
>   that this entails. You can see in Listing 5.5 and Listing 5.6 as well that we are careful to avoid this by setting the `Survey` flag when we 
>   processed a document and filtering on that flag in Listing 5.4.
>
> * The document you got may have already been changed on the server. Typically, the lag time of between a document being modified and the 
>   subscription processing that document is very short. That can lead you to think that this happens instantaneously or even worse, as part of 
>   the same operation of modifying the document.
>   Nothing could be further from the truth. A document may be changed between the time the server has sent you the document and the time you 
>   finished processing and saving it. In Listing 5.5 we handled that explicitly using optimistic concurrency and in Listing 5.6 we used 
>   patching to avoid having to deal with the issue. 
>
> * If you are using subscriptions to integrate with other pieces of your infrastructure (such as sending emails, for example), you have to be
>   ready for failure on that end and have some meaningful strategy for handling it. Your options are to either propogate the error up the chain,
>   which will force the subscription to close (and retry from last susccessful batch) or you can catch the exception and handle it in some manner.

On the other hand, in Listing 5.6, we first send the email, then set the flag and save it. This means that if there is an error sending the email
we'll retry the document later on. However, if we had an error saving the flag to the server and we already sent the email, we might send the email
twice. You need to consider what scenario you are trying to prevent, double email send or no email sent at all.

Instead of relying on two phase commit and distributed transactions, a much better alternative is to use the facilities of each system 
on its own. That topic goes beyond the scope of this book, but idempotent operations or de-duplication for operations can both give you a 
safe path to follow in the precense of errors in a distributed system. If the email system will recognize that this email has already been 
sent, the code in Listing 5.6 will have no issue. We'll never skip sending an email and we'll never send the same email twice. 

> **Distributed transactions and RavendB**
>
> The main reason that we have to face this issue is that we are forced to integrate between two systems that do not share a transaction boundary.
> In other words, theoretically speaking, if we could share a transaction between the email sending and the write to RavenDB, the problem would be
> solved. 
>
> In practice, RavenDB had support for distributed transactions with multiple resources up until version 3.x, when we deprecated this feature and 
> version 4.0 removed it completely. Distributed transactiosn (also known as two phase commit or 2PC) _sound_ wonderful. Here you have a complex
> interaction between several different components in your system, and you can use a transaction to orchestrate it all in a nice and simple 
> manner. 
>
> Except it doesn't work like this. Any distributed transaction system that I had worked with had issues related to failure handling and partial
> success. A distributed transaction coordinator basically require all praticipants in the transaction to promise it that if it tells them to
> commit the transaction, it will be successful. In fact, the way a coordinator usually work is by having one round of promises, and if all
> participants have been able to make that promise a second round with confirmations. Hence, the two phase commit name.
>
> The problem starts when you have gotten a promise from all the participants, you already confirmed with a few of them that the transaction
> has been committed, and one of the particpants fail to commit for whatever reason (hardware failure, for example). In that case, the 
> transaction is in a funny, half committed state. 
>
> The coordinator will tell you that this is a bug in the participant, that it shouldn't have made a promise that it couldn't keep. And typically
> coordinators will retry such transactions (manually or automatically) and recover from trasients errors. But the problem with "it is an issue
> with this particular particpant, not the coordinator" line of thinking is that those kind of errors are happening in production. 
>
> In one particular project, we had to restart the coordinator and manually resolve hanging transactions on a bi-weekly basis, and it wasn't a 
> very large or busy website. [Joe Armstrong](https://en.wikipedia.org/wiki/Joe_Armstrong_(programming) ), Inventor of Erlang,
> described^[That particular lecture was over a decade ago, and I still vividly remember it, it was _that_ good.] the problem far better than I 
> could:
>
> > The Two Generals' Problem is reality, but the computer industry says, it doesn't believe 
> > in mathematics: Two phase commit^[There is also the _three_ phase commit, which just add to the fun and doesn't actually solve the issue.] 
> > always works!

There is another issue with the code in Listing 5.5 and Listing 5.6. They are incredibly wasteful in the number of remote calls that they are making. One of 
the key benefits of using batch processing is the fact that we can handle things, well, in a batch. However, both Listing 5.5 and Listing 5.6 will create a
session per document in the batch. The default batch size (assuming we have enough documents to send to fill a batch) is in the order of 4,096 items. That 
means that if we have a full batch, the code in either one of the previous listings will generate 8,192 remote calls. That is a _lot_ of work to send to the
server, all of which is handled in a serial fashion.

We can take advantage of the batch nature of subscriptions to do _much_ better. Turn you attention to Listing 5.7. 

```{caption="Efficeintly process the batch" .cs}
await subscription.Run( batch =>
{
	using(var session = batch.OpenSession())
	{
		var customerIds = batch.Items
			.Select(item=> item.Result.CustomerId)
			.Distinct()
			.ToList();
		// force load of all the customers in the batch
		// in a single request to the server 
		session.Load<Customer>(customerIds);

		foreach(var item in batch.Items)
		{
			SupportCall call = item.Document;
			var age = DateTime.Today - call.Started;
			if( age > DateTime.FromDays(14))
				return; // no need to send survey for old stuff

			// customer was already loaded into the session
			// no remote call will be made
			var customer = session.Load<Customer>(
				call.CustomerId);

			// register the change on the session
			// no remote call will be made
			session.Advanced.Patch<SupportCall, bool>(
				result.Id,
				c => c.Survey,
				true);

			SendSurveyEmailTo(customer, call);

		}

		// send a single request with all the 
		// changes registered on the seession
		session.SaveChanges();
	}
});
```

Listing 5.7 and Listing 5.6 are functionally identical. They have the same exact behavior, except that Listing 5.6 will generate 8,192 requests, and Listing 5.7
will generate just 2. Yep, the code in Listing 5.7 is always going to generate just two requests. First a bulk load of all the customers in the batch and then
a single `SaveChanges` with all the changes for all the support calls in the batch.

Note that we are relying on the `Unit of Work` nature of the session. Once we loaded a document into it, trying to load it again will give us the already loaded
version without going to the server. Without this feature, the amount of calls to `Load` would have probably forced us over the budget of remote calls allowed
for the session^[Remember, that budget is configurable, but it is there mostly to help you realize that generate so many requests is probably not healthy for 
you].

Listing 5.7 takes full advantage of the batch nature of subscriptions, in fact, the whole reason why the batch expose a `List` property instead of just being
an enumerable is to allow you to make those kinds of optimizations. By making it obvious that scanning the list of items per batch is effectively free, we are 
left with the option of traversing it multiple times and optimizing our behavior.

## Complex conditionals 

We already saw how we can create a subscription that filter documents on the server side, in Listing 5.4. The code there used a Linq expression and 
the client API was able to turn that into a JavaScript function that was sent to the server. Listing 5.4 was a pretty simple expression, but the code
that handles the translation between Linq expressions and JavaScript is quite smart and is able to handle much more complex conditions.

However, putting a complex condition in a single Linq expression is not a good recipe for good code. A better alternative is to skip the convienance of 
the Linq expression and go directly to the JavaScript. In Listing 5.8, we are going to see how we can subscribe to all the support calls that require 
special attention.

```{caption="Creating a subscription using JavaScript filtering" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	var watchwords = ['annoy', 'hard', 'silly'];

	var lastIndex = this['@metadata']['Last-Monitored-Index'] || 0;

	for(var i = lastIndex; i < this.Comments.length; i++)
	{
		for(var j = 0; j < watchwords.length; j++)
		{
			var comment = this.Comments[i].toLowerCase();
			if(comment.indexOf(watchowrd[i]) != -1)
				return true;
		}
	}

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

The interesting code in Listing 5.8 is in the `Script` property. We are defining a few words that we will watch for, and if we see them in the comments 
of a support call, we want to give it some special attention via this subscription. We that by simply scanning through the array of `Comments` and checking
if any of the comments contains any of the words that we are looking for.

The one interesting tidbit is the use of `this['@metadata']['Last-Monitored-Index']`, what is that for? Remember that a subscription will be sent all the 
documents that match its criteria. And whenever a document is changed, it will be chcked if it match this subscription. That means that if we didn't have
some sort of check to stop it, our subscription will process any support call that had a comment with one of the words we watch for _every single time
that call is processed_. 

In order to avoid that scenario, we set a metadata value named `Last-Monitored-Index` when we process the subscription. You can see how that work in
Listing 5.9.

```{caption="Escalating problematic calls" .cs}
await subscription.Run( batch =>
{
	const string script = @"
var existing = this['@metadata']['Last-Monitored-Index'] || 0;
this['@metadata']['Last-Monitored-Index'] = Math.max(idx, existing);
";
	using(var session = batch.OpenSession())
	{
		foreach(var item in batch.Items)
		{
			// mark the last index that we
			// already observed using Patch
			session.Advanced.Defer(
				new PatchCommandData(
					id: item.Id,
					etag: null, 
					patch: new PatchRequest
					{
						Script = script,
						Values = 
						{
							["idx"] = item.Result.Comments.Count
						}
					},
					patchIfMissing: null, 
				);

			// actually escalate the call
		}

		session.SaveChanges();
	}
});
```

We are simply setting the `Last-Monitored-Index` to the size of the `Comments` on the call and saving it back to the server. This will ensure that we'll only get the support call again only if there are _new_ comments with the words that we are watching for. The code in Listing 5.9 is going out of its way to be a good
citizen and not go to the server any more times than it needs to. This is also a good chance to demonstrate a real usecase for using `Defer` in production code. The use of `Defer` means that we both don't need to worry about the number of calls and that we have handled concurrency. 

Assuming that we don't have a lot of calls that requires escalation, just having a session per batch item (and the associated number of requests) is likely good
enough. And if we _do_ have a higher number of escalated calls, we probably have other, more serious issues. 

> **Maintaining per document subscription state**
>
> Subscription often need to maintain some sort of state on a per document basis. In the case of Listing 5.9, we needed to keep track of the last 
> monitored index, but other times you'll have much more complex state to track. For example, imagine that we need to kick off a workflow that will
> escalate a call once it passed a certain threshold. We might need to keep track of the state of the workflow and have that be account for in the
> subscription itself.
>
> Using the metadata to do it works quite nicely if we are talking about small and simple state. However, as the complexity grows, it isn't viable
> to keep it all in the document metadata and we'll typically introduce a separate document to maintain the state of the subscription. In we are 
> tracking support calls, then for `SupportCalls/238-B` will have a `SupportCalls/238-B/EscalationState` document that will conatain the relevant
> information for the subscription.

Listing 5.8 and Listing 5.9 together show us how a subscription can perform rather complex operations and open up some really interesting options for 
processing documents. But even so, we aren't done, we can do even more with subscriptions.

## Complex scripts

We have used conditional subscriptions to filter the documents that we want to process, and since this filtering is happening on the server side, it 
allows us to reduce the number of documents that we have to send to the subscription. This is awesome, but a really interesting feature of 
subscriptions is that we don't actually _need_ to send the full documents to the client, we can select just the relevant details to send. 

We want to do some processing on highly voted support calls, but we don't need to get the full document, we just need the actual issue and the number
of votes for that call. Instead of sending the full document over the wire, we can use the code in Listing 5.10 to efficeintly.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	return { 
		Issue: this.Issue, 
		Votes: this.Votes 
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

What we are doing in Listing 5.10 is to filter the support calls. If the call has less then 10 votes, we'll just return from the script. RavenDB consider
a `return` or `return false` or `return null` to be an indication that we want to skip this document, and will not send that to the client. On the other
hand, with `return true`, the document will be sent to the client. However, we aren't limited to just returning `true`. We can also return an object of our 
own. That object can be built by us and contain just the stuff that we want to send to the client. RavenDB will send that object directly to the client.

There is an issue here, however. In Listing 5.10, we are creating a subscription on `SupportCall`. However, the value that will be sent to the client for
the subscription to process is _not_ a `SupportCall` document. It is our own object that we created. That means that on the client side, we need to know 
how to handle that. This requires a bit of a change in how we open the subscription, as you can see in Listing 5.11.

```{caption="Opening subscription with a different target" .cs}
public class SupportCallSubscriptionOutput
{
	public string Issue;
	public int Votes;
}

var subscription = store.Subscriptions
	.Open<SupportCallSubscriptionResult>(options));

await subscription.Run(batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCallSubscriptionResult result = item.Result;
		// do something with the 
		// result.Issue, result.Votes
	}
});
```

In order to consume the subscription in a type safe way, we create a class that matches the output that we'll get from the subscription script, and we'll 
use that when we open the subscription. As you can see, there isn't actually a requirement that the type that you use to `Create` the subscription and the
type you use for `Open` will be the same and scenarios like the one outlined in Listing 5.10 and 5.11 make it very useful.

If this was all we could do with the subscription script, it would have been very useful in reducing the amount of data that is sent over the wire, but 
there is actually more options available to us that we haven't gotten around to yet. Consider Listing 5.6, there we get the support call and immediately
have to load the associated customer. That can lead to a remote call per item in the batch. We have already gone over why this can be a very bad idea in
term of overall performance. Even with the optimization we implemented in Listing 5.7, there is still another remote call to do. We can do better.

We can ask RavenDB to handle that as part of the subscription processing directly. Take a look at Listing 5.12, which does just that.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	var customer = LoadDocument(this.CustomerId);

	return { 
		Issue: this.Issue, 
		Votes: this.Votes,
		Customer: {
			Name: customer.Name,
			Email: customer.Email
		}
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

In Listing 5.12, we are calling `LoadDocument` as part of the processing of the subscription on the server side. This allow us to get the customer instance
and send pieces of it back to the client. In order to consume it, we'll need to change the `SupportCallSubscriptionOutput` class that we introduced in 
Listing 5.11 to add the new fields. 

When processing the output of this subscription, we don't need to make any othe remote call, and can 

## Subscription and failover in a cluster

result.OpenSession, BeforeBatch / AfterBatch calls.
no failover by design.

The state of the batch

`SubscriptionException` handling

`Start` / `StartAsync`

Multiple calls to `Subscribe`

Load customer as part pf returning it.


## Using subscription for queues

using (var subscription = store.Subscriptions.Open(new SubscriptionConnectionOptions()))
{
    await subscription.Run(async batch =>
    {
        using (var session = batch.OpenAsyncSession())
        {
            foreach (var item in batch.Items)
            {
                // process message
                session.Delete(item.Id);
            }
            await session.SaveChangesAsync();
        }
    });
}


## Error handling with subscriptions

What happens when there is an error in the processing of a document? Imagine that we had code inside the lambda in the `Run` method and that code threw an 
exception. Unless you set `SubscriptionConnectionOptions.IgnoreSubscriberErrors`^[And you probably shouldn't do that.], we will abort processing of the 
subscription and the `Run` will raise an error. Typical handling in that scenario is to dispose of the subscription and immediately open it again. 

Assuming the error is transient, we'll start processing from the last batch we got and continue forward from there. If the error isn't transient, for example,
some `NullReferenceException` because the code isn't check for it, the error will obviously repeat itself. You might want to set an upper limit to the number
of errors you'll try to recover from in a given time period, and just fail completely afterward. This depend heavily on the kind of error reporting and 
recovery you are using in your applications. 

Note that this apply only to errors that came from the code processing the document. All other errors (connection to server, failover between servers, etc) are
already handled by RavenDB. The reason that we abort the subscription in the case of subscriber error is that there really isn't anything else that we can do.
We don't want to skip processing the document, and just logging the error is possible (in fact, that is exactly what we do if `IgnoreSubscriberErrors` is set)
but no one ever reads the log until the problem was already discovered, which is typically very late in the game. 

However, the actual processing of the documents and any concurrency and error handling during that is on you. In practice, you generally don't 
have to worry about it. An error thrown during document processing will kill your subscription. We saw in Listing 4.32 that after we start
the subscription, we need to pay attention to `SubscriptionLifetimeTask`. If an error is raised during document processing, that error will
close the subscription and any waiters on `SubscriptionLifetimeTask` will get that error.

The typical manner in which you will handle errors with subscriptions is just to retry the whole subscription, as shown in Listing 4.38.

```{caption="Retrying subscription on error" .cs}
var errorTimings = new Queue<DateTime>();
while(true)
{
	var options = new SubscriptionConnectionOptions(subsId);
	var subscription = store.Subscriptions.Open<SupportCall>(
		options));
	subscription.Subscribe(/* redacted for brevity */);
	subscription.Start();

	try
	{
		await subscription.SubscriptionLifetimeTask;
		return; // subscription deleted, process shutting down
	}
	catch(SubscriptionException)
	{
		// not recoverable, log it and exit
		return;
	}
	catch(Exception e)
	{
		// log exception
		errorTimings.Enqueue(DateTime.UtcNow);
		if(errorTimings.Count < 5)
			continue;

		var firstErrorTime = errorTimings.Dequeue();
		var timeSinceErr = DateTime.UtcNow - firstErrorTime;
		if (timeSinceErr < TimeSpan.FromMinutes(5))
			continue;

		// log subscription shut down for constant errors
		return;
	}
}

```

Listing 4.38 shows a typically way to handle errors in subscriptions. If your document processing code throws, that exception will be raised via
`SubscriptionLifetimeTask`. In that case, we use the `errorTimings` queue to check if we got more than 5 errors in the space of the last 5 minutes. If 
we didn't, we continue normally and usually just this is enough to handle most transient errors. However, if we got more than 5 errors in the space of 
5 minutes, we'll abort the subscription and typically alert an admin that something strange is going on.

The policy shown in Listing 4.38 (max 5 errors in 5 minutes) is a trivially simple one. I have seen production system that just kep blindly retrying 
and others that had far more sophisticated process for recovery using exponential backoff to try to avoid long running but eventually transient failure.
Exactly how you'll handle error recovery in your system is up to you and the operations teams that will maintain the application in production.



## Subscription deployment patterns

Error handling & recovery

Run until nothing to do for 15 minutes

Nightly process

Only a single client can have a subscription opened at any given point in time. 

Conflict handling

## Versioned Subscriptions

Handling deletes