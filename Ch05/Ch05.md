
# Advanced Client API Usage

With a good grounding in RavenDB Concepts, we can turn our mind to a more detailed exploration of the RavenDB Client API and what we can do with it.
We have already seen how we can use the client API for CRUD and querying (although we'll deal with that a lot more in [Part II](#part-ii)). Now we'll dive right in and see how we can get the most out of RavenDB. In particular, in this chapter we'll cover how we can get the most by doing the least.

## Lazy is the new fast

The [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing) were already covered in [chapter 1](#distributed-computing), but it is worth going over a couple of them anyway. This time, the relevant fallacy is: Latency is zero.

The reasons that the Fallacies are so well known is that we keep tripping over them. Even experienced developers fall into the habit of thinking that their environment (everything is local) is the production environment, and end up making a lot of remote calls. But latency _isn't_ zero, and usually, just the cost of going to the database is much higher than actually executing whatever database operation we wanted. 

The RavenDB Client API deals with this in several ways. First, whenever possible, we batch calls together. When we call `Store` on a few entities, we don't go to the server. Instead, we wait for the call to `SaveChanges` which then saves all the the changes in one remote call. Second, we have a budget in place on how many remote calls a single session can do. If your code exceeds this budget, and exception will be thrown. 

Because of this limit, we have a lot of ways in which we can reduce the number of times we have to go to the server. One such example is the [Includes feature](#include), which tells to RavenDB that we'll want the associated documents as well as the one we asked for. But what is probably the most interesting way to reduce the number of remote calls is to be lazy. Let us first look at Listing 1 and then we'll discuss what is going on there.

```{caption="{Using Lazy Operations}" .cs }  
var lazyOrder = DocumentSession.Advanced.Lazily
	.Include<Order>(x => x.Company)
	.Load("orders/1");

var lazyProducts = DocumentSession.Query<Product>()
	.Where(x=>x.Category == "categories/2")
	.Lazily();

DocumentSession.Advanced.Eagerly.ExecuteAllPendingLazyOperations();

var order = lazyOrder.Value;

var products = lazyProducts.Value;

var company = DocumentSession.Load<Company>(order.Company);

// show order, company & products to the user
```

In Listing 1, instead of writing `DocumentSession.Include(...).Load(...)`, we used the `Advanced.Lazily` option, and instead of ending the query with a `ToList` we used the `Lazily` extension method. That much is obvious, but what does this _mean_?
When we use lazy operations, we aren't actually executing the operation. We merely register that we want that to happen. It is only when `Eagerly.ExecuteAllPendingLazyOperations` is called that those operation are executing, and they all happen in one round trip.

Consider the case of a waiter in a restaurant, when the waiter is taking the order from a group of people, it is possible for him to go to the kitchen and let the cook know about a new Fish & Chips^[This part of the book was written while I'm hungry, there might be additional food metaphores.] plate whenever a member in the group is making an order. But it is far more efficient to wait until every member in the group has ordered, and then going to the kitchen once.

> Eagerly.ExecuteAllPendingLazyOperations vs. lazyOrder.Value
> 
> In Listing 1, we have used the `Eagerly.ExecuteAllPendingLazyOperations` method to force all the lazy values to execute. But that is merely a convienance method. We could have done the same by just calling `lazyOrder.Value`. 
>
> At that point, the lazy object would initialize itself, realize that it hasn't been executed yet, and call `Eagerly.ExecuteAllPendingLazyOperations` itself. So the end result is very much the same, but we still prefer to call `Eagerly.ExecuteAllPendingLazyOperations` explicitly. 
> 
> It is easy to not notice that you are using the `lazyOrder.Value` property and trigger a query. I believe that it is much better when you have an explicit lazy evaluation boundary, rather than an implicit one. That is especially true if you are coming into the code several months or years after you wrote it.

That is exactly what lazy does. Except that it is actually even better. Lazy batch all the requests until `Eagerly.ExecuteAllPendingLazyOperations` is called, and then send them to the server. But on the server side, we unpack the request and then execute all of the requests _in parallel_. The idea is that we can reduce the overall latency by reducing the number of remote calls, and executing the requests in parallel means that we are done process everything so much faster.

Except for the deferred execution of the lazy operation, it behave in exactly the same way. That holds true for options like `Include` as well. Although, of course, the included document is only loaded into the session when the lazy operation has completed. Every read operation supported by RavenDB is also available to be executed as a lazy request.

On the server side, we buffer all the responses for the lazy request until all the inner requests are done, then we send them to the client. That means that if you want to get back a very large amount of data, you probably don't want to use lazy, or be prepare for increased memory usage on the server side. 

A better approach for dealing with very large requests is streaming.

## Streaming results

## Bulk inserts

## Partial document updates

## Listeners

It is pretty common to want to run some code whenever something happens in RavenDB. The classic example is when you want to store some audit information about who modified a document. In the previous section, we saw that we can do that manually, but that is both tedious and prone to errors or ommisions. It would be much better if we could do it in a single place.

That is why the RavenDB Client API has the notion of listeners. Listeners allows you to define, in a single place, additional behavior that RavenDB will execute at particular points in time. RavenDB has the following listeners:

* `IDocumentStoreListener` - called when an entity is stored on the server.
* `IDocumentDeleteListener` - called when a document is being deleted.
* `IDocumentQueryListener` - called before a query is made to the server.
* `IDocumentConversionListener` - called when converting an entity to a document and vice versa.
* `IDocumentConflictListener` - called when a replication conflicted is encountered, this listener is discussed in depth in Chapter 10, Replication.

The store and delete listeners are pretty obvious. They are called whenever a document is stored (which can be a new document or an updated to an existing one) or when the document is deleted. A common use case for the store listener is as an audit listener, which can record which user last touched a document. A delete listener can be used to prevent deletion of a document based on your business logic, and a query listener can modify any query issued.

You can see examples of all three in Listing 1.

```{caption="{Store, Delete and Query listeners}" .cs }  
public class AuditStoreListener : IDocumentStoreListener
{
	public bool BeforeStore(string key, 
		object entityInstance, RavenJObject metadata, 
		RavenJObject original)
	{
		metadata["Last-Modified-By"] = WindowsIdentity
			.GetCurrent().Name;
		return false;
	}

	public void AfterStore(string key, 
		object entityInstance, RavenJObject metadata)
	{
	}
}

public class PreventActiveUserDeleteListener : 
	IDocumentDeleteListener
{
	public void BeforeDelete(string key, 
		object entityInstance, RavenJObject metadata)
	{
		var user = entityInstance as User;
		if (user == null)
			return;
		if (user.IsActive)
			throw new InvalidOperationException(
				"Cannot delete active user: " +
				 user.Name);
	}
}

public class OnlyActiveUsersQueryListener : 
	IDocumentQueryListener
{
	public void BeforeQueryExecuted(
		IDocumentQueryCustomization queryCustomization)
	{
		var userQuery = queryCustomization as
			IDocumentQuery<User>;
		if (userQuery == null)
			return;
		userQuery.AndAlso().WhereEquals("IsActive", true);
	}
}
```
In the `AuditStoreListener`, we modify the metadata to include the current user name. Note that we return `false` from the `BeforeStore` method as an indication that we didn't change the `entityInstance` parameter. This is an optimization step, so we won't be forced to re-serialize the `entityInstance` if if wasn't changed by the listener.

In the `PreventActiveUserDeleteListener` case, we throw if an active user is being deleted. This is very straightforward and easy to follow. It is the case of `OnlyActiveUsersQueryListener` that is interesting. Here we check if we are querying on users (by checking if the query to customize is an instance of `IDocumentQuery<User>`) and if it is, we also add a filter on active users only. In this manner, we can ensure that all user queries will operate only on active users.

We register the listeners on the document store during the initialization. Listing 2 shows the updated `CreateDocumentStore` method on the `DocumentStoreHolder` class.

```{caption="Registering listeners in the document store" .cs }   
private static IDocumentStore CreateDocumentStore()
{
	var documentStore = new DocumentStore
	{
		Url = "http://localhost:8080",
		DefaultDatabase = "Northwind",
	};

	documentStore.RegisterListener(
			new AuditStoreListener());
	documentStore.RegisterListener(
			new PreventActiveUserDeleteListener());
	documentStore.RegisterListener(
			new OnlyQueryActiveUsers());

	documentStore.Initialize();
	return documentStore;
}
```

Once registered, the listeners are active and will be called whenever their respected action occur. 

The `IDocumentConversionListener` allows you a fine grained control over the process of the conversion process of entities to documents and vice versa. If you need to pull data from an additional system when a document is loaded, this is usually the place where you'll put it^[That said, pulling data froms secondary sources on document load is frowned upon, documents are coherent and independent. You shouldn't require additional data, and that is usually a performance problem].

A far more common scenario for conversion listener is to handle versioning, whereby you modify the old version of the document to match an update entity definition on the fly. This is a way for you to do rolling migrations, without an expensive stop-the-world step along the way.

While the document conversion listener is a great aid in controling the conversion process, if all you care about is the actual serialization, without the need to run your own logic, it is probably best to go directly to the serializer and use that.

## The Serialization Process

RavenDB uses the [Newtonsoft.JSON](http://james.newtonking.com/json) library for serialization. This is a very rich library with quite a lot of options and levers that you can tweak. 
Because of version incompatibilities between RavenDB and other libraries that also has a dependeny on Newtonsoft.JSON, RavenDB has internalized the Newtonsoft.JSON library. 
To access the RavenDB copy of Newtonsoft.JSON, you need to use the following namespace: `Raven.Imports.Newtonsoft.Json`. 

Newtonsoft.JSON has several options for customizing the serialization process. One of those is a set of attributes (`JsonObjectAttribute`, `JsonPropertyAttribute`, etc). Because RavenDB has its own copy, it is possible to have two sets of such attributes. One for serialization of the entity to a doucment in RavenDB, and another for serialization of the document for external consumption.

Another method of customizing the serialization in Newtonsoft.JSON is using the `documentStore.Conventions.CustomizeJsonSerializer` event.
Whenever a serializer is created by RavenDB, this event is called and allow you to define the serializer's settings. You can see an example of that in Listing 3.

```{caption="Customizing the serialization of money" .cs }   
DocumentStoreHolder.Store.Conventions
	.CustomizeJsonSerializer += serializer => 
	{
		serializer.Converters.Add(new JsonMoneyConverter());
	};

public class JsonMoneyConverter : JsonConverter
{
	public override void WriteJson(JsonWriter writer, 
		object value, JsonSerializer serializer)
	{
		var money = (Money) value;
		writer.WriteValue(money.Amount + " " + money.Currency);
	}

	public override object ReadJson(JsonReader reader, 
		Type objectType, object existingValue, 
		JsonSerializer serializer)
	{
		var parts = reader.ReadAsString().Split();
		return new Money
		{
			Amount = decimal.Parse(parts[0]),
			Currency = parts[1]
		};
	}

	public override bool CanConvert(Type objectType)
	{
		return objectType == typeof (Money);
	}
}

public class Money
{
	public string Currency { get; set; }
	public decimal Amount { get; set; }
}
```
The idea in Listing 3 is to have a `Money` object that holds both the amount and the currency, but to serialize it to JSON as a string string property. So a `Money` object reprensenting 10 US Dollars would be serialized to the following string: "10 USD".

The JsonMoneyConverter converts to and from the string representation, and the json serializer customization event register the converter with the serializer. Note that this is probably not a good idea, and you will want to store the `Money` without modifications, so you can do things like sum up order by currentcy, or actually work with the data.

I would only consider using this approach as an intermediary step, probably as part of a migration if I had two versions of the application working concurrently on the same database.

## Changes() API

## Result transformers

### Load Document

### Include

## Summary

 How you can use listeners to provide application wide behavior from a single location. For example, handling auditing metadata in a listener means that you never have to worry about forgetting to write the audit code.
We also learned about the the serialization process and how you can have fine grained control over everything that goes on during serialization and deserialization.