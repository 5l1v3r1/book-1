
# Advanced Client API Usage

With a good grounding in RavenDB Concepts, we can turn our mind to a more detailed exploration of the RavenDB Client API and what we can do with it.
We have already seen how we can use the client API for CRUD and querying (although we'll deal with that a lot more in [Part II](#part-ii)). Now we'll dive right in and see how we can get the most out of RavenDB. In particular, in this chapter we'll cover how we can get the most by doing the least.

## Lazy is the new fast

The [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing) were already covered in [chapter 1](#distributed-computing), but it is worth going over a couple of them anyway. This time, the relevant fallacy is: Latency is zero.

The reasons that the Fallacies are so well known is that we keep tripping over them. Even experienced developers fall into the habit of thinking that their environment (everything is local) is the production environment, and end up making a lot of remote calls. But latency _isn't_ zero, and usually, just the cost of going to the database is much higher than actually executing whatever database operation we wanted. 

The RavenDB Client API deals with this in several ways. First, whenever possible, we batch calls together. When we call `Store` on a few entities, we don't go to the server. Instead, we wait for the call to `SaveChanges` which then saves all the the changes in one remote call. Second, we have a budget in place on how many remote calls a single session can do. If your code exceeds this budget, and exception will be thrown. 

Because of this limit, we have a lot of ways in which we can reduce the number of times we have to go to the server. One such example is the [Includes feature](#include), which tells to RavenDB that we'll want the associated documents as well as the one we asked for. But what is probably the most interesting way to reduce the number of remote calls is to be lazy. Let us first look at Listing 1 and then we'll discuss what is going on there.

```{caption="{Using Lazy Operations}" .cs }  
var lazyOrder = DocumentSession.Advanced.Lazily
	.Include<Order>(x => x.Company)
	.Load("orders/1");

var lazyProducts = DocumentSession.Query<Product>()
	.Where(x=>x.Category == "categories/2")
	.Lazily();

DocumentSession.Advanced
	.Eagerly.ExecuteAllPendingLazyOperations();

var order = lazyOrder.Value;

var products = lazyProducts.Value;

var company = DocumentSession.Load<Company>(order.Company);

// show order, company & products to the user
```

In Listing 1, instead of writing `DocumentSession.Include(...).Load(...)`, we used the `Advanced.Lazily` option, and instead of ending the query with a `ToList` we used the `Lazily` extension method. That much is obvious, but what does this _mean_?
When we use lazy operations, we aren't actually executing the operation. We merely register that we want that to happen. It is only when `Eagerly.ExecuteAllPendingLazyOperations` is called that those operation are executing, and they all happen in one round trip.

Consider the case of a waiter in a restaurant, when the waiter is taking the order from a group of people, it is possible for him to go to the kitchen and let the cook know about a new Fish & Chips^[This part of the book was written while I'm hungry, there might be additional food metaphores.] plate whenever a member in the group is making an order. But it is far more efficient to wait until every member in the group has ordered, and then going to the kitchen once.

> **Eagerly.ExecuteAllPendingLazyOperations vs. lazyOrder.Value**
> 
> In Listing 1, we have used the `Eagerly.ExecuteAllPendingLazyOperations` method to force all the lazy values to execute. But that is merely a convienance method. We could have done the same by just calling `lazyOrder.Value`. 
>
> At that point, the lazy object would initialize itself, realize that it hasn't been executed yet, and call `Eagerly.ExecuteAllPendingLazyOperations` itself. So the end result is very much the same, but we still prefer to call `Eagerly.ExecuteAllPendingLazyOperations` explicitly. 
> 
> It is easy to not notice that you are using the `lazyOrder.Value` property and trigger a query. I believe that it is much better when you have an explicit lazy evaluation boundary, rather than an implicit one. That is especially true if you are coming into the code several months or years after you wrote it.

That is exactly what lazy does. Except that it is actually even better. Lazy batch all the requests until `Eagerly.ExecuteAllPendingLazyOperations` is called, and then send them to the server. But on the server side, we unpack the request and then execute all of the requests _in parallel_. The idea is that we can reduce the overall latency by reducing the number of remote calls, and executing the requests in parallel means that we are done process everything so much faster.

Except for the deferred execution of the lazy operation, it behave in exactly the same way. That holds true for options like `Include` as well. Although, of course, the included document is only loaded into the session when the lazy operation has completed. Every read operation supported by RavenDB is also available to be executed as a lazy request.

On the server side, we buffer all the responses for the lazy request until all the inner requests are done, then we send them to the client. That means that if you want to get back a very large amount of data, you probably don't want to use lazy, or be prepare for increased memory usage on the server side. 

A better approach for dealing with very large requests is [streaming](#streaming-results), but before we get there, let us look at another way in which RavenDB is stopping bad things from happening.

## Unbounded Results Set Prevention

RavenDB was designed from the get go to be safe by deafult. One of the way that this express itself is in the internal governors that it has. We have seen one such governor in the limit on the number of remote calls that a session can do. Another such governor is the limit on the number of results that will be returned from the database. Take a look at the following snippet:

	var orders = DocumentSession.Query<Order>()
		.ToList();

There are 880 orders in the Northwind database, how many results will this query return? As you can imagine, the answer is _not_ 880. Code like this snippet is _bad_, because it make assumptions about the size of the data. What would happen if there were three million orders in the database? Would we really want to load and materialize them all? This problem is called Unbounded Result Set, and it is very common in production, because it sneaks up on you. You start out your system, and everything is fast and fine. Over time, more and more data is added, and you end up with a system that slows down. In most cases I've seen, just after reading all of the orders, we discarded 99.5% of them and just showed the user the last 15.

With RavenDB, such a thing is not possible. If you don't specify otherwise, all queries are assumed to have a `.Take(128)` on them. So the answer to my previous question is that the snippet above would result in 128 order documents being returned. Some people are quite upset with this design decision, their argument boils down to: "This code might kill my system, but I want it to behave like I expect it to". I'm not sure why they expect to kill their system, but they can do that without RavenDB's help^[You can still shoot yourself in the foot with RavenDB, but we like to think it would take some effort to do so.]. That way, hopefully it wouldn't be us that would get the 2 AM wake up call and try to resolve what is going on.

Naturally, a developer's first response to hearing about the default `.Take(128)` clause is this:

	var orders = DocumentSession.Query<Order>()
		.Take(int.MaxValue) // fix RavenDB bug
		.ToList();

Which is why we have an additional limit. You can specify a take clause up to 1024 in size. Any value greater than 1024 will be interperted as 1024. Now, if you _really_ want, you can change that by specifying the `Raven/MaxPageSize` configuration, but we very strongly recommend against that. RavenDB is designed for OLTP scenarios, and there are really very few situations where you want to read a _lot_ of data to process a user's request.

For the situations where you actually do need all the data, the `Query` API isn't really a suitable interface for that. That is why we have result streaming in RavenDB.

## Streaming results

Large result sets should be rare in your applications. RavenDB is designed for OLTP applications, and there is very little cause for loading tens of thousands of documents and try showing them all the a user. They just have no way of processing so much information. It is much better to give them paging, so they can consume the data at a reasonable rate, and reduce the overall load on the entire system.

But there is one relatively common case where you do need to have access to the entire dataset: Excel. 

More properly, any time that you need to give the user access to a whole lot of records to be processed offline. Usually you output things in a format that Excel can understand, so the users can work with the data in a really nice tool. But reports in general is a very common scenario for this requirement.

So how can we do that? One way to handle that would be to just page, something like the abomination in Listing 2:

```{caption="{The WRONG way to get all users}" .cs}
public List<User> GetAllUsers()
{
	// This code is an example of how NOT
	// to do things, do NOT try to use it
	// ever. I mean it!
	List<User> allUsers = new List();
	int start = 0;
	while(true)
	{
		using(var session = DocumentStoreHolder.OpenSession())
		{
			   var current = session.Query<User>()
				   	.Take(1024)
				   	.Skip(start)
				   	.ToList();
			   if(current.Count == 0)
			        break;

			   start+= current.Count;
			   allUsers.AddRange(current);
		}
	}
	return allUsers;
}
```

I call the code an abomination because it has quite a few problems. To start with, this code will work just fine on small amount of data, but as the data size grows, it will do more and more work, and consume more and more resources. Let us assume that we have a moderate number of users, a 100,000 or so. 
The cost of the code in Listing 2 is: go to the server 98 times, do deeper paging on each request, hold the entire 100,000 users in memory. 

Note that the code there is also evil because it uses a different session per loop iteration, preventing RavenDB from detecting the fact that this code is going to hammer the database with requests. Another problem is when can we start processing this? What happens is the code buffer all the results, so we have to wait until the entire process is done to start handling this. All of those means longer duration, higher memory usage and a lot of waste.

And what happens if someone is adding or deleting documents while this code is running? We don't make a single request, so it happens in multiple transactions, so there is also this issue. In short, never write code like this. RavenDB has builtin support for properly handling large number of results, and it is intentionally modelled to be efficeint at that scale.
Say hello to streaming. Here is the same `GetAllUsers` method, now written properly:

```{caption="{The proper way to get all users}" .cs}
public IEnumerable<User> GetAllUsers()
{
	var allUsers = DocumentSession.Query<User>();
	IEnumerator<StreamResult<User>> stream =
		DocumentSession.Advanced.Stream(allUsers);
	while (stream.MoveNext())
	{
		yield return stream.Current.Document;
	}
}
```

Beside the fact that there is a lot less code here, let us take a look at what this code does. Instead of sending multiple queries to the server, we are making a single query. We also indicate that this is a streaming query, which means that the RavenDB Client API will have a very different behavior, and use a different endpoint for processing this query.

> **Paging and streaming**
>
> By default, a stream will fetch as many results as you need (up to 2.1 billion or so), but you can also apply all the normal paging rules to a stream as well. 
> Just as a `.Take(10 * 1000)` to the query before you pass it to the `Stream` method.

On the client side, the differences are:

* The use of enumerator to immediately expose the results are they stream in, instead of waiting for all of them to arrive before giving anything back to your code.
* The result of the `Stream` operation are _not_ tracked by the session. There can be a _lot_ of results, and tracking them would put a lot of memory pressure on your system. It is also very rare to call `SaveChanges` on a session that is taking part in streaming operation, so we don't lose anything.

The dedicated endpoint on the server side has different behavior as well. Obviously, it does not apply the `Raven/MaxPageSize` limit. But much more importantly, it will stream the results to you without doing any buffering. So the client can start processing the results before the server has finished sending them. Another benefit is consistency, throughout the entire streaming operation, we are going to be running under a single transaction. 

What happens is that we operate on a database snapshot, so any addition / deletions / modifications are just not visible. As far as the streaming operation is concerned, the database is frozen at the time of the beginning of the streaming operation.

> **RavenDB Excel Integration **
> 
> I mentioned earlier that a very common use case for streaming is the need to expose the database data to users in Excel format. Because this is such a common scenario, RavenDB comes with a dedicated support for that. The output of the `Stream` operation is usually JSON, but we can ask RavenDB to output it in CSV format that can be readily consumed in Excel.
> You can go to the [Excel Integration documentation](http://ravendb.net/docs/http-api/excel-integration) page to see the walk through. 
>
> The nice thing about that is that you can even update the data from the database into Excel after the first import. 
>
> That said, please read the [Reporting Chapter](#reporting) for fuller discussion on how to handle reporting with RavenDB. In general just giving users access to the raw data in your database result in a mess down the road.

The `Stream` operation accept a query, or a document id prefix, or even just the latest etag that you have (which allow you to read all documents in update order). This is the usual way you're going to use to fetch large amount of data from RavenDB.

But what if I want to go the other way around? What if I want to _save_ a lot of data into RavenDB? That is why we have the bulk insert operation.

## Bulk inserts

## Partial document updates

## Listeners

It is pretty common to want to run some code whenever something happens in RavenDB. The classic example is when you want to store some audit information about who modified a document. In the previous section, we saw that we can do that manually, but that is both tedious and prone to errors or ommisions. It would be much better if we could do it in a single place.

That is why the RavenDB Client API has the notion of listeners. Listeners allows you to define, in a single place, additional behavior that RavenDB will execute at particular points in time. RavenDB has the following listeners:

* `IDocumentStoreListener` - called when an entity is stored on the server.
* `IDocumentDeleteListener` - called when a document is being deleted.
* `IDocumentQueryListener` - called before a query is made to the server.
* `IDocumentConversionListener` - called when converting an entity to a document and vice versa.
* `IDocumentConflictListener` - called when a replication conflicted is encountered, this listener is discussed in depth in Chapter 10, Replication.

The store and delete listeners are pretty obvious. They are called whenever a document is stored (which can be a new document or an updated to an existing one) or when the document is deleted. A common use case for the store listener is as an audit listener, which can record which user last touched a document. A delete listener can be used to prevent deletion of a document based on your business logic, and a query listener can modify any query issued.

You can see examples of all three in Listing 1.

```{caption="{Store, Delete and Query listeners}" .cs }  
public class AuditStoreListener : IDocumentStoreListener
{
	public bool BeforeStore(string key, 
		object entityInstance, RavenJObject metadata, 
		RavenJObject original)
	{
		metadata["Last-Modified-By"] = WindowsIdentity
			.GetCurrent().Name;
		return false;
	}

	public void AfterStore(string key, 
		object entityInstance, RavenJObject metadata)
	{
	}
}

public class PreventActiveUserDeleteListener : 
	IDocumentDeleteListener
{
	public void BeforeDelete(string key, 
		object entityInstance, RavenJObject metadata)
	{
		var user = entityInstance as User;
		if (user == null)
			return;
		if (user.IsActive)
			throw new InvalidOperationException(
				"Cannot delete active user: " +
				 user.Name);
	}
}

public class OnlyActiveUsersQueryListener : 
	IDocumentQueryListener
{
	public void BeforeQueryExecuted(
		IDocumentQueryCustomization queryCustomization)
	{
		var userQuery = queryCustomization as
			IDocumentQuery<User>;
		if (userQuery == null)
			return;
		userQuery.AndAlso().WhereEquals("IsActive", true);
	}
}
```
In the `AuditStoreListener`, we modify the metadata to include the current user name. Note that we return `false` from the `BeforeStore` method as an indication that we didn't change the `entityInstance` parameter. This is an optimization step, so we won't be forced to re-serialize the `entityInstance` if if wasn't changed by the listener.

In the `PreventActiveUserDeleteListener` case, we throw if an active user is being deleted. This is very straightforward and easy to follow. It is the case of `OnlyActiveUsersQueryListener` that is interesting. Here we check if we are querying on users (by checking if the query to customize is an instance of `IDocumentQuery<User>`) and if it is, we also add a filter on active users only. In this manner, we can ensure that all user queries will operate only on active users.

We register the listeners on the document store during the initialization. Listing 2 shows the updated `CreateDocumentStore` method on the `DocumentStoreHolder` class.

```{caption="Registering listeners in the document store" .cs }   
private static IDocumentStore CreateDocumentStore()
{
	var documentStore = new DocumentStore
	{
		Url = "http://localhost:8080",
		DefaultDatabase = "Northwind",
	};

	documentStore.RegisterListener(
			new AuditStoreListener());
	documentStore.RegisterListener(
			new PreventActiveUserDeleteListener());
	documentStore.RegisterListener(
			new OnlyQueryActiveUsers());

	documentStore.Initialize();
	return documentStore;
}
```

Once registered, the listeners are active and will be called whenever their respected action occur. 

The `IDocumentConversionListener` allows you a fine grained control over the process of the conversion process of entities to documents and vice versa. If you need to pull data from an additional system when a document is loaded, this is usually the place where you'll put it^[That said, pulling data froms secondary sources on document load is frowned upon, documents are coherent and independent. You shouldn't require additional data, and that is usually a performance problem].

A far more common scenario for conversion listener is to handle versioning, whereby you modify the old version of the document to match an update entity definition on the fly. This is a way for you to do rolling migrations, without an expensive stop-the-world step along the way.

While the document conversion listener is a great aid in controling the conversion process, if all you care about is the actual serialization, without the need to run your own logic, it is probably best to go directly to the serializer and use that.

## The Serialization Process

RavenDB uses the [Newtonsoft.JSON](http://james.newtonking.com/json) library for serialization. This is a very rich library with quite a lot of options and levers that you can tweak. 
Because of version incompatibilities between RavenDB and other libraries that also has a dependeny on Newtonsoft.JSON, RavenDB has internalized the Newtonsoft.JSON library. 
To access the RavenDB copy of Newtonsoft.JSON, you need to use the following namespace: `Raven.Imports.Newtonsoft.Json`. 

Newtonsoft.JSON has several options for customizing the serialization process. One of those is a set of attributes (`JsonObjectAttribute`, `JsonPropertyAttribute`, etc). Because RavenDB has its own copy, it is possible to have two sets of such attributes. One for serialization of the entity to a doucment in RavenDB, and another for serialization of the document for external consumption.

Another method of customizing the serialization in Newtonsoft.JSON is using the `documentStore.Conventions.CustomizeJsonSerializer` event.
Whenever a serializer is created by RavenDB, this event is called and allow you to define the serializer's settings. You can see an example of that in Listing 3.

```{caption="Customizing the serialization of money" .cs }   
DocumentStoreHolder.Store.Conventions
	.CustomizeJsonSerializer += serializer => 
	{
		serializer.Converters.Add(new JsonMoneyConverter());
	};

public class JsonMoneyConverter : JsonConverter
{
	public override void WriteJson(JsonWriter writer, 
		object value, JsonSerializer serializer)
	{
		var money = (Money) value;
		writer.WriteValue(money.Amount + " " + money.Currency);
	}

	public override object ReadJson(JsonReader reader, 
		Type objectType, object existingValue, 
		JsonSerializer serializer)
	{
		var parts = reader.ReadAsString().Split();
		return new Money
		{
			Amount = decimal.Parse(parts[0]),
			Currency = parts[1]
		};
	}

	public override bool CanConvert(Type objectType)
	{
		return objectType == typeof (Money);
	}
}

public class Money
{
	public string Currency { get; set; }
	public decimal Amount { get; set; }
}
```
The idea in Listing 3 is to have a `Money` object that holds both the amount and the currency, but to serialize it to JSON as a string string property. So a `Money` object reprensenting 10 US Dollars would be serialized to the following string: "10 USD".

The JsonMoneyConverter converts to and from the string representation, and the json serializer customization event register the converter with the serializer. Note that this is probably not a good idea, and you will want to store the `Money` without modifications, so you can do things like sum up order by currentcy, or actually work with the data.

I would only consider using this approach as an intermediary step, probably as part of a migration if I had two versions of the application working concurrently on the same database.

## Changes() API

## Result transformers

### Load Document

### Include

## Summary

 How you can use listeners to provide application wide behavior from a single location. For example, handling auditing metadata in a listener means that you never have to worry about forgetting to write the audit code.
We also learned about the the serialization process and how you can have fine grained control over everything that goes on during serialization and deserialization.