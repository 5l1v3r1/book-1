
# Data Subscriptions

The final topic of this chapter is also one of my favorites. In many applications, you have the need to process the data inside the database. It can be
because you need to run analytics on your orders, take steps at various steps in a workflow or run business processes over the documents stored inside
RavenDB. The common thing that is shared among all of those requirements is that we need to process all documents matching a particular criteria and
we must do that on an ongoing basis.

In other words, subscriptions are a good way to handle any scenario in which I want to run a business process on a set of data, and continue to run the 
process on all incoming data that also match my criteria. Typically, you would implement such a process by issuing a query, remembering your most recently
read value and going from there. But those kind of systems require polling, rather complex to build and scale and in general require a lot of upkeep around 
them.

In contrast, RavenDB Subscriptions allow you to create a subscription for a set of documents matching a given criteria and the database itself will manage
the subscription and make sure that you have process all the documents matching your subscription. This also include error handling, recovery and retries,
on the fly updates, etc. 

Working with subscriptions is divided into two distinct operations. Creating the subscription and opening it. In Listing 4.32 you can see how we create a 
subscription to process customers. 

```{caption="Creating a subscription to process customers" .cs}
var options = new SubscriptionCreationOptions<Customer>();
string subsId = store.Subscriptions.Create(options);
```

The result of the `Create` call is a subscription id, which we can pass to the `Open` call. Only a single client can have a subscription opened at any given
point in time. The code in Listing 4.32 will create a subscription that will match all the `Customer` documents in the database. Listing 4.33 shows how we
open the subscription and register to get notifications. 

```{caption="Opening and using a subscription to process customers" .cs}
var options = new SubscriptionConnectionOptions(subsId);
using(var subscription = store.Subscriptions.Open<Customer>(options)))
{
	// wait until the subscription is done
	// typically, a subscription lasts forever
	await subscription.Run(/* redacted for brevity */);
}
```

There isn't much in Listing 4.33. We open a subscription using the previous gotten `subsId`, then we `Subscribe`... something, I redacted it for now, and 
we'll see what it is in the nexte section. We then start the subscription and it is the next bit that is interesting. We wait on the `SubscriptionLifetimeTask`,
why do we do that?
In general, a subscription will live for a very long time, typically, the lifetime of the process that is running it. In fact, typically you'll have a process
dedicated just to running subscriptions (or even a process per subscription). 

The actual handling of the incoming documents is done by the subscribers, and it is done on a background thread. On the other hand, a subscription will typically
remained subscribed to the server until it is deleted, another subscription client took over its connection or an error occured in the processing of the 
subscription that it cannot recover from. Either way, we need to wait until any of those happen, and that is handled by waiting on `SubscriptionLifetimeTask`. 

In Listing 4.33 we used the `subsId` from Listing 4.32. This is typical to how we are using subscriptions. The subscription id should be persistent and survive
restat of the process or machine, becuase it is used to represent the state of the subscription and what doucments have already been processed by that particular
subscription.

You can set your own subscription id during the `Create` call, which give you well known subscription name to use, or you can ask RavenDB to choose one for you, 
as we have done in Listing 4.32. Note that even if you use a hard coded subscription id, it still needs to be created before you can call `Open` on it. 

Why do we have all those moving parts? We have the creation of the subscription, opening it, tracking its lifetime and we haven't even gotten to the part where
we are actually processing documents using it. 

The reason for this is that subscriptions are very long lived processes, which are resilient to failure. One a subscription is created, a client will open it 
and then keep a connection open to the server, getting fed all the documents that match the subscription criteria. This is true for all existing data in the
database. 

Once we have gone over all the documents currently in the database, the subscription will go to sleep, but will remain connected to the server. Whenever a new
or updated document will match the subscription criteria, it will be sent again. Errors during this process, either in the network, the server or the client are
tolerated and recoverable. Subscriptions will ensure that a client will recieve each matching document at least once^[Although errors may cause you to recieve 
the same document multiple times, you are guranteed to never miss a document]. 

Typically, on a subscription that already processed all existing documents in the database, the lag time between a new document coming in and the subscription 
recieving it is a few milliseconds. Under load, when there are many such documents, we'll use batches of documents and send them to the client for processing
as fast as it can process them.

After every successfully completed batch, the subscription will acknowledge to the server that it completed processing this batch, at which point the next batch
will be sent. It is the process of acknowledging the processing of the batch that make the process reliable. Until the client confirmed reciept of the batch,
we'll not move forward and send the next one. 

> **Subscription in a cluster**
>
> The subscription will connect to a server in the cluster, which may redirect the subscription to a more suited server for that particular subscription. Once
> the subscription found the appropriate server, it will open the subscription on that server and start getting documents from it. A failure of the client will
> result in a retry (either from the same client or possibly another one that was waiting to take over). A failure of the server will cause the client to 
> transparently switch over to another server. 
>
> The entire process is highly available on both client and server sides. The idea is that once you setup your subscriptions, you just need to make sure that the
> processes that open and process the subscription is running, and the entire system will hum along, automatically recovering from any failures along the way.

Okay, that is about as much as we can talk about subscriptions without actually showing what they are _doing_. Let us go to handle the actual document processing.

## Processing documents via subscriptions.

We previously seen the `Subscribe` method, in Listing 4.33. But we haven't seen yet what is actually subscribing. The `Subscribe` method is simply taking an 
`IObserver<Customer>` argument, we can also pass a lambda there, with the help of Reactive Extensions. Listing 4.34 shows the code to handle the subscription
that we redacted from Listing 4.33. 

```{caption="Processing customers via subscription" .cs}
subscription.Subscribe((SubscriptionResult<Customer> result) =>
{
	Customer customer = result.Document;
	// do something with this customer
});
```

After all this build up, the actual code in Listing 4.34 is pretty boring. What subscriptions gives us is an observer over documents. In this case, we register
a lambda to be called with a customer. This code will first go through all the customers in the database, invoking the lambda one at a time. Once we have gone
through all the customer documents, this subscription will wait, and whenever a new customer comes in, or an existing customer is modifed, we'll be called with
that document. 

What can we do with this? Well, quite a lot, as it turns out. We can use this to run all sort of business processes. For example, we may want to check if this
customer have a valid address, and if so, record the GPS coordinates so we can run spatial queries on it. Because of the way subscriptions work, we get a full
batch of documents from the server, and we can run heavy processing on them, we aren't limited by the data streaming over the network, unlike streaming, we won't 
run out of time here.

> **Subscriptions are background tasks**
>
> It may be obvious, but I wanted to state this explicitly, subscriptions are background tasks for the server. There is no requirement that a subscription 
> will be opened at any given point in time, and a subscription that wasn't opened will simply get all the documents that it needs to since the last 
> acknowledged batch.
> 
> That means that if a document was modified multiple times, it is possible that the subscription will only be called upon it once. See the section about
> Versioned Subscription if you care about this scenario.

One of the things we can do here is to open a new session, modify the document we got from the subscription, `Store` the document and call `SaveChanges` on it, 
right from the subscription lambda itself. But note that doing so will also typically put that document right back on the path to be called again with this 
subscription, so you need to be aware of that and protect against infinite loops like that. 

## Conditional Subscriptions

Subscriptions so far are useful, but not really something to get excited about. But the fun part starts now. Subscirptions aren't limited to just fetching all
the documents in a particular collection. We can do much better than this. Let us say that we want to send a survey for all customers where we had a complex
support call. The first step for that is to created a subscription using the code in Listing 4.35.

```{caption="Creating a subscription for complex calls" .cs}
var options = new SubscriptionCreationOptions<SupportCall>(
		call => 
			call.Comments.Count > 25 && 
			call.Votes > 10 		 && 
			call.Survey == false
	);
string subsId = store.Subscriptions.Create(options);
```

We are registering for subscriptions on support calls that have more than 10 votes and over 25 comments, and we add a flag to denote that we already sent the
survey. It is important to note that this filtering is happening on the _server side_, not on the client. Internally we'll transform the conditional into a
JavaScript expression and send it to the server, to be evaluated on each doucment in turn. Any matching document will be sent for the client for processing.
Of course, this is just part of the work, we still need to handle the subscription itself. This is done in Listing 4.36.

```{caption="Taking surveys of complex calls" .cs}
subscription.Subscribe((SubscriptionResult<SupportCall> result) =>
{
	SupportCall call = result.Document;
	if( (DateTime.Today - call.Started) > DateTime.FromDays(14))
		return; // not need to send survey for old stuff


	using(var session = store.OpenSession())
	{
		var customer = session.Load<Customer>(
			call.CustomerId);

		call.Survey = true;		

		session.Store(call, result.Etag, result.Id);

		try
		{
			session.SaveChanges();
		}
		catch(ConcurrenyException)
		{
			// will be retried by the subscription
			return;
		}	

		SendSurveyEmailTo(customer, call);
	}
});
```

A lot of stuff is going on in Listing 4.36. Even though we removed the code for actually opening the subscription (this is identical to Listing 4.33) there is 
still a lot going on. First, we fetch the customer for this support call, then we mark the call as having sent the survey. Then we call `Store` and pass it
not just the instance that we got from the subscription, but also the etag and id for this document. This ensures that when we call `SaveChanges`, if the 
document has changed in the meantime on the server side, we'll get an error.

In this case, this is an expected error, and if we got a concurrency exception, we can just ignore it and skip processing this document. There is a bit of a 
trickery involved here. Because the document have changed, the subscription will get it again anyway, so we'll skip sending it now, but we'll be sending it 
later.

Finally, we send the actual email. Note that in real production code, there is alsothe  need to decide what to do if sending the email failed. In this case, 
the code is assuming that it cannot fail, and favor skipping sending the email rather then sending it twice. Typical mail system have options to ignore 
duplicate emails in a certain time period, which is probably how you would solve this in production.

There are a few things to rembmer if you are using the subscription to write back to the same database you are subscribing to:

* Avoiding subscription/modify loop. When you modify a document that you are subscribing to, the server with send it to 
  the subscription again. If you'll modify it every time that it is processed, you'll effectively create an infinite loop, with all the costs
  that this entails. You can see in Listing 4.36 and Listing 4.37 that we were careful to avoid this by setting the `Survey` flag when we 
  processed a document and filtering on that flag in Listing 4.35.

* The document you got may have already been changed on the server. Typically, the lag time of between a document being modified and the 
  subscription processing that document is very short. That can lead you to think that this happens instantaneously or even worse, as part of 
  the same operation of modifying the document.
  
  Nothing could be further from the truth. A document may be changed between the time the server has sent you the document and the time you 
  finished processing and saving it. In Listing 4.36 we handled that explicitly using optimistic concurrency and in Listing 4.37 we used 
  patching to avoid having to deal with the issue. 

* If you are using subscriptions to integrate with other pieces of your infrastructure (such as sending emails, for example), you have to be
  ready for failure on that end and have some meaningful strategy for handling it. Your options are to either propogate the error up the chain,
  which will force the subscription to close (and retry from last susccessful batch) or you can catch the exception and handle it in some manner.

Instead of using explicit concurrency handling, you can also write the code in Listing 4.36 using a `Patch` command, as you can see in Listing 4.37.

```{caption="Taking surveys of complex calls, using patches" .cs}
subscription.Subscribe((SubscriptionResult<SupportCall> result) =>
{
	SupportCall call = result.Document;
	if( (DateTime.Today - call.Started) > DateTime.FromDays(14))
		return; // not need to send survey for old stuff


	using(var session = store.OpenSession())
	{
		var customer = session.Load<Customer>(
			call.CustomerId);

		session.Advanced.Patch<SupportCall, bool>(
			result.Id,
			c => c.Survey,
			true);

		SendSurveyEmailTo(customer, call);

		session.SaveChanges();
	}
});
```

In Listing 4.37 we are doing pretty much the same thing we did in Listing 4.36. The difference is that we are using a `Patch` command to do so, 
which saves us from having to check for concurrency violations. Even if the document have changed between the time the server has sent it to us
we will only set the `Survey` field on it. In Listing 4.37 we are also sending the survey email _before_ we set the `Survey` flag, so a failure
to send the email will be throw all the way to the calling code, which will typically retry the subscription. This is different from the code in
Listing 4.36, where we first set the flag and then send the email.

The main difference here is in what happens in the case of an error being raised when sending the survey email. In Listing 4.36, we have already
set the flag and sent it to the server, so the error will mean that we didn't send the email. The subscription will retry, of course, but the
document was already changed and will be filtered from us. In Listing 4.36, if there was an error in sending email, the email will not be sent.

On the other hand, in Listing 4.37, we first send the email, then set the flag and save it. This means that if there is an error sending the email
we'll retry the document later on. However, if we had an error saving the flag to the server and we already sent the email, we might send the email
twice. You need to consider what scenario you are trying to prevent, double email send or no email sent at all.

> **Distributed transactions and RavendB**
>
> The main reason that we have to face this issue is that we are forced to integrate between two systems that do not share a transaction boundary.
> In other words, theoretically speaking, if we could share a transaction between the email sending and the write to RavenDB, the problem would be
> solved. 
>
> In practice, RavenDB had support for distributed transactions with multiple resources up until version 3.x, when we deprecated this feature and 
> version 4.0 removed it completely. Distributed transactiosn (also known as two phase commit or 2PC) _sound_ wonderful. Here you have a complex
> interaction between several different components in your system, and you can use a transaction to orchestrate it all in a nice and simple 
> manner. 
>
> Except it doesn't work like this. Any distributed transaction system that I had worked with had issues related to failure handling and partial
> success. A distributed transaction coordinator basically require all praticipants in the transaction to promise it that if it tells them to
> commit the transaction, it will be successful. In fact, the way a coordinator usually work is by having one round of promises, and if all
> participants have been able to make that promise a second round with confirmations. Hence, the two phase commit name.
>
> The problem starts when you have gotten a promise from all the participants, you already confirmed with a few of them that the transaction
> has been committed, and one of the particpants fail to commit for whatever reason (hardware failure, for example). In that case, the 
> transaction is in a funny, half committed state. 
>
> The coordinator will tell you that this is a bug in the participant, that it shouldn't have made a promise that it couldn't keep. And typically
> coordinators will retry such transactions (manually or automatically) and recover from trasients errors. But the problem with "it is an issue
> with this particular particpant, not the coordinator" line of thinking is that those kind of errors are happening in production. 
>
> In one particular project, we had to restart the coordinator and manually resolve hanging transactions on a bi-weekly basis, and it wasn't a 
> very large or busy website. [Joe Armstrong](https://en.wikipedia.org/wiki/Joe_Armstrong_(programming) ), Inventor of Erlang,
> described^[That particular lecture was over a decade ago, and I still vividly remember it, it was _that_ good.] the problem far better than I 
> could:
>
> > The Two Generals' Problem is reality, but the computer industry says, it doesn't believe 
> > in mathematics: Two phase commit^[There is also the _three_ phase commit, which just add to the fun and doesn't actually solve the issue.] 
> > always works!

Instead of relying on two phase commit and distributed transactions, a much better alternative is to use the facilities of each system 
on its own. That topic goes beyond the scope of this book, but idempotent operations or de-duplication for operations can both give you a 
safe path to follow in the precense of errors in a distributed system. If the email system will recognize that this email has already been 
sent, the code in Listing 4.37 will have no issue. We'll never skip sending an email and we'll never send the same email twice. 

## Error handling with subscriptions

What happens when there is an error in the processing of a document? Imagine that we had code inside the lambda in Listing 4.34 and that code threw an exception.
Unless you set `SubscriptionConnectionOptions.IgnoreSubscriberErrors`^[You probably shouldn't do that.], we will abort processing of the subscription and the
`SubscriptionLifetimeTask` will raise an error. Typical handling in that scenario is to dispose of the subscription and immediately open it again. 

Assuming the error is transient, we'll start processing from the last batch we got and continue forward from there. If the error isn't transient, for example,
some `NullReferenceException` because the code isn't check for it, the error will obviously repeat itself. You might want to set an upper limit to the number
of errors you'll try to recover from in a given time period, and just fail completely afterward. This depend heavily on the kind of error reporting and 
recovery you are using in your applications. 

Note that this apply only to errors that came from the code processing the document. All other errors (connection to server, failover between servers, etc) are
already handled by RavenDB. The reason that we abort the subscription in the case of subscriber error is that there really isn't anything else that we can do.
We don't want to skip processing the document, and just logging the error is possible (in fact, that is exactly what we do if `IgnoreSubscriberErrors` is set)
but no one ever reads the log until the problem was already discovered, which is typically very late in the game. 

However, the actual processing of the documents and any concurrency and error handling during that is on you. In practice, you generally don't 
have to worry about it. An error thrown during document processing will kill your subscription. We saw in Listing 4.32 that after we start
the subscription, we need to pay attention to `SubscriptionLifetimeTask`. If an error is raised during document processing, that error will
close the subscription and any waiters on `SubscriptionLifetimeTask` will get that error.

The typical manner in which you will handle errors with subscriptions is just to retry the whole subscription, as shown in Listing 4.38.

```{caption="Retrying subscription on error" .cs}
var errorTimings = new Queue<DateTime>();
while(true)
{
	var options = new SubscriptionConnectionOptions(subsId);
	var subscription = store.Subscriptions.Open<SupportCall>(
		options));
	subscription.Subscribe(/* redacted for brevity */);
	subscription.Start();

	try
	{
		await subscription.SubscriptionLifetimeTask;
		return; // subscription deleted, process shutting down
	}
	catch(Exception e)
	{
		// log exception
		errorTimings.Enqueue(DateTime.UtcNow);
		if(errorTimings.Count < 5)
			continue;

		var firstErrorTime = errorTimings.Dequeue();
		var timeSinceErr = DateTime.UtcNow - firstErrorTime;
		if (timeSinceErr < TimeSpan.FromMinutes(5))
			continue;

		// log subscription shut down for constant errors
		return;
	}
}

```

Listing 4.38 shows a typically way to handle errors in subscriptions. If your document processing code throws, that exception will be raised via
`SubscriptionLifetimeTask`. In that case, we use the `errorTimings` queue to check if we got more than 5 errors in the space of the last 5 minutes. If 
we didn't, we continue normally and usually just this is enough to handle most transient errors. However, if we got more than 5 errors in the space of 
5 minutes, we'll abort the subscription and typically alert an admin that something strange is going on.

The policy shown in Listing 4.38 (max 5 errors in 5 minutes) is a trivially simple one. I have seen production system that just kep blindly retrying 
and others that had far more sophisticated process for recovery using exponential backoff to try to avoid long running but eventually transient failure.
Exactly how you'll handle error recovery in your system is up to you and the operations teams that will maintain the application in production.

## Complex conditionals 

We already saw how we can create a subscription that filter documents on the server side, in Listing 4.35. The code there used a labmda expression and 
the client API was able to turn that into a JavaScript function that was sent to the server. Listing 4.35 was a pretty simple expression, but the code
that handles the translation between lambdas and JavaScript is quite smart and is able to handle much more complex conditionals.

However, putting a complex condition in a single lambda is not a good recipe for good code. A better alternative is to skip the convienance of the lambda
and go directly to the JavaScript. In Listing 4.39, we are going to see how we can subscribe to all the support calls that require special attention.


```{caption="Creating a subscription using JavaScript filtering" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	var watchwords = ['annoy', 'hard', 'silly'];

	var lastIndex = this['@metadata']['Last-Monitored-Index'] || 0;

	for(var i = lastIndex; i < this.Comments.length; i++)
	{
		for(var j = 0; j < watchwords.length; j++)
		{
			var comment = this.Comments[i].toLowerCase();
			if(comment.indexOf(watchowrd[i]) != -1)
				return true;
		}
	}

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

The interesting code in Listing 4.39 is in the `Script` variable. We are defining a few words that will will watch for, and if we see them in the comments 
of a support call, we want to give it some special attention via this subscription. We that by simply scanning through the array of `Comments` and seeing
if any of the comments contains any of the words that we are looking for.

The one interesting tidbit is the use of `this['@metadata']['Last-Monitored-Index']`, what is that for? Remember that a subscription will be sent all the 
documents that match its criteria. And whenever a document is changed, it will be chcked if it match this subscription. That means that if we didn't have
some sort of check to stop it, our subscription will process any support call that had a comment with one of the words we watch for _every single time
that call is processed_. 

In order to avoid that scenario, we set a metadata value named `Last-Monitored-Index` when we process the subscription. You can see how that work in
Listing 4.40.

```{caption="Escalating problematic calls" .cs}
subscription.Subscribe((SubscriptionResult<SupportCall> result) =>
{
	SupportCall call = result.Document;

	using(var session = store.OpenSession())
	{
		session.Store(call, result.Etag, result.Id);
		var metadata = session.Advanced.GetMetdataFor(call);
		metadata["Last-Monitored-Index"] = call.Comments.Count;

		try
		{
			session.SaveChanges();
		}
		catch(ConcurrenyException)
		{
			// will be retried by the subscription
			return;
		}	
	}

	// actually escalate the call
});
```

We are simply setting the `Last-Monitored-Index` to the size of the `Comments` on the call (taking care of concurrency in the middle) and saving it back
to the server. This will ensure that we'll only get the support call again only if there are _new_ comments with the words that we are watching for.

> **Maintaining per document subscription state**
>
> Subscription often need to maintain some sort of state on a per document basis. In the case of Listing 4.40, we needed to keep track of the last 
> monitored index, but other times you'll have much more complex state to track. For example, imagine that we need to kick off a workflow that will
> escalate a call once it passed a certain threshold. We might need to keep track of the state of the workflow and have that be account for in the
> subscription itself.
>
> Using the metadata to do it works quite nicely if we are talking about small and simple state. However, as the complexity grows, it isn't viable
> to keep it all in the document metadata and we'll typically introduce a separate document to maintain the state of the subscription. In we are 
> tracking support call, then for `SupportCalls/238-B` will have a `SupportCalls/238-B/EscalationState` document that will conatain the relevant
> information for the subscription.

Listing 4.39 and Listing 4.40 together show us how a subscription can perform rather complex operations and open up some really interesting options for 
processing documents. But even so, we aren't done, we can do even more with subscriptions.

## Complex scripts

We have used conditional subscriptions to filter the documents that we want to process, and since this filtering is happening on the server side, it 
allows us to reduce the number of documents that we have to send to the subscription. This is awesome, but a really interesting feature of 
subscriptions is that we don't actually _need_ to send the full documents to the client, we can select just the relevant details to send. 

We want to do some processing on highly voted support calls, but we don't need to get the full document, we just need the actual issue and the number
of votes for that call. Instead of sending the full document over the wire, we can use the code in Listing 4.41 to efficeintly.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	return { 
		Issue: this.Issue, 
		Votes: this.Votes 
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

What we are doing in Listing 4.41 is to filter the support calls. If the call has less then 10 votes, we'll just return from the script. RavenDB consider
a `return` or `return false` or `return null` to be an indication that we want to skip this document, and will not send that to the client. On the other
hand, with `return true`, the document will be sent to the client. However, we aren't limited to just returning `true`. We can also return an object of our 
own. That object can be built by us and contain just the stuff that we want to send to the client. 

There is an issue here, however. In Listing 4.41, we are creating a subscription on `SupportCall`. However, the value that will be sent to the client for
the subscription to process is _not_ a `SupportCall` document. It is our own object that we created. That means that on the client side, we need to know 
how to handle that. This requires a bit of a change in how we open the subscription, as you can see in Listing 4.42.

```{caption="Opening subscription with a different target" .cs}
public class SupportCallSubscriptionOutput
{
	public string Issue;
	public int Votes;
}

var subscription = store.Subscriptions
	.Open<SupportCallSubscriptionResult>(options));

subscription.Subscribe(result =>
{
	var output = result.Document;
	// do something with the 
	// output.Issue, output.Votes
});
```

In order to consume the subscription in a type safe way, we create a class that matches the output that we'll get from the subscription script, and we'll 
use that when we open the subscription. As you can see, there isn't actually a requirement that the type that you use to `Create` the subscription and the
type you use for `Open` will be the same and scenarios like the one outling in Listing 4.41 and 4.42 make it very useful.

If this was all we could do with the subscription script, it would have been very useful in reducing the amount of data that is sent over the wire, but 
there is actually more options available to us that we haven't gotten around to yet. Consider Listing 4.37, there we get the support call and immediately
have to load the associated customer. That can lead to a remote call per item in the batch. We have already gone over why this can be a very bad idea in
term of overall performance. 

Instead, we can ask RavenDB to handle that as part of the subscription processing directly. Take a look at Listing 4.43, which does this.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	var customer = LoadDocument(this.CustomerId);

	return { 
		Issue: this.Issue, 
		Votes: this.Votes,
		Customer: {
			Name: customer.Name,
			Email: customer.Email
		}
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

In Listing 4.43, we are calling `LoadDocument` as part of the processing of the subscription on the server side. This allow us to get the customer instance
and send pieces of it back to the client. In order to consume it, we'll need to change the `SupportCallSubscriptionOutput` class that we introduced in 
Listing 4.42 to add the new fields. 

When processing the output of this subscription, we don't need to make any othe remote call, and can 

## Subscription and failover in a cluster

result.OpenSession, BeforeBatch / AfterBatch calls.
no failover by design.

The state of the batch

`SubscriptionException` handling

`Start` / `StartAsync`

Multiple calls to `Subscribe`

Load customer as part pf returning it.


## Versioned Subscriptions
